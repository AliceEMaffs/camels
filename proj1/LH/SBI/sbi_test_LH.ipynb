{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmcolors\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#from torch.distributions import Uniform, ExpTransform, TransformedDistribution #, AffineTransform\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> 690783e54 (edited scaling to just scale UVLF data between 0-1 and keep colours as is, as already between 0-1. Edited LH sbi test script for test metrics to see how well training is generalising with test set.)
   "source": [
    "# Key Concepts\n",
    "'''\n",
    "1. Forward problem: θ → x (simulation)\n",
    "2. Inverse problem: x → θ (what we're solving)\n",
    "3. Posterior: P(θ|x) probability distribution over parameters\n",
    "4. Prior: Initial assumptions about parameter ranges\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# seaparate into train and test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "#from torch.distributions import Uniform, ExpTransform, TransformedDistribution #, AffineTransform\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import joblib\n",
    "import os \n",
    "import ili\n",
    "from ili.dataloaders import NumpyLoader\n",
    "from ili.inference import InferenceRunner\n",
    "from ili.validation.metrics import PosteriorCoverage, PlotSinglePosterior\n",
    "\n",
    "from sbi.utils.user_input_checks import process_prior\n",
    "\n",
    "sys.path.append(\"/disk/xray15/aem2/camels/proj1/\")\n",
<<<<<<< HEAD
    "from setup_params_LH import plot_uvlf, plot_colour\n",
    "from setup_params_LH import *\n",
=======
>>>>>>> 690783e54 (edited scaling to just scale UVLF data between 0-1 and keep colours as is, as already between 0-1. Edited LH sbi test script for test metrics to see how well training is generalising with test set.)
    "from priors import initialise_priors\n",
    "from variables_config import n_bins_lf, n_bins_colour #, colour_limits, uvlf_limits\n",
    "from setup_params_LH import *\n",
    "\n",
    "# parameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = \"IllustrisTNG\"\n",
    "spec_type = \"attenuated\"\n",
    "sps = \"BC03\"\n",
    "# 084 = 0.1, 060=1.05, 052=1.48, 044=2\n",
    "snap = ['086'] # ['044', '052', '060',\n",
    "bands = \"all\" # or just GALEX?\n",
    "\n",
    "# lets try UVLF and colours this time.\n",
    "colours = True  \n",
    "luminosity_functions = True\n",
    "name = f\"{model}_{bands}_{sps}_{spec_type}_{n_bins_lf}_{n_bins_colour}\"\n",
    "\n",
    "# initialize CAMELS and load parameter info using camels.py\n",
    "cam = camels(model=model, sim_set='LH')\n",
    "\n",
    "if colours and not luminosity_functions:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/6pams/LH/IllustrisTNG/models/colours_only/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/6pams/LH/IllustrisTNG/test/sbi_plots/colours_only/\"\n",
    "    \n",
    "elif luminosity_functions and not colours:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/6pams/LH/IllustrisTNG/models/lf_only/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/6pams/LH/IllustrisTNG/test/sbi_plots/lf_only\"\n",
    "\n",
    "elif colours and luminosity_functions:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/6pams/LH/IllustrisTNG/models/colours_lfs/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/6pams/LH/IllustrisTNG/test/sbi_plots/colours_lfs/\"\n",
    "\n",
    "# You might want to add an else for safety:\n",
    "else:\n",
    "    raise ValueError(\"At least one of colours or luminosity_functions must be True\")\n",
    "\n",
    "print(\"Saving model in \", model_out_dir)\n",
    "print(\"Saving plots in \", plots_out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available snapshots first\n",
    "available_snaps = get_available_snapshots()\n",
    "print(f\"Available snapshots: {available_snaps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chris version of sbi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prior = initialise_priors(device=device, astro=True, dust=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    theta, x = get_theta_x(\n",
    "            photo_dir=f\"/disk/xray15/aem2/data/6pams/\",\n",
    "            spec_type=spec_type,\n",
    "            model=model,\n",
    "            snap=snap,\n",
    "            sps=sps,\n",
    "            n_bins_lf=13,\n",
    "            n_bins_colour=n_bins_colour,\n",
    "            colours=colours,\n",
    "            luminosity_functions=luminosity_functions,\n",
    "            device=device,\n",
    "    )\n",
    "    print(theta.shape, x.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if luminosity_functions:\n",
    "    fig = plot_uvlf(x)\n",
    "    plt.savefig('/disk/xray15/aem2/plots/6pams/LH/IllustrisTNG/test/LFs_test/lf_check.png')\n",
    "    plt.title('UVLF Before any scaling/normalisation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colours:\n",
    "    fig = plot_colour(x)\n",
    "    plt.savefig('/disk/xray15/aem2/plots/6pams/LH/IllustrisTNG/test/colours_test/colour_check.png')\n",
    "    plt.title('Colour Before any scaling/normalisation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_all = np.array([np.hstack(_x) for _x in x])\n",
    "\n",
    "# import test mask\n",
    "test_mask = np.loadtxt(\"/disk/xray15/aem2/data/6pams/test_mask.txt\", dtype=bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original breakdown of your data\n",
    "print(\"Original data shape:\", x_all.shape)\n",
    "print(\"Expected UVLF columns:\", 2 * (n_bins_lf - 1))\n",
    "print(\"Expected color columns:\", n_bins_colour - 1)\n",
    "print(\"Total columns:\", 2 * (n_bins_lf - 1) + (n_bins_colour - 1))\n",
    "\n",
    "# Extract UVLF and color directly from original\n",
    "original_uvlf = x_all[:, :2*(n_bins_lf-1)]\n",
    "original_color = x_all[:, 2*(n_bins_lf-1):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCALED DATA\n",
    "## We are scaling only the UVLF data so it is between 0-1 , as colour is already between 0-1 we don't want to scale it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original breakdown of your data\n",
    "print(\"Original data shape:\", x.shape)\n",
    "print(\"Expected UVLF columns:\", 2 * (n_bins_lf - 1))\n",
    "print(\"Expected color columns:\", n_bins_colour - 1)\n",
    "print(\"Total columns:\", 2 * (n_bins_lf - 1) + (n_bins_colour - 1))\n",
    "\n",
    "# Extract UVLF and color directly from original\n",
    "original_uvlf = x[:, :2*(n_bins_lf-1)]\n",
    "original_color = x[:, 2*(n_bins_lf-1):]\n",
    "\n",
    "# Let's try a different approach - extract the color data first then scale only UVLF\n",
    "def scale_uvlf_only(x, n_bins_lf=15, n_bins_colour=13):\n",
    "    # Make a copy to avoid modifying the original\n",
    "    x_copy = x.copy()\n",
    "    \n",
    "    # Get the indices for UVLF data\n",
    "    n_uvlf_cols = 2 * (n_bins_lf - 1)\n",
    "    \n",
    "    # Extract and scale only the UVLF portion\n",
    "    uvlf_data = x_copy[:, :n_uvlf_cols]\n",
    "    uvlf_min = uvlf_data.min()\n",
    "    uvlf_max = uvlf_data.max()\n",
    "    \n",
    "    # Apply scaling directly to the UVLF portion of the copy\n",
    "    x_copy[:, :n_uvlf_cols] = (uvlf_data - uvlf_min) / (uvlf_max - uvlf_min)\n",
    "    \n",
    "    return x_copy\n",
    "\n",
    "# Try this new approach\n",
    "x_all_scaled = scale_uvlf_only(x, n_bins_lf=n_bins_lf, n_bins_colour=n_bins_colour)\n",
    "\n",
    "# Check if color data is preserved\n",
    "color_after_scaling = x_all_scaled[:, 2*(n_bins_lf-1):]\n",
    "print(\"Original color data (sample):\", original_color[0, :5])\n",
    "print(\"Color data after new scaling (sample):\", color_after_scaling[0, :5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to tensor for later use in training\n",
    "x_tensor_scaled = torch.tensor(x_all_scaled, dtype=torch.float32, device=device)\n",
    "\n",
    "# Extract components for plotting\n",
    "uvlf_data = x_all_scaled[:, :2*(n_bins_lf-1)]\n",
    "color_data = x_all_scaled[:, 2*(n_bins_lf-1):]\n",
    "\n",
    "# Plot UVLF\n",
    "if luminosity_functions:\n",
    "    fig = plot_uvlf(uvlf_data, n_bins=n_bins_lf)\n",
    "    fig.axes[0].set_ylabel('Normalized φ', fontsize=12)\n",
    "    plt.title('UVLF SCALED independently')\n",
    "    plt.savefig('/disk/xray15/aem2/plots/6pams/LH/IllustrisTNG/test/LFs_test/uvlf_check_scaledLFonly.png')\n",
    "    plt.show()\n",
    "\n",
    "# Plot color distribution\n",
    "if colours:\n",
    "    fig = plot_colour(color_data, n_bins=n_bins_colour, n_sims_to_plot=15)\n",
    "    plt.title('Colour NOT SCALED independently')\n",
    "    plt.savefig('/disk/xray15/aem2/plots/6pams/LH/IllustrisTNG/test/colours_test/colour_check_scaledLFonly.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Network architecture improvements\n",
    "hidden_features = 60  # Increase from 30 for more capacity\n",
    "num_transforms = 4    # Increase from 4 for more expressive transforms\n",
    "num_nets = 2\n",
    "# # num_bins = 10 # spline bins, this is default in sbi package anyway.\n",
    "\n",
    "# Create larger ensemble\n",
    "# Increase ensemble size and capacity slightly\n",
    "nets = [\n",
    "    ili.utils.load_nde_sbi(\n",
    "        engine=\"NPE\",\n",
    "        model=\"nsf\", \n",
    "        hidden_features=hidden_features,      # Reduce further for better σ8 and ASN1/2\n",
    "        num_transforms=num_transforms,        # Reduce transforms for simpler model\n",
    "    ) for _ in range(num_nets)         # Keep ensemble size\n",
    "]\n",
    "\n",
    "train_args = {\n",
    "    \"training_batch_size\": 4,\n",
    "    \"learning_rate\": 5e-4,      # Keep current LR\n",
    "    \"stop_after_epochs\": 10,     # Keep early stopping criteria\n",
    "    \"max_num_epochs\": 25,       # Middle ground for epochs\n",
    "    # \"clip_max_norm\": 0.8,       # Keep current clipping\n",
    "    \"validation_fraction\": 0.1, # Keep current validation split\n",
    "    \"use_combined_loss\": True,\n",
    "    \"show_train_summary\": True,\n",
    "    \"dataloader_kwargs\": {\n",
    "        \"num_workers\": 0,\n",
    "        \"pin_memory\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create config string for filenames\n",
    "config_str = (f\"batch{train_args['training_batch_size']}_\"\n",
    "             f\"lr{train_args['learning_rate']}_\"\n",
    "             f\"epochs{train_args['stop_after_epochs']}_\"\n",
    "             f\"max_num_epochs{train_args['max_num_epochs']}_\"\n",
    "             f\"validation_fraction{train_args['validation_fraction']}_\"\n",
    "            #  f\"clip_max_norm{train_args['clip_max_norm']}_\"\n",
    "             f\"h{hidden_features}_t{num_transforms}_nn{num_nets}\")\n",
    "\n",
    "# Create a new directory with the config string\n",
    "config_plots_dir = os.path.join(plots_out_dir, config_str)\n",
    "config_model_dir = os.path.join(model_out_dir, config_str)\n",
    "os.makedirs(config_plots_dir, exist_ok=True)\n",
    "os.makedirs(config_model_dir, exist_ok=True)\n",
    "print(\"Model will be saved to: \",config_model_dir)\n",
    "print(\"Plots will be saved to: \",config_plots_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep the existing loader setup with scaled data\n",
    "loader = NumpyLoader(\n",
    "    x=x_all_scaled[~test_mask],\n",
    "    theta=torch.tensor(theta[~test_mask, :], device=device)\n",
    ")\n",
    "\n",
    "\n",
    "runner = InferenceRunner.load(\n",
    "    backend=\"sbi\",\n",
    "    engine=\"NPE\",\n",
    "    prior=prior,\n",
    "    nets=nets,\n",
    "    device=device,\n",
    "    train_args=train_args,\n",
    "    proposal=None,\n",
    "    out_dir=model_out_dir,\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "\n",
    "posterior_ensemble, summaries = runner(loader=loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_np = x_all_scaled[test_mask]\n",
    "theta_test_np = theta[test_mask].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(theta_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# First plot: Training diagnostics with two subplots\n",
    "def plot_training_diagnostics(summaries):\n",
    "    \"\"\"Plot training diagnostics with loss and overfitting gap\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    train_losses = summaries[0]['training_log_probs']\n",
    "    val_losses = summaries[0]['validation_log_probs']\n",
    "    epochs = range(len(train_losses))\n",
    "    \n",
    "    ax1.plot(epochs, train_losses, '-', label='Training', color='blue')\n",
    "    ax1.plot(epochs, val_losses, '--', label='Validation', color='red')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Log probability')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overfitting gap\n",
    "    gap = np.array(train_losses) - np.array(val_losses)\n",
    "    ax2.plot(epochs, gap, '-', color='purple')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss difference')\n",
    "    ax2.set_title('Overfitting Gap')\n",
    "    ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Second plot: Ensemble training curves\n",
    "def plot_ensemble_training(summaries):\n",
    "    \"\"\"Plot training curves for each ensemble member\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6,4))\n",
    "    c = list(mcolors.TABLEAU_COLORS)\n",
    "    for i, m in enumerate(summaries):\n",
    "        ax.plot(m['training_log_probs'], ls='-', label=f\"{i}_train\", c=c[i])\n",
    "        ax.plot(m['validation_log_probs'], ls='--', label=f\"{i}_val\", c=c[i])\n",
    "    ax.set_xlim(0)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Log probability')\n",
    "    ax.legend()\n",
    "    return fig\n",
    "\n",
    "# Save training plots in the new directory\n",
    "fig1 = plot_training_diagnostics(summaries)\n",
    "plt.savefig(os.path.join(config_plots_dir, f'training_analysis_{name}.png'),\n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "fig2 = plot_ensemble_training(summaries)\n",
    "plt.savefig(os.path.join(config_plots_dir, f'ensemble_training_{name}.png'),\n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Create metric and get plots\n",
    "metric = PosteriorCoverage(\n",
    "    num_samples=int(1000),\n",
    "    sample_method='direct',\n",
    "    labels=cam.labels,\n",
    "    plot_list=[\"coverage\", \"histogram\", \"predictions\", \"tarp\", \"logprob\"], \n",
    ")\n",
    "\n",
    "# Create metric and get plots\n",
    "metric = PosteriorCoverage(\n",
    "    num_samples=int(1000),\n",
    "    sample_method='direct',\n",
    "    labels=cam.labels,\n",
    "    plot_list=[\"coverage\", \"histogram\", \"predictions\", \"tarp\", \"logprob\"], \n",
    ")\n",
    "\n",
    "# Get the metric plots\n",
    "plot_types = [\"coverage\", \"histogram\", \"predictions\", \"tarp\", \"logprob\"]\n",
    "figs = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x=x_test_np,\n",
    "    theta=theta_test_np,\n",
    "    signature=f\"coverage_{name}_\"\n",
    ")\n",
    "\n",
    "# Save and display the metric plots\n",
    "def save_and_display_metric_plots(figs, plot_types, config_plots_dir, name):\n",
    "    for fig, plot_type in zip(figs, plot_types):\n",
    "        if fig is not None:\n",
    "            # Ensure the figure is a matplotlib figure\n",
    "            if hasattr(fig, 'savefig'):\n",
    "                plot_path = os.path.join(config_plots_dir, f'{plot_type}_{name}.png')\n",
    "                fig.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "                plt.figure(fig.number)  # Make this figure the current figure\n",
    "                plt.show()  # Display the figure\n",
    "                plt.close(fig)  # Close the figure to free up memory\n",
    "            else:\n",
    "                print(f\"Warning: {plot_type} figure is not a standard matplotlib figure\")\n",
    "\n",
    "# Save and display the metric plots\n",
    "save_and_display_metric_plots(figs, plot_types, config_plots_dir, name)\n",
    "\n",
    "print(f\"All plots saved in: {config_plots_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors / Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first try tweaking the time and then try tweaking the model a little and making it simpler and leting it run for longer.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ili.validation.metrics import PosteriorSamples\n",
    "\n",
    "# Create the metric object\n",
    "metric = PosteriorSamples(\n",
    "    num_samples=int(1e4),  # 10,000 samples like your supervisor used\n",
    "    sample_method=\"direct\",\n",
    ")\n",
    "\n",
    "# Now use it to get posterior samples\n",
    "psamps = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x=x_all_scaled[test_mask],\n",
    "    theta=theta[test_mask],\n",
    ")\n",
    "\n",
    "# Calculate the percentiles and metrics\n",
    "perc = np.percentile(psamps, q=[16, 50, 84], axis=0)\n",
    "\n",
    "# Calculate RMSE, epsilon, R², and χ²\n",
    "rmse = np.sqrt(\n",
    "    np.sum((theta.cpu().numpy()[test_mask, :] - perc[1, :, :])**2, axis=0) / \n",
    "    np.sum(test_mask)\n",
    ")\n",
    "\n",
    "# Mean relative error (epsilon)\n",
    "mre = np.sum(\n",
    "    ((perc[2, :, :] - perc[0, :, :]) / 2) / perc[1, :, :], axis=0\n",
    ") / np.sum(test_mask)\n",
    "\n",
    "# R-squared\n",
    "theta_hat = np.sum(theta.cpu().numpy()[test_mask, :], axis=0) / np.sum(test_mask)\n",
    "r2 = 1 - np.sum(\n",
    "    (theta.cpu().numpy()[test_mask, :] - perc[1, :, :])**2, axis=0\n",
    ") / np.sum(\n",
    "    (theta.cpu().numpy()[test_mask, :] - theta_hat)**2, axis=0\n",
    ")\n",
    "\n",
    "# Chi-squared\n",
    "chi2 = np.sum(\n",
    "    (theta.cpu().numpy()[test_mask, :] - perc[1, :, :])**2 /\n",
    "    ((perc[2, :, :] - perc[0, :, :]) / 2)**2, axis=0\n",
    ") / np.sum(test_mask)\n",
    "\n",
    "# Print results for each parameter\n",
    "for i, param in enumerate(cam.labels):\n",
    "    print(f\"\\nMetrics for {param}:\")\n",
    "    print(f\"RMSE: {rmse[i]:.4f}\")\n",
    "    print(f\"Epsilon: {mre[i]:.4f}\")\n",
    "    print(f\"R²: {r2[i]:.4f}\")\n",
    "    print(f\"χ²: {chi2[i]:.4f}\")\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the file path\n",
    "metrics_file = os.path.join(config_plots_dir, f'metrics_{name}.txt')\n",
    "\n",
    "# Write metrics to the file\n",
    "with open(metrics_file, 'w') as f:\n",
    "    for i, param in enumerate(cam.labels):\n",
    "        f.write(f\"\\nMetrics for {param}:\\n\")\n",
    "        f.write(f\"RMSE: {rmse[i]:.4f}\\n\")\n",
    "        f.write(f\"Epsilon: {mre[i]:.4f}\\n\")\n",
    "        f.write(f\"R²: {r2[i]:.4f}\\n\")\n",
    "        f.write(f\"χ²: {chi2[i]:.4f}\\n\")\n",
    "\n",
    "print(f\"Metrics saved in: {metrics_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "# Using variables already defined:\n",
    "# x_all_scaled, theta, test_mask, cam, posterior_ensemble\n",
    "\n",
    "def create_corner_plot(samples, true_values, param_names, figsize=(12, 12)):\n",
    "    \"\"\"\n",
    "    Create a corner plot showing marginal and joint distributions of parameter samples.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    samples : array-like\n",
    "        Array of shape (n_samples, n_params) containing posterior samples\n",
    "    true_values : array-like\n",
    "        Array of shape (n_params,) containing true parameter values\n",
    "    param_names : list\n",
    "        List of parameter names\n",
    "    figsize : tuple\n",
    "        Figure size\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib Figure\n",
    "        Corner plot figure\n",
    "    \"\"\"\n",
    "    n_params = samples.shape[1]\n",
    "    fig, axes = plt.subplots(n_params, n_params, figsize=figsize)\n",
    "    \n",
    "    # Set up the axes\n",
    "    for i in range(n_params):\n",
    "        for j in range(n_params):\n",
    "            if i < j:\n",
    "                axes[i, j].set_visible(False)\n",
    "                continue\n",
    "                \n",
    "            if i == j:  # Diagonal: show marginal distributions\n",
    "                axes[i, i].hist(samples[:, i], bins=25, alpha=0.7, density=True)\n",
    "                axes[i, i].axvline(true_values[i], color='red', linestyle='--')\n",
    "                \n",
    "                # Only show x labels on bottom row\n",
    "                if i < n_params - 1:\n",
    "                    axes[i, i].set_xticklabels([])\n",
    "                else:\n",
    "                    axes[i, i].set_xlabel(param_names[i])\n",
    "                \n",
    "                # Remove y ticks for cleaner look\n",
    "                axes[i, i].set_yticks([])\n",
    "                \n",
    "            else:  # Off-diagonal: show joint distributions\n",
    "                axes[i, j].scatter(samples[:, j], samples[:, i], alpha=0.1, s=1)\n",
    "                axes[i, j].scatter(true_values[j], true_values[i], color='red', s=20, marker='*')\n",
    "                \n",
    "                # Only show x labels on bottom row\n",
    "                if i < n_params - 1:\n",
    "                    axes[i, j].set_xticklabels([])\n",
    "                else:\n",
    "                    axes[i, j].set_xlabel(param_names[j])\n",
    "                \n",
    "                # Only show y labels on leftmost column\n",
    "                if j > 0:\n",
    "                    axes[i, j].set_yticklabels([])\n",
    "                else:\n",
    "                    axes[i, j].set_ylabel(param_names[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_parameter_recovery(true_values, estimated_values, uncertainty_low, uncertainty_high, param_names):\n",
    "    \"\"\"\n",
    "    Plot true vs. estimated parameter values with error bars.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_values : array-like\n",
    "        Array of shape (n_samples, n_params) containing true parameter values\n",
    "    estimated_values : array-like\n",
    "        Array of shape (n_samples, n_params) containing estimated (median) values\n",
    "    uncertainty_low : array-like\n",
    "        Array of shape (n_samples, n_params) containing lower bounds\n",
    "    uncertainty_high : array-like\n",
    "        Array of shape (n_samples, n_params) containing upper bounds\n",
    "    param_names : list\n",
    "        List of parameter names\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib Figure\n",
    "        Parameter recovery plot\n",
    "    \"\"\"\n",
    "    n_params = true_values.shape[1]\n",
    "    fig, axes = plt.subplots(1, n_params, figsize=(n_params*4, 4))\n",
    "    \n",
    "    for i in range(n_params):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Calculate value ranges for equal aspect ratio\n",
    "        param_min = min(true_values[:, i].min(), estimated_values[:, i].min())\n",
    "        param_max = max(true_values[:, i].max(), estimated_values[:, i].max())\n",
    "        param_range = param_max - param_min\n",
    "        \n",
    "        # Add some padding\n",
    "        param_min -= 0.1 * param_range\n",
    "        param_max += 0.1 * param_range\n",
    "        \n",
    "        # Plot diagonal line\n",
    "        ax.plot([param_min, param_max], [param_min, param_max], 'k--', alpha=0.5)\n",
    "        \n",
    "        # Plot error bars\n",
    "        for j in range(len(true_values)):\n",
    "            ax.errorbar(\n",
    "                true_values[j, i], \n",
    "                estimated_values[j, i], \n",
    "                yerr=[[estimated_values[j, i] - uncertainty_low[j, i]], \n",
    "                      [uncertainty_high[j, i] - estimated_values[j, i]]],\n",
    "                fmt='o', alpha=0.5, color='blue', capsize=3\n",
    "            )\n",
    "        \n",
    "        # Compute R² for this parameter\n",
    "        true_mean = np.mean(true_values[:, i])\n",
    "        ss_tot = np.sum((true_values[:, i] - true_mean)**2)\n",
    "        ss_res = np.sum((true_values[:, i] - estimated_values[:, i])**2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        # Compute RMSE\n",
    "        rmse = np.sqrt(np.mean((true_values[:, i] - estimated_values[:, i])**2))\n",
    "        \n",
    "        ax.set_xlabel(f'True {param_names[i]}')\n",
    "        ax.set_ylabel(f'Estimated {param_names[i]}')\n",
    "        ax.set_title(f'{param_names[i]}\\nR² = {r2:.3f}, RMSE = {rmse:.3f}')\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlim(param_min, param_max)\n",
    "        ax.set_ylim(param_min, param_max)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def visualize_test_set_results(config_plots_dir, name):\n",
    "    \"\"\"\n",
    "    Generate and save visualization of test set results.\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(config_plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Set up CUDA if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # 1. Get posterior samples for test set\n",
    "    test_x = x_all_scaled[test_mask]\n",
    "    test_theta = theta[test_mask]\n",
    "    \n",
    "    # Number of test samples to visualize (limit to avoid overwhelming plots)\n",
    "    n_test_samples = min(20, test_theta.shape[0])\n",
    "    \n",
    "    # 2. Get predictions for each test sample\n",
    "    predictions = []\n",
    "    for i in range(n_test_samples):\n",
    "        # Get posterior samples for this test point\n",
    "        x_i = test_x[i].unsqueeze(0)  # Add batch dimension\n",
    "        samples = posterior_ensemble.sample((10000,), x=x_i.to(device)).cpu().numpy()\n",
    "        predictions.append(samples)\n",
    "    \n",
    "    # 3. Compute statistics from posterior samples\n",
    "    perc = []\n",
    "    for i in range(n_test_samples):\n",
    "        sample_perc = np.percentile(predictions[i], q=[16, 50, 84], axis=0)\n",
    "        perc.append(sample_perc)\n",
    "    \n",
    "    # Convert to numpy arrays for easier processing\n",
    "    perc = np.array(perc)  # Shape: [n_test_samples, 3 (percentiles), n_params]\n",
    "    test_theta_np = test_theta.cpu().numpy()[:n_test_samples]\n",
    "    \n",
    "    # 4. Create corner plot for a few test examples\n",
    "    for i in range(min(5, n_test_samples)):\n",
    "        fig = create_corner_plot(\n",
    "            samples=predictions[i], \n",
    "            true_values=test_theta_np[i], \n",
    "            param_names=cam.labels,\n",
    "            figsize=(15, 15)\n",
    "        )\n",
    "        fig.suptitle(f'Posterior Distribution for Test Sample {i+1}', fontsize=16)\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.97])  # Make room for suptitle\n",
    "        plt.savefig(os.path.join(config_plots_dir, f'corner_plot_test_sample_{i+1}_{name}.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # 5. Create parameter recovery plot\n",
    "    medians = perc[:, 1, :]  # Shape: [n_test_samples, n_params]\n",
    "    lower_bounds = perc[:, 0, :]\n",
    "    upper_bounds = perc[:, 2, :]\n",
    "    \n",
    "    fig = plot_parameter_recovery(\n",
    "        true_values=test_theta_np, \n",
    "        estimated_values=medians, \n",
    "        uncertainty_low=lower_bounds, \n",
    "        uncertainty_high=upper_bounds, \n",
    "        param_names=cam.labels\n",
    "    )\n",
    "    fig.suptitle('Parameter Recovery on Test Set', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])  # Make room for suptitle\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'parameter_recovery_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 6. Create a table of metrics for each parameter\n",
    "    metrics = []\n",
    "    for i, param in enumerate(cam.labels):\n",
    "        # Calculate metrics for this parameter\n",
    "        rmse = np.sqrt(np.mean((test_theta_np[:, i] - medians[:, i])**2))\n",
    "        mre = np.mean(((upper_bounds[:, i] - lower_bounds[:, i]) / 2) / medians[:, i])\n",
    "        true_mean = np.mean(test_theta_np[:, i])\n",
    "        ss_tot = np.sum((test_theta_np[:, i] - true_mean)**2)\n",
    "        ss_res = np.sum((test_theta_np[:, i] - medians[:, i])**2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        chi2 = np.mean((test_theta_np[:, i] - medians[:, i])**2 / \n",
    "                      ((upper_bounds[:, i] - lower_bounds[:, i]) / 2)**2)\n",
    "        \n",
    "        metrics.append({\n",
    "            'Parameter': param,\n",
    "            'RMSE': rmse,\n",
    "            'MRE': mre,\n",
    "            'R²': r2,\n",
    "            'χ²': chi2\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_df.to_csv(os.path.join(config_plots_dir, f'metrics_summary_{name}.csv'), index=False)\n",
    "    \n",
    "    # 7. Visualize color distributions and UVLFs from the test set\n",
    "    # Extract a few test examples to visualize\n",
    "    for i in range(min(3, n_test_samples)):\n",
    "        # Get the original (unscaled) data for this test sample\n",
    "        x_original = x_all[test_mask][i]\n",
    "        \n",
    "        # Plot UVLF\n",
    "        fig_uvlf = plot_uvlf(np.array([x_original]), n_bins=n_bins_lf)\n",
    "        plt.suptitle(f'UVLF for Test Sample {i+1}', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.savefig(os.path.join(config_plots_dir, f'uvlf_test_sample_{i+1}_{name}.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close(fig_uvlf)\n",
    "        \n",
    "        # Plot Color Distribution\n",
    "        fig_color = plot_colour(np.array([x_original]), n_bins=n_bins_colour, n_sims_to_plot=1)\n",
    "        plt.suptitle(f'Color Distribution for Test Sample {i+1}', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.savefig(os.path.join(config_plots_dir, f'color_test_sample_{i+1}_{name}.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close(fig_color)\n",
    "    \n",
    "    # 8. Create a heatmap of parameter correlations\n",
    "    correlation_matrix = np.zeros((len(cam.labels), len(cam.labels)))\n",
    "    for i in range(len(cam.labels)):\n",
    "        for j in range(len(cam.labels)):\n",
    "            r = np.corrcoef(test_theta_np[:, i], medians[:, j])[0, 1]\n",
    "            correlation_matrix[i, j] = r\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    im = ax.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xticks(np.arange(len(cam.labels)))\n",
    "    ax.set_yticks(np.arange(len(cam.labels)))\n",
    "    ax.set_xticklabels(cam.labels)\n",
    "    ax.set_yticklabels(cam.labels)\n",
    "    \n",
    "    # Rotate x labels for better readability\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Add a colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel('Correlation', rotation=-90, va=\"bottom\")\n",
    "    \n",
    "    # Add correlation values as text\n",
    "    for i in range(len(cam.labels)):\n",
    "        for j in range(len(cam.labels)):\n",
    "            text = ax.text(j, i, f\"{correlation_matrix[i, j]:.2f}\",\n",
    "                           ha=\"center\", va=\"center\", color=\"black\" if abs(correlation_matrix[i, j]) < 0.5 else \"white\")\n",
    "    \n",
    "    ax.set_title(\"Correlation between True and Estimated Parameters\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'parameter_correlation_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 9. Create a summary plot for all parameters\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, param in enumerate(cam.labels):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot true vs. estimated with error bars\n",
    "        ax.errorbar(\n",
    "            test_theta_np[:, i], \n",
    "            medians[:, i], \n",
    "            yerr=[medians[:, i] - lower_bounds[:, i], upper_bounds[:, i] - medians[:, i]],\n",
    "            fmt='o', alpha=0.5, capsize=3, label='Posterior median with 68% CI'\n",
    "        )\n",
    "        \n",
    "        # Add diagonal line\n",
    "        param_min = min(test_theta_np[:, i].min(), medians[:, i].min())\n",
    "        param_max = max(test_theta_np[:, i].max(), medians[:, i].max())\n",
    "        param_range = param_max - param_min\n",
    "        param_min -= 0.1 * param_range\n",
    "        param_max += 0.1 * param_range\n",
    "        ax.plot([param_min, param_max], [param_min, param_max], 'k--', alpha=0.7, label='Perfect recovery')\n",
    "        \n",
    "        # Compute metrics\n",
    "        true_mean = np.mean(test_theta_np[:, i])\n",
    "        ss_tot = np.sum((test_theta_np[:, i] - true_mean)**2)\n",
    "        ss_res = np.sum((test_theta_np[:, i] - medians[:, i])**2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        rmse = np.sqrt(np.mean((test_theta_np[:, i] - medians[:, i])**2))\n",
    "        \n",
    "        ax.set_xlabel(f'True {param}')\n",
    "        ax.set_ylabel(f'Estimated {param}')\n",
    "        ax.set_title(f'{param}\\nR² = {r2:.3f}, RMSE = {rmse:.3f}')\n",
    "        \n",
    "        if i == 0:  # Only add legend to first subplot\n",
    "            ax.legend()\n",
    "        \n",
    "    # Remove any unused subplots\n",
    "    for i in range(len(cam.labels), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "        \n",
    "    fig.suptitle('Parameter Recovery Summary', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'parameter_summary_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"All visualizations saved in: {config_plots_dir}\")\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Call the function to generate visualizations\n",
    "metrics_df = visualize_test_set_results(config_plots_dir, name)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def compare_train_test_performance():\n",
    "    \"\"\"\n",
    "    Compare the model performance on training and test sets.\n",
    "    \"\"\"\n",
    "    # Set up CUDA if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Get samples for both training and test sets\n",
    "    train_x = x_all_scaled[~test_mask]\n",
    "    train_theta = theta[~test_mask]\n",
    "    test_x = x_all_scaled[test_mask]\n",
    "    test_theta = theta[test_mask]\n",
    "    \n",
    "    # Use a subsample to keep computation manageable\n",
    "    n_train_samples = min(50, train_theta.shape[0])\n",
    "    n_test_samples = min(50, test_theta.shape[0])\n",
    "    \n",
    "    # Randomly select indices for subsampling\n",
    "    train_indices = np.random.choice(train_theta.shape[0], n_train_samples, replace=False)\n",
    "    \n",
    "    # For test, use all if fewer than n_test_samples, otherwise randomly select\n",
    "    if test_theta.shape[0] <= n_test_samples:\n",
    "        test_indices = np.arange(test_theta.shape[0])\n",
    "    else:\n",
    "        test_indices = np.random.choice(test_theta.shape[0], n_test_samples, replace=False)\n",
    "    \n",
    "    # Get predictions for training samples\n",
    "    train_predictions = []\n",
    "    for i in train_indices:\n",
    "        x_i = train_x[i].unsqueeze(0)  # Add batch dimension\n",
    "        samples = posterior_ensemble.sample((1000,), x=x_i.to(device)).cpu().numpy()\n",
    "        train_predictions.append(samples)\n",
    "    \n",
    "    # Get predictions for test samples\n",
    "    test_predictions = []\n",
    "    for i in test_indices:\n",
    "        x_i = test_x[i].unsqueeze(0)  # Add batch dimension\n",
    "        samples = posterior_ensemble.sample((1000,), x=x_i.to(device)).cpu().numpy()\n",
    "        test_predictions.append(samples)\n",
    "    \n",
    "    # Compute statistics from posterior samples\n",
    "    train_perc = []\n",
    "    for i in range(len(train_indices)):\n",
    "        sample_perc = np.percentile(train_predictions[i], q=[16, 50, 84], axis=0)\n",
    "        train_perc.append(sample_perc)\n",
    "    \n",
    "    test_perc = []\n",
    "    for i in range(len(test_indices)):\n",
    "        sample_perc = np.percentile(test_predictions[i], q=[16, 50, 84], axis=0)\n",
    "        test_perc.append(sample_perc)\n",
    "    \n",
    "    # Convert to numpy arrays for easier processing\n",
    "    train_perc = np.array(train_perc)  # Shape: [n_samples, 3 (percentiles), n_params]\n",
    "    test_perc = np.array(test_perc)\n",
    "    train_theta_np = train_theta.cpu().numpy()[train_indices]\n",
    "    test_theta_np = test_theta.cpu().numpy()[test_indices]\n",
    "    \n",
    "    # Calculate metrics for both sets\n",
    "    train_metrics = []\n",
    "    test_metrics = []\n",
    "    \n",
    "    for i, param in enumerate(cam.labels):\n",
    "        # Training metrics\n",
    "        train_rmse = np.sqrt(np.mean((train_theta_np[:, i] - train_perc[:, 1, i])**2))\n",
    "        train_mre = np.mean(((train_perc[:, 2, i] - train_perc[:, 0, i]) / 2) / train_perc[:, 1, i])\n",
    "        train_true_mean = np.mean(train_theta_np[:, i])\n",
    "        train_ss_tot = np.sum((train_theta_np[:, i] - train_true_mean)**2)\n",
    "        train_ss_res = np.sum((train_theta_np[:, i] - train_perc[:, 1, i])**2)\n",
    "        train_r2 = 1 - (train_ss_res / train_ss_tot)\n",
    "        train_chi2 = np.mean((train_theta_np[:, i] - train_perc[:, 1, i])**2 / \n",
    "                          ((train_perc[:, 2, i] - train_perc[:, 0, i]) / 2)**2)\n",
    "        \n",
    "        # Test metrics\n",
    "        test_rmse = np.sqrt(np.mean((test_theta_np[:, i] - test_perc[:, 1, i])**2))\n",
    "        test_mre = np.mean(((test_perc[:, 2, i] - test_perc[:, 0, i]) / 2) / test_perc[:, 1, i])\n",
    "        test_true_mean = np.mean(test_theta_np[:, i])\n",
    "        test_ss_tot = np.sum((test_theta_np[:, i] - test_true_mean)**2)\n",
    "        test_ss_res = np.sum((test_theta_np[:, i] - test_perc[:, 1, i])**2)\n",
    "        test_r2 = 1 - (test_ss_res / test_ss_tot)\n",
    "        test_chi2 = np.mean((test_theta_np[:, i] - test_perc[:, 1, i])**2 / \n",
    "                         ((test_perc[:, 2, i] - test_perc[:, 0, i]) / 2)**2)\n",
    "        \n",
    "        train_metrics.append({\n",
    "            'Parameter': param,\n",
    "            'Dataset': 'Training',\n",
    "            'RMSE': train_rmse,\n",
    "            'MRE': train_mre,\n",
    "            'R²': train_r2,\n",
    "            'χ²': train_chi2\n",
    "        })\n",
    "        \n",
    "        test_metrics.append({\n",
    "            'Parameter': param,\n",
    "            'Dataset': 'Test',\n",
    "            'RMSE': test_rmse,\n",
    "            'MRE': test_mre,\n",
    "            'R²': test_r2,\n",
    "            'χ²': test_chi2\n",
    "        })\n",
    "    \n",
    "    # Combine metrics\n",
    "    all_metrics = pd.DataFrame(train_metrics + test_metrics)\n",
    "    \n",
    "    # Create directory for plots\n",
    "    os.makedirs(config_plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    all_metrics.to_csv(os.path.join(config_plots_dir, f'train_test_metrics_comparison_{name}.csv'), index=False)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    # 1. Barplot comparisons\n",
    "    metrics_to_plot = ['RMSE', 'R²', 'MRE', 'χ²']\n",
    "    \n",
    "    for metric in metrics_to_plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = sns.barplot(x='Parameter', y=metric, hue='Dataset', data=all_metrics)\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, p in enumerate(ax.patches):\n",
    "            height = p.get_height()\n",
    "            ax.text(p.get_x() + p.get_width()/2., height + 0.01,\n",
    "                   f'{height:.3f}', ha=\"center\", fontsize=9)\n",
    "        \n",
    "        plt.title(f'Comparison of {metric} Between Training and Test Sets')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config_plots_dir, f'train_test_{metric}_comparison_{name}.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    # 2. Create a combined visualization for all metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        ax = axes[i]\n",
    "        sns.barplot(x='Parameter', y=metric, hue='Dataset', data=all_metrics, ax=ax)\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for j, p in enumerate(ax.patches):\n",
    "            height = p.get_height()\n",
    "            ax.text(p.get_x() + p.get_width()/2., height + 0.01,\n",
    "                   f'{height:.3f}', ha=\"center\", fontsize=8)\n",
    "        \n",
    "        ax.set_title(f'Comparison of {metric}')\n",
    "        \n",
    "    fig.suptitle('Performance Metrics: Training vs Test Set', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'train_test_metrics_summary_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 3. Create a 2D comparison plot for each parameter\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, param in enumerate(cam.labels):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot training samples\n",
    "        ax.errorbar(\n",
    "            train_theta_np[:, i], \n",
    "            train_perc[:, 1, i], \n",
    "            yerr=[train_perc[:, 1, i] - train_perc[:, 0, i], train_perc[:, 2, i] - train_perc[:, 1, i]],\n",
    "            fmt='o', alpha=0.4, capsize=3, label='Training', color='blue'\n",
    "        )\n",
    "        \n",
    "        # Plot test samples\n",
    "        ax.errorbar(\n",
    "            test_theta_np[:, i], \n",
    "            test_perc[:, 1, i], \n",
    "            yerr=[test_perc[:, 1, i] - test_perc[:, 0, i], test_perc[:, 2, i] - test_perc[:, 1, i]],\n",
    "            fmt='o', alpha=0.4, capsize=3, label='Test', color='red'\n",
    "        )\n",
    "        \n",
    "        # Add diagonal line\n",
    "        all_theta = np.concatenate([train_theta_np[:, i], test_theta_np[:, i]])\n",
    "        all_medians = np.concatenate([train_perc[:, 1, i], test_perc[:, 1, i]])\n",
    "        param_min = min(all_theta.min(), all_medians.min())\n",
    "        param_max = max(all_theta.max(), all_medians.max())\n",
    "        param_range = param_max - param_min\n",
    "        param_min -= 0.1 * param_range\n",
    "        param_max += 0.1 * param_range\n",
    "        ax.plot([param_min, param_max], [param_min, param_max], 'k--', alpha=0.7)\n",
    "        \n",
    "        # Get Train and Test R² for title\n",
    "        train_r2 = all_metrics[(all_metrics['Parameter'] == param) & \n",
    "                               (all_metrics['Dataset'] == 'Training')]['R²'].values[0]\n",
    "        test_r2 = all_metrics[(all_metrics['Parameter'] == param) & \n",
    "                             (all_metrics['Dataset'] == 'Test')]['R²'].values[0]\n",
    "        \n",
    "        ax.set_xlabel(f'True {param}')\n",
    "        ax.set_ylabel(f'Estimated {param}')\n",
    "        ax.set_title(f'{param}\\nTrain R² = {train_r2:.3f}, Test R² = {test_r2:.3f}')\n",
    "        \n",
    "        if i == 0:  # Only add legend to first subplot\n",
    "            ax.legend()\n",
    "        \n",
    "    # Remove any unused subplots\n",
    "    for i in range(len(cam.labels), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "        \n",
    "    fig.suptitle('Parameter Recovery: Training vs Test Set', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'train_test_recovery_comparison_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"Training vs. Test comparison visualizations saved in: {config_plots_dir}\")\n",
    "    \n",
    "    return all_metrics\n",
    "\n",
    "# Call the function to generate the comparison\n",
    "comparison_metrics = compare_train_test_performance()\n",
    "print(comparison_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def conduct_posterior_predictive_checks():\n",
    "    \"\"\"\n",
    "    Perform posterior predictive checks to validate model performance.\n",
    "    \n",
    "    This involves:\n",
    "    1. Sampling from the posterior distributions for test set observations\n",
    "    2. Using these parameter samples to simulate data (forward model)\n",
    "    3. Comparing simulated data to observed data\n",
    "    \n",
    "    Since we don't have direct access to the forward model, we'll:\n",
    "    - Analyze posterior coverage and calibration\n",
    "    - Check parameter correlations and degeneracies\n",
    "    - Evaluate how well the constraints match expected values\n",
    "    \"\"\"\n",
    "    # Set up CUDA if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Get test set data\n",
    "    test_x = x_all_scaled[test_mask]\n",
    "    test_theta = theta[test_mask]\n",
    "    \n",
    "    # Number of test samples to analyze\n",
    "    n_test_samples = min(50, test_theta.shape[0])\n",
    "    test_indices = np.random.choice(test_theta.shape[0], n_test_samples, replace=False)\n",
    "    \n",
    "    # Create directory for outputs\n",
    "    os.makedirs(config_plots_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Analyze posterior width vs error\n",
    "    # This checks if posterior width (uncertainty) correlates with actual error\n",
    "    \n",
    "    # Get samples for each test point\n",
    "    all_samples = []\n",
    "    for i in test_indices:\n",
    "        x_i = test_x[i].unsqueeze(0)\n",
    "        samples = posterior_ensemble.sample((1000,), x=x_i.to(device)).cpu().numpy()\n",
    "        all_samples.append(samples)\n",
    "    \n",
    "    all_samples = np.array(all_samples)\n",
    "    test_theta_selected = test_theta[test_indices].cpu().numpy()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    medians = np.median(all_samples, axis=1)\n",
    "    lower_bounds = np.percentile(all_samples, 16, axis=1)\n",
    "    upper_bounds = np.percentile(all_samples, 84, axis=1)\n",
    "    \n",
    "    # Calculate errors and uncertainties\n",
    "    errors = np.abs(medians - test_theta_selected)\n",
    "    uncertainties = (upper_bounds - lower_bounds) / 2\n",
    "    \n",
    "    # Visualize error vs uncertainty\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, param in enumerate(cam.labels):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot error vs uncertainty\n",
    "        ax.scatter(uncertainties[:, i], errors[:, i], alpha=0.7)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        corr = np.corrcoef(uncertainties[:, i], errors[:, i])[0, 1]\n",
    "        \n",
    "        # Add a diagonal line for reference\n",
    "        max_val = max(uncertainties[:, i].max(), errors[:, i].max()) * 1.1\n",
    "        ax.plot([0, max_val], [0, max_val], 'k--', alpha=0.5)\n",
    "        \n",
    "        ax.set_xlabel(f'Posterior Uncertainty (68% CI width/2) for {param}')\n",
    "        ax.set_ylabel(f'Absolute Error for {param}')\n",
    "        ax.set_title(f'{param}: Error vs. Uncertainty\\nCorrelation: {corr:.3f}')\n",
    "        \n",
    "    # Remove any unused subplots\n",
    "    for i in range(len(cam.labels), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "        \n",
    "    fig.suptitle('Posterior Calibration: Error vs. Uncertainty', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'error_vs_uncertainty_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 2. Check coverage: what fraction of true values fall within different CI levels?\n",
    "    # Ideal: the 68% CI should contain the true value 68% of the time\n",
    "    \n",
    "    # Check coverage at different percentile levels\n",
    "    percentile_levels = [50, 68, 90, 95, 99]\n",
    "    coverage_results = []\n",
    "    \n",
    "    for i, param in enumerate(cam.labels):\n",
    "        param_coverage = []\n",
    "        \n",
    "        for level in percentile_levels:\n",
    "            # Calculate lower and upper bounds for this confidence level\n",
    "            half_width = level / 2\n",
    "            lower = np.percentile(all_samples, 50 - half_width, axis=1)[:, i]\n",
    "            upper = np.percentile(all_samples, 50 + half_width, axis=1)[:, i]\n",
    "            \n",
    "            # Check if true values fall within bounds\n",
    "            within_bounds = np.logical_and(\n",
    "                test_theta_selected[:, i] >= lower,\n",
    "                test_theta_selected[:, i] <= upper\n",
    "            )\n",
    "            coverage = np.mean(within_bounds) * 100\n",
    "            \n",
    "            param_coverage.append({\n",
    "                'Parameter': param,\n",
    "                'Confidence Level': f'{level}%',\n",
    "                'Expected Coverage': level,\n",
    "                'Actual Coverage': coverage\n",
    "            })\n",
    "        \n",
    "        coverage_results.extend(param_coverage)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    coverage_df = pd.DataFrame(coverage_results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    coverage_df.to_csv(os.path.join(config_plots_dir, f'coverage_analysis_{name}.csv'), index=False)\n",
    "    \n",
    "    # Visualize coverage\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    for param in cam.labels:\n",
    "        param_data = coverage_df[coverage_df['Parameter'] == param]\n",
    "        ax.plot(param_data['Expected Coverage'], param_data['Actual Coverage'], \n",
    "               'o-', label=param)\n",
    "    \n",
    "    # Add diagonal line for perfect coverage\n",
    "    ax.plot([0, 100], [0, 100], 'k--', alpha=0.7, label='Perfect Coverage')\n",
    "    \n",
    "    ax.set_xlabel('Expected Coverage (%)')\n",
    "    ax.set_ylabel('Actual Coverage (%)')\n",
    "    ax.set_title('Posterior Coverage Analysis')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.savefig(os.path.join(config_plots_dir, f'coverage_analysis_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 3. Look for parameter degeneracies in the posterior\n",
    "    # For a randomly selected test point, visualize full posterior\n",
    "    selected_test_idx = test_indices[0]\n",
    "    x_selected = test_x[selected_test_idx].unsqueeze(0)\n",
    "    samples_selected = posterior_ensemble.sample((5000,), x=x_selected.to(device)).cpu().numpy()\n",
    "    \n",
    "    # Create corner plot\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    n_params = len(cam.labels)\n",
    "    gs = GridSpec(n_params, n_params, figure=fig)\n",
    "    \n",
    "    for i in range(n_params):\n",
    "        for j in range(n_params):\n",
    "            if i < j:  # Upper triangle - leave empty\n",
    "                continue\n",
    "                \n",
    "            ax = fig.add_subplot(gs[i, j])\n",
    "            \n",
    "            if i == j:  # Diagonal - plot marginal distributions\n",
    "                ax.hist(samples_selected[:, i], bins=30, alpha=0.7, density=True)\n",
    "                ax.axvline(test_theta[selected_test_idx, i].item(), color='red', linestyle='--')\n",
    "                \n",
    "                if i < n_params - 1:\n",
    "                    ax.set_xticklabels([])\n",
    "                else:\n",
    "                    ax.set_xlabel(cam.labels[i])\n",
    "                \n",
    "                ax.set_yticks([])\n",
    "                \n",
    "            else:  # Lower triangle - plot 2D distributions\n",
    "                ax.scatter(samples_selected[:, j], samples_selected[:, i], alpha=0.1, s=1)\n",
    "                ax.scatter(test_theta[selected_test_idx, j].item(), \n",
    "                          test_theta[selected_test_idx, i].item(), \n",
    "                          color='red', marker='*', s=100)\n",
    "                \n",
    "                if i < n_params - 1:\n",
    "                    ax.set_xticklabels([])\n",
    "                else:\n",
    "                    ax.set_xlabel(cam.labels[j])\n",
    "                    \n",
    "                if j > 0:\n",
    "                    ax.set_yticklabels([])\n",
    "                else:\n",
    "                    ax.set_ylabel(cam.labels[i])\n",
    "    \n",
    "    fig.suptitle('Posterior Parameter Degeneracies for a Test Point', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'parameter_degeneracies_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 4. Calculate Kullback-Leibler divergence between prior and posterior\n",
    "    # This measures how much information we've gained from the data\n",
    "    \n",
    "    # Generate prior samples\n",
    "    prior_samples = prior.sample((10000,)).cpu().numpy()\n",
    "    \n",
    "    # Calculate KL divergence for each parameter and each test point\n",
    "    kl_divergences = []\n",
    "    \n",
    "    # Use only a few test points to avoid excessive computation\n",
    "    for i in test_indices[:5]:\n",
    "        x_i = test_x[i].unsqueeze(0)\n",
    "        posterior_samples = posterior_ensemble.sample((10000,), x=x_i.to(device)).cpu().numpy()\n",
    "        \n",
    "        for j, param in enumerate(cam.labels):\n",
    "            # Use histogram approximation for KL divergence\n",
    "            bins = 50\n",
    "            prior_hist, bin_edges = np.histogram(prior_samples[:, j], bins=bins, density=True)\n",
    "            posterior_hist, _ = np.histogram(posterior_samples[:, j], bins=bin_edges, density=True)\n",
    "            \n",
    "            # Avoid division by zero and log of zero\n",
    "            prior_hist = np.maximum(prior_hist, 1e-10)\n",
    "            posterior_hist = np.maximum(posterior_hist, 1e-10)\n",
    "            \n",
    "            # KL divergence: sum(p(x) * log(p(x)/q(x)))\n",
    "            kl = np.sum(posterior_hist * np.log(posterior_hist / prior_hist)) * (bin_edges[1] - bin_edges[0])\n",
    "            \n",
    "            kl_divergences.append({\n",
    "                'Test Point': i,\n",
    "                'Parameter': param,\n",
    "                'KL Divergence': kl\n",
    "            })\n",
    "    \n",
    "    kl_df = pd.DataFrame(kl_divergences)\n",
    "    \n",
    "    # Visualize KL divergence\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Parameter', y='KL Divergence', data=kl_df)\n",
    "    plt.title('Information Gain (KL Divergence) from Prior to Posterior')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'kl_divergence_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Compute credible interval width for each parameter\n",
    "    # This shows which parameters are well-constrained vs poorly-constrained\n",
    "    \n",
    "    ci_widths = []\n",
    "    \n",
    "    for i, param in enumerate(cam.labels):\n",
    "        for ci_level in [68, 95]:\n",
    "            half_width = ci_level / 2\n",
    "            lower_percentile = 50 - half_width\n",
    "            upper_percentile = 50 + half_width\n",
    "            \n",
    "            # Calculate width for prior\n",
    "            prior_lower = np.percentile(prior_samples[:, i], lower_percentile)\n",
    "            prior_upper = np.percentile(prior_samples[:, i], upper_percentile)\n",
    "            prior_width = prior_upper - prior_lower\n",
    "            \n",
    "            # Calculate width for each test point\n",
    "            posterior_widths = []\n",
    "            \n",
    "            for j in test_indices:\n",
    "                x_j = test_x[j].unsqueeze(0)\n",
    "                posterior_samples = posterior_ensemble.sample((1000,), x=x_j.to(device)).cpu().numpy()\n",
    "                \n",
    "                posterior_lower = np.percentile(posterior_samples[:, i], lower_percentile)\n",
    "                posterior_upper = np.percentile(posterior_samples[:, i], upper_percentile)\n",
    "                posterior_width = posterior_upper - posterior_lower\n",
    "                \n",
    "                posterior_widths.append(posterior_width)\n",
    "            \n",
    "            # Average width across test points\n",
    "            avg_posterior_width = np.mean(posterior_widths)\n",
    "            \n",
    "            # Width reduction (prior to posterior)\n",
    "            width_reduction = 1 - (avg_posterior_width / prior_width)\n",
    "            \n",
    "            ci_widths.append({\n",
    "                'Parameter': param,\n",
    "                'CI Level': f'{ci_level}%',\n",
    "                'Prior Width': prior_width,\n",
    "                'Posterior Width': avg_posterior_width,\n",
    "                'Width Reduction': width_reduction * 100  # as percentage\n",
    "            })\n",
    "    \n",
    "    ci_width_df = pd.DataFrame(ci_widths)\n",
    "    \n",
    "    # Save to CSV\n",
    "    ci_width_df.to_csv(os.path.join(config_plots_dir, f'credible_interval_analysis_{name}.csv'), index=False)\n",
    "    \n",
    "    # Visualize width reduction\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Only use 68% CI for clarity\n",
    "    plot_data = ci_width_df[ci_width_df['CI Level'] == '68%']\n",
    "    \n",
    "    sns.barplot(x='Parameter', y='Width Reduction', data=plot_data)\n",
    "    plt.title('Parameter Constraint Improvement (Prior to Posterior)')\n",
    "    plt.ylabel('Width Reduction (%)')\n",
    "    plt.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'constraint_improvement_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Rank parameters by recoverability\n",
    "    # Combine multiple metrics to assess overall parameter recovery quality\n",
    "    \n",
    "    # Use metrics we've already calculated\n",
    "    param_metrics = []\n",
    "    \n",
    "    for i, param in enumerate(cam.labels):\n",
    "        # Average error\n",
    "        avg_error = np.mean(errors[:, i])\n",
    "        \n",
    "        # Average uncertainty\n",
    "        avg_uncertainty = np.mean(uncertainties[:, i])\n",
    "        \n",
    "        # Error-to-uncertainty ratio (closer to 1 is better)\n",
    "        err_unc_ratio = avg_error / avg_uncertainty\n",
    "        \n",
    "        # Width reduction from prior\n",
    "        width_red = plot_data[plot_data['Parameter'] == param]['Width Reduction'].values[0]\n",
    "        \n",
    "        # Actual vs. expected coverage at 68%\n",
    "        coverage_at_68 = coverage_df[(coverage_df['Parameter'] == param) & \n",
    "                                    (coverage_df['Confidence Level'] == '68%')]['Actual Coverage'].values[0]\n",
    "        coverage_error = abs(coverage_at_68 - 68)\n",
    "        \n",
    "        # Combined score (lower is better)\n",
    "        # Normalize each component to [0,1] range before combining\n",
    "        combined_score = (\n",
    "            err_unc_ratio / 2 +  # Penalize ratio far from 1 (0 is perfect)\n",
    "            (1 - width_red / 100) +  # Higher width reduction is better\n",
    "            coverage_error / 68  # Lower coverage error is better\n",
    "        ) / 3\n",
    "        \n",
    "        param_metrics.append({\n",
    "            'Parameter': param,\n",
    "            'Avg Error': avg_error,\n",
    "            'Avg Uncertainty': avg_uncertainty,\n",
    "            'Error/Uncertainty Ratio': err_unc_ratio,\n",
    "            'Width Reduction (%)': width_red,\n",
    "            'Coverage Error': coverage_error,\n",
    "            'Combined Score': combined_score\n",
    "        })\n",
    "    \n",
    "    param_metrics_df = pd.DataFrame(param_metrics)\n",
    "    param_metrics_df = param_metrics_df.sort_values('Combined Score')\n",
    "    \n",
    "    # Save metrics\n",
    "    param_metrics_df.to_csv(os.path.join(config_plots_dir, f'parameter_ranking_{name}.csv'), index=False)\n",
    "    \n",
    "    # Visualization of parameter ranking\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Parameter', y='Combined Score', data=param_metrics_df, \n",
    "               order=param_metrics_df['Parameter'])\n",
    "    plt.title('Parameter Recovery Quality (Lower Score is Better)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'parameter_ranking_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Final summary table\n",
    "    summary = pd.DataFrame({\n",
    "        'Parameter': cam.labels,\n",
    "        'Best Constrained?': param_metrics_df['Parameter'].iloc[0] == np.array(cam.labels),\n",
    "        'Width Reduction (%)': [param_metrics_df[param_metrics_df['Parameter'] == p]['Width Reduction (%)'].values[0] \n",
    "                               for p in cam.labels],\n",
    "        'Coverage at 68% CI': [coverage_df[(coverage_df['Parameter'] == p) & \n",
    "                                         (coverage_df['Confidence Level'] == '68%')]['Actual Coverage'].values[0] \n",
    "                              for p in cam.labels],\n",
    "        'Error/Uncertainty Ratio': [param_metrics_df[param_metrics_df['Parameter'] == p]['Error/Uncertainty Ratio'].values[0] \n",
    "                                   for p in cam.labels]\n",
    "    })\n",
    "    \n",
    "    summary.to_csv(os.path.join(config_plots_dir, f'parameter_summary_{name}.csv'), index=False)\n",
    "    print(f\"Posterior predictive checks completed. Results saved in: {config_plots_dir}\")\n",
    "    \n",
    "    return summary, coverage_df, param_metrics_df\n",
    "\n",
    "# Execute the analysis\n",
    "summary, coverage_df, param_metrics_df = conduct_posterior_predictive_checks()\n",
    "print(\"\\nParameter Performance Summary:\")\n",
    "print(summary)\n",
    "\n",
    "print(\"\\nBest recovered parameters (ranked):\")\n",
    "print(param_metrics_df[['Parameter', 'Combined Score']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
