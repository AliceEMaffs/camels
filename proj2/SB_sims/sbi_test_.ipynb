{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/xray15/aem2/envs/camels/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model in  /disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/lf_only/\n",
      "Saving plots in  /disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/lfs_only/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import GPUtil\n",
    "import itertools\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import joblib\n",
    "import ili  # Import ili for the SBI functionality\n",
    "from ili.dataloaders import NumpyLoader\n",
    "from ili.inference import InferenceRunner\n",
    "from ili.validation.metrics import PosteriorCoverage, PlotSinglePosterior\n",
    "\n",
    "from sbi.utils.user_input_checks import process_prior\n",
    "\n",
    "sys.path.append(\"/disk/xray15/aem2/camels/proj2\")\n",
    "from setup_params_1P import plot_uvlf, plot_colour\n",
    "from setup_params_SB import *\n",
    "from priors_SB import initialise_priors_SB28\n",
    "from variables_config_28 import uvlf_limits, n_bins_lf, colour_limits, n_bins_colour\n",
    "# parameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = \"IllustrisTNG\"\n",
    "spec_type = \"attenuated\"\n",
    "sps = \"BC03\"\n",
    "snap = [\"044\"]\n",
    "bands = \"all\" # or just GALEX?\n",
    "\n",
    "colours = False  \n",
    "luminosity_functions = True\n",
    "name = f\"{model}_{bands}_{sps}_{spec_type}_{n_bins_lf}_{n_bins_colour}\"\n",
    "\n",
    "cam = camels(model=model, sim_set='SB28')\n",
    "\n",
    "if colours and not luminosity_functions:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/colours_only/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/colours_only/\"\n",
    "    \n",
    "elif luminosity_functions and not colours:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/lf_only/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/lfs_only/\"\n",
    "\n",
    "elif colours and luminosity_functions:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/colours_lfs/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/colours_lfs/\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"At least one of colours or luminosity_functions must be True\")\n",
    "\n",
    "print(\"Saving model in \", model_out_dir)\n",
    "print(\"Saving plots in \", plots_out_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-25, -14)\n"
     ]
    }
   ],
   "source": [
    "print(uvlf_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available simulations: ['SB28_0', 'SB28_1', 'SB28_10', 'SB28_100', 'SB28_1000', 'SB28_1001', 'SB28_1002', 'SB28_1003', 'SB28_1004', 'SB28_1005', 'SB28_1006', 'SB28_1007', 'SB28_1008', 'SB28_1009', 'SB28_101', 'SB28_1010', 'SB28_1011', 'SB28_1012', 'SB28_1013', 'SB28_1014', 'SB28_1015', 'SB28_1016', 'SB28_1017', 'SB28_1018', 'SB28_1019', 'SB28_102', 'SB28_1020', 'SB28_1021', 'SB28_1022', 'SB28_1023', 'SB28_1024', 'SB28_1025', 'SB28_1026', 'SB28_1027', 'SB28_1028', 'SB28_1029', 'SB28_103', 'SB28_1030', 'SB28_1031', 'SB28_1032', 'SB28_1033', 'SB28_1034', 'SB28_1035', 'SB28_1036', 'SB28_1037', 'SB28_1038', 'SB28_1039', 'SB28_104', 'SB28_1040', 'SB28_1041', 'SB28_1042', 'SB28_1043', 'SB28_1044', 'SB28_1045', 'SB28_1046', 'SB28_1047', 'SB28_1048', 'SB28_1049', 'SB28_105', 'SB28_1050', 'SB28_1051', 'SB28_1052', 'SB28_1053', 'SB28_1054', 'SB28_1055', 'SB28_1056', 'SB28_1057', 'SB28_1058', 'SB28_1059', 'SB28_106', 'SB28_1060', 'SB28_1061', 'SB28_1062', 'SB28_1063', 'SB28_1064', 'SB28_1065', 'SB28_1066', 'SB28_1067', 'SB28_1068', 'SB28_1069', 'SB28_107', 'SB28_1070', 'SB28_1071', 'SB28_1072', 'SB28_1073', 'SB28_1074', 'SB28_1075', 'SB28_1076', 'SB28_1077', 'SB28_1078', 'SB28_1079', 'SB28_108', 'SB28_1080', 'SB28_1081', 'SB28_1082', 'SB28_1083', 'SB28_1084', 'SB28_1085', 'SB28_1086', 'SB28_1087', 'SB28_1088', 'SB28_1089', 'SB28_109', 'SB28_1090', 'SB28_1091', 'SB28_1092', 'SB28_1093', 'SB28_1094', 'SB28_1095', 'SB28_1096', 'SB28_1097', 'SB28_1098', 'SB28_1099', 'SB28_11', 'SB28_110', 'SB28_1100', 'SB28_1101', 'SB28_1102', 'SB28_1103', 'SB28_1104', 'SB28_1105', 'SB28_1106', 'SB28_1107', 'SB28_1108', 'SB28_1109', 'SB28_111', 'SB28_1110', 'SB28_1111', 'SB28_1112', 'SB28_1113', 'SB28_1114', 'SB28_1115', 'SB28_1116', 'SB28_1117', 'SB28_1118', 'SB28_1119', 'SB28_112', 'SB28_1120', 'SB28_1121', 'SB28_1122', 'SB28_1123', 'SB28_1124', 'SB28_1125', 'SB28_1126', 'SB28_1127', 'SB28_1128', 'SB28_1129', 'SB28_113', 'SB28_1130', 'SB28_1131', 'SB28_1132', 'SB28_1133', 'SB28_1134', 'SB28_1135', 'SB28_1136', 'SB28_1137', 'SB28_1138', 'SB28_1139', 'SB28_114', 'SB28_1140', 'SB28_1141', 'SB28_1142', 'SB28_1143', 'SB28_1144', 'SB28_1145', 'SB28_1146', 'SB28_1147', 'SB28_1148', 'SB28_1149', 'SB28_115', 'SB28_1150', 'SB28_1151', 'SB28_1152', 'SB28_1153', 'SB28_1154', 'SB28_1155', 'SB28_1156', 'SB28_1157', 'SB28_1158', 'SB28_1159', 'SB28_116', 'SB28_1160', 'SB28_1161', 'SB28_1162', 'SB28_1163', 'SB28_1164', 'SB28_1165', 'SB28_1166', 'SB28_1167', 'SB28_1168', 'SB28_1169', 'SB28_117', 'SB28_1170', 'SB28_1171', 'SB28_1172', 'SB28_1173', 'SB28_1174', 'SB28_1175', 'SB28_1176', 'SB28_1177', 'SB28_1178', 'SB28_1179', 'SB28_118', 'SB28_1180', 'SB28_1181', 'SB28_1182', 'SB28_1183', 'SB28_1184', 'SB28_1185', 'SB28_1186', 'SB28_1187', 'SB28_1188', 'SB28_1189', 'SB28_119', 'SB28_1190', 'SB28_1191', 'SB28_1192', 'SB28_1193', 'SB28_1194', 'SB28_1195', 'SB28_1196', 'SB28_1197', 'SB28_1198', 'SB28_1199', 'SB28_12', 'SB28_120', 'SB28_1200', 'SB28_1201', 'SB28_1202', 'SB28_1203', 'SB28_1204', 'SB28_1205', 'SB28_1206', 'SB28_1207', 'SB28_1208', 'SB28_1209', 'SB28_121', 'SB28_1210', 'SB28_1211', 'SB28_1212', 'SB28_1213', 'SB28_1214', 'SB28_1215', 'SB28_1216', 'SB28_1217', 'SB28_1218', 'SB28_1219', 'SB28_122', 'SB28_1220', 'SB28_1221', 'SB28_1222', 'SB28_1223', 'SB28_1224', 'SB28_1225', 'SB28_1226', 'SB28_1227', 'SB28_1228', 'SB28_1229', 'SB28_123', 'SB28_1230', 'SB28_1231', 'SB28_1232', 'SB28_1233', 'SB28_1234', 'SB28_1235', 'SB28_1236', 'SB28_1237', 'SB28_1238', 'SB28_1239', 'SB28_124', 'SB28_1240', 'SB28_1241', 'SB28_1242', 'SB28_1243', 'SB28_1244', 'SB28_1245', 'SB28_1246', 'SB28_1247', 'SB28_1248', 'SB28_1249', 'SB28_125', 'SB28_1250', 'SB28_1251', 'SB28_1252', 'SB28_1253', 'SB28_1254', 'SB28_1255', 'SB28_1256', 'SB28_1257', 'SB28_1258', 'SB28_1259', 'SB28_126', 'SB28_1260', 'SB28_1261', 'SB28_1262', 'SB28_1263', 'SB28_1264', 'SB28_1265', 'SB28_1266', 'SB28_1267', 'SB28_1268', 'SB28_1269', 'SB28_127', 'SB28_1270', 'SB28_1271', 'SB28_1272', 'SB28_1273', 'SB28_1274', 'SB28_1275', 'SB28_1276', 'SB28_1277', 'SB28_1278', 'SB28_1279', 'SB28_128', 'SB28_1280', 'SB28_1281', 'SB28_1282', 'SB28_1283', 'SB28_1284', 'SB28_1285', 'SB28_1286', 'SB28_1287', 'SB28_1288', 'SB28_1289', 'SB28_129', 'SB28_1290', 'SB28_1291', 'SB28_1292', 'SB28_1293', 'SB28_1294', 'SB28_1295', 'SB28_1296', 'SB28_1297', 'SB28_1298', 'SB28_1299', 'SB28_13', 'SB28_130', 'SB28_1300', 'SB28_1301', 'SB28_1302', 'SB28_1303', 'SB28_1304', 'SB28_1305', 'SB28_1306', 'SB28_1307', 'SB28_1308', 'SB28_1309', 'SB28_131', 'SB28_1310', 'SB28_1311', 'SB28_1312', 'SB28_1313', 'SB28_1314', 'SB28_1315', 'SB28_1316', 'SB28_1317', 'SB28_1318', 'SB28_1319', 'SB28_132', 'SB28_1320', 'SB28_1321', 'SB28_1322', 'SB28_1323', 'SB28_1324', 'SB28_1325', 'SB28_1326', 'SB28_1327', 'SB28_1328', 'SB28_1329', 'SB28_133', 'SB28_1330', 'SB28_1331', 'SB28_1332', 'SB28_1333', 'SB28_1334', 'SB28_1335', 'SB28_1336', 'SB28_1337', 'SB28_1338', 'SB28_1339', 'SB28_134', 'SB28_1340', 'SB28_1341', 'SB28_1342', 'SB28_1343', 'SB28_1344', 'SB28_1345', 'SB28_1346', 'SB28_1347', 'SB28_1348', 'SB28_1349', 'SB28_135', 'SB28_1350', 'SB28_1351', 'SB28_1352', 'SB28_1353', 'SB28_1354', 'SB28_1355', 'SB28_1356', 'SB28_1357', 'SB28_1358', 'SB28_1359', 'SB28_136', 'SB28_1360', 'SB28_1361', 'SB28_1362', 'SB28_1363', 'SB28_1364', 'SB28_1365', 'SB28_1366', 'SB28_1367', 'SB28_1368', 'SB28_1369', 'SB28_137', 'SB28_1370', 'SB28_1371', 'SB28_1372', 'SB28_1373', 'SB28_1374', 'SB28_1375', 'SB28_1376', 'SB28_1377', 'SB28_1378', 'SB28_1379', 'SB28_138', 'SB28_1380', 'SB28_1381', 'SB28_1382', 'SB28_1383', 'SB28_1384', 'SB28_1385', 'SB28_1386', 'SB28_1387', 'SB28_1388', 'SB28_1389', 'SB28_139', 'SB28_1390', 'SB28_1391', 'SB28_1392', 'SB28_1393', 'SB28_1394', 'SB28_1395', 'SB28_1396', 'SB28_1397', 'SB28_1398', 'SB28_1399', 'SB28_14', 'SB28_140', 'SB28_1400', 'SB28_1401', 'SB28_1402', 'SB28_1403', 'SB28_1404', 'SB28_1405', 'SB28_1406', 'SB28_1407', 'SB28_1408', 'SB28_1409', 'SB28_141', 'SB28_1410', 'SB28_1411', 'SB28_1412', 'SB28_1413', 'SB28_1414', 'SB28_1415', 'SB28_1416', 'SB28_1417', 'SB28_1418', 'SB28_1419', 'SB28_142', 'SB28_1420', 'SB28_1421', 'SB28_1422', 'SB28_1423', 'SB28_1424', 'SB28_1425', 'SB28_1426', 'SB28_1427', 'SB28_1428', 'SB28_1429', 'SB28_143', 'SB28_1430', 'SB28_1431', 'SB28_1432', 'SB28_1433', 'SB28_1434', 'SB28_1435', 'SB28_1436', 'SB28_1437', 'SB28_1438', 'SB28_1439', 'SB28_144', 'SB28_1440', 'SB28_1441', 'SB28_1442', 'SB28_1443', 'SB28_1444', 'SB28_1445', 'SB28_1446', 'SB28_1447', 'SB28_1448', 'SB28_1449', 'SB28_145', 'SB28_1450', 'SB28_1451', 'SB28_1452', 'SB28_1453', 'SB28_1454', 'SB28_1455', 'SB28_1456', 'SB28_1457', 'SB28_1458', 'SB28_1459', 'SB28_146', 'SB28_1460', 'SB28_1461', 'SB28_1462', 'SB28_1463', 'SB28_1464', 'SB28_1465', 'SB28_1466', 'SB28_1467', 'SB28_1468', 'SB28_1469', 'SB28_147', 'SB28_1470', 'SB28_1471', 'SB28_1472', 'SB28_1473', 'SB28_1474', 'SB28_1475', 'SB28_1476', 'SB28_1477', 'SB28_1478', 'SB28_1479', 'SB28_148', 'SB28_1480', 'SB28_1481', 'SB28_1482', 'SB28_1483', 'SB28_1484', 'SB28_1485', 'SB28_1486', 'SB28_1487', 'SB28_1488', 'SB28_1489', 'SB28_149', 'SB28_1490', 'SB28_1491', 'SB28_1492', 'SB28_1493', 'SB28_1494', 'SB28_1495', 'SB28_1496', 'SB28_1497', 'SB28_1498', 'SB28_1499', 'SB28_15', 'SB28_150', 'SB28_1500', 'SB28_1501', 'SB28_1502', 'SB28_1503', 'SB28_1504', 'SB28_1505', 'SB28_1506', 'SB28_1507', 'SB28_1508', 'SB28_1509', 'SB28_151', 'SB28_1510', 'SB28_1511', 'SB28_1512', 'SB28_1513', 'SB28_1514', 'SB28_1515', 'SB28_1516', 'SB28_1517', 'SB28_1518', 'SB28_1519', 'SB28_152', 'SB28_1520', 'SB28_1521', 'SB28_1522', 'SB28_1523', 'SB28_1524', 'SB28_1525', 'SB28_1526', 'SB28_1527', 'SB28_1528', 'SB28_1529', 'SB28_153', 'SB28_1530', 'SB28_1531', 'SB28_1532', 'SB28_1533', 'SB28_1534', 'SB28_1535', 'SB28_1536', 'SB28_1537', 'SB28_1538', 'SB28_1539', 'SB28_154', 'SB28_1540', 'SB28_1541', 'SB28_1542', 'SB28_1543', 'SB28_1544', 'SB28_1545', 'SB28_1546', 'SB28_1547', 'SB28_1548', 'SB28_1549', 'SB28_155', 'SB28_1550', 'SB28_1551', 'SB28_1552', 'SB28_1553', 'SB28_1554', 'SB28_1555', 'SB28_1556', 'SB28_1557', 'SB28_1558', 'SB28_1559', 'SB28_156', 'SB28_1560', 'SB28_1561', 'SB28_1562', 'SB28_1563', 'SB28_1564', 'SB28_1565', 'SB28_1566', 'SB28_1567', 'SB28_1568', 'SB28_1569', 'SB28_157', 'SB28_1570', 'SB28_1571', 'SB28_1572', 'SB28_1573', 'SB28_1574', 'SB28_1575', 'SB28_1576', 'SB28_1577', 'SB28_1578', 'SB28_1579', 'SB28_158', 'SB28_1580', 'SB28_1581', 'SB28_1582', 'SB28_1583', 'SB28_1584', 'SB28_1585', 'SB28_1586', 'SB28_1587', 'SB28_1588', 'SB28_1589', 'SB28_159', 'SB28_1590', 'SB28_1591', 'SB28_1592', 'SB28_1593', 'SB28_1594', 'SB28_1595', 'SB28_1596', 'SB28_1597', 'SB28_1598', 'SB28_1599', 'SB28_16', 'SB28_160', 'SB28_1600', 'SB28_1601', 'SB28_1602', 'SB28_1603', 'SB28_1604', 'SB28_1605', 'SB28_1606', 'SB28_1607', 'SB28_1608', 'SB28_1609', 'SB28_161', 'SB28_1610', 'SB28_1611', 'SB28_1612', 'SB28_1613', 'SB28_1614', 'SB28_1615', 'SB28_1616', 'SB28_1617', 'SB28_1618', 'SB28_1619', 'SB28_162', 'SB28_1620', 'SB28_1621', 'SB28_1622', 'SB28_1623', 'SB28_1624', 'SB28_1625', 'SB28_1626', 'SB28_1627', 'SB28_1628', 'SB28_1629', 'SB28_163', 'SB28_1630', 'SB28_1631', 'SB28_1632', 'SB28_1633', 'SB28_1634', 'SB28_1635', 'SB28_1636', 'SB28_1637', 'SB28_1638', 'SB28_1639', 'SB28_164', 'SB28_1640', 'SB28_1641', 'SB28_1642', 'SB28_1643', 'SB28_1644', 'SB28_1645', 'SB28_1646', 'SB28_1647', 'SB28_1648', 'SB28_1649', 'SB28_165', 'SB28_1650', 'SB28_1651', 'SB28_1652', 'SB28_1653', 'SB28_1654', 'SB28_1655', 'SB28_1656', 'SB28_1657', 'SB28_1658', 'SB28_1659', 'SB28_166', 'SB28_1660', 'SB28_1661', 'SB28_1662', 'SB28_1663', 'SB28_1664', 'SB28_1665', 'SB28_1666', 'SB28_1667', 'SB28_1668', 'SB28_1669', 'SB28_167', 'SB28_1670', 'SB28_1671', 'SB28_1672', 'SB28_1673', 'SB28_1674', 'SB28_1675', 'SB28_1676', 'SB28_1677', 'SB28_1678', 'SB28_1679', 'SB28_168', 'SB28_1680', 'SB28_1681', 'SB28_1682', 'SB28_1683', 'SB28_1684', 'SB28_1685', 'SB28_1686', 'SB28_1687', 'SB28_1688', 'SB28_1689', 'SB28_169', 'SB28_1690', 'SB28_1691', 'SB28_1692', 'SB28_1693', 'SB28_1694', 'SB28_1695', 'SB28_1696', 'SB28_1697', 'SB28_1698', 'SB28_1699', 'SB28_17', 'SB28_170', 'SB28_1700', 'SB28_1701', 'SB28_1702', 'SB28_1703', 'SB28_1704', 'SB28_1705', 'SB28_1706', 'SB28_1707', 'SB28_1708', 'SB28_1709', 'SB28_171', 'SB28_1710', 'SB28_1711', 'SB28_1712', 'SB28_1713', 'SB28_1714', 'SB28_1715', 'SB28_1716', 'SB28_1717', 'SB28_1718', 'SB28_1719', 'SB28_172', 'SB28_1720', 'SB28_1721', 'SB28_1722', 'SB28_1723', 'SB28_1724', 'SB28_1725', 'SB28_1726', 'SB28_1727', 'SB28_1728', 'SB28_1729', 'SB28_173', 'SB28_1730', 'SB28_1731', 'SB28_1732', 'SB28_1733', 'SB28_1734', 'SB28_1735', 'SB28_1736', 'SB28_1737', 'SB28_1738', 'SB28_1739', 'SB28_174', 'SB28_1740', 'SB28_1741', 'SB28_1742', 'SB28_1743', 'SB28_1744', 'SB28_1745', 'SB28_1746', 'SB28_1747', 'SB28_1748', 'SB28_1749', 'SB28_175', 'SB28_1750', 'SB28_1751', 'SB28_1752', 'SB28_1753', 'SB28_1754', 'SB28_1755', 'SB28_1756', 'SB28_1757', 'SB28_1758', 'SB28_1759', 'SB28_176', 'SB28_1760', 'SB28_1761', 'SB28_1762', 'SB28_1763', 'SB28_1764', 'SB28_1765', 'SB28_1766', 'SB28_1767', 'SB28_1768', 'SB28_1769', 'SB28_177', 'SB28_1770', 'SB28_1771', 'SB28_1772', 'SB28_1773', 'SB28_1774', 'SB28_1775', 'SB28_1776', 'SB28_1777', 'SB28_1778', 'SB28_1779', 'SB28_178', 'SB28_1780', 'SB28_1781', 'SB28_1782', 'SB28_1783', 'SB28_1784', 'SB28_1785', 'SB28_1786', 'SB28_1787', 'SB28_1788', 'SB28_1789', 'SB28_179', 'SB28_1790', 'SB28_1791', 'SB28_1792', 'SB28_1793', 'SB28_1794', 'SB28_1795', 'SB28_1796', 'SB28_1797', 'SB28_1798', 'SB28_1799', 'SB28_18', 'SB28_180', 'SB28_1800', 'SB28_1801', 'SB28_1802', 'SB28_1803', 'SB28_1804', 'SB28_1805', 'SB28_1806', 'SB28_1807', 'SB28_1808', 'SB28_1809', 'SB28_181', 'SB28_1810', 'SB28_1811', 'SB28_1812', 'SB28_1813', 'SB28_1814', 'SB28_1815', 'SB28_1816', 'SB28_1817', 'SB28_1818', 'SB28_1819', 'SB28_182', 'SB28_1820', 'SB28_1821', 'SB28_1822', 'SB28_1823', 'SB28_1824', 'SB28_1825', 'SB28_1826', 'SB28_1827', 'SB28_1828', 'SB28_1829', 'SB28_183', 'SB28_1830', 'SB28_1831', 'SB28_1832', 'SB28_1833', 'SB28_1834', 'SB28_1835', 'SB28_1836', 'SB28_1837', 'SB28_1838', 'SB28_1839', 'SB28_184', 'SB28_1840', 'SB28_1841', 'SB28_1842', 'SB28_1843', 'SB28_1844', 'SB28_1845', 'SB28_1846', 'SB28_1847', 'SB28_1848', 'SB28_1849', 'SB28_185', 'SB28_1850', 'SB28_1851', 'SB28_1852', 'SB28_1853', 'SB28_1854', 'SB28_1855', 'SB28_1856', 'SB28_1857', 'SB28_1858', 'SB28_1859', 'SB28_186', 'SB28_1860', 'SB28_1861', 'SB28_1862', 'SB28_1863', 'SB28_1864', 'SB28_1865', 'SB28_1866', 'SB28_1867', 'SB28_1868', 'SB28_1869', 'SB28_187', 'SB28_1870', 'SB28_1871', 'SB28_1872', 'SB28_1873', 'SB28_1874', 'SB28_1875', 'SB28_1876', 'SB28_1877', 'SB28_1878', 'SB28_1879', 'SB28_188', 'SB28_1880', 'SB28_1881', 'SB28_1882', 'SB28_1883', 'SB28_1884', 'SB28_1885', 'SB28_1886', 'SB28_1887', 'SB28_1888', 'SB28_1889', 'SB28_189', 'SB28_1890', 'SB28_1891', 'SB28_1892', 'SB28_1893', 'SB28_1894', 'SB28_1895', 'SB28_1896', 'SB28_1897', 'SB28_1898', 'SB28_1899', 'SB28_19', 'SB28_190', 'SB28_1900', 'SB28_1901', 'SB28_1902', 'SB28_1903', 'SB28_1904', 'SB28_1905', 'SB28_1906', 'SB28_1907', 'SB28_1908', 'SB28_1909', 'SB28_191', 'SB28_1910', 'SB28_1911', 'SB28_1912', 'SB28_1913', 'SB28_1914', 'SB28_1915', 'SB28_1916', 'SB28_1917', 'SB28_1918', 'SB28_1919', 'SB28_192', 'SB28_1920', 'SB28_1921', 'SB28_1922', 'SB28_1923', 'SB28_1924', 'SB28_1925', 'SB28_1926', 'SB28_1927', 'SB28_1928', 'SB28_1929', 'SB28_193', 'SB28_1930', 'SB28_1931', 'SB28_1932', 'SB28_1933', 'SB28_1934', 'SB28_1935', 'SB28_1936', 'SB28_1937', 'SB28_1938', 'SB28_1939', 'SB28_194', 'SB28_1940', 'SB28_1941', 'SB28_1942', 'SB28_1943', 'SB28_1944', 'SB28_1945', 'SB28_1946', 'SB28_1947', 'SB28_1948', 'SB28_1949', 'SB28_195', 'SB28_1950', 'SB28_1951', 'SB28_1952', 'SB28_1953', 'SB28_1954', 'SB28_1955', 'SB28_1956', 'SB28_1957', 'SB28_1958', 'SB28_1959', 'SB28_196', 'SB28_1960', 'SB28_1961', 'SB28_1962', 'SB28_1963', 'SB28_1964', 'SB28_1965', 'SB28_1966', 'SB28_1967', 'SB28_1968', 'SB28_1969', 'SB28_197', 'SB28_1970', 'SB28_1971', 'SB28_1972', 'SB28_1973', 'SB28_1974', 'SB28_1975', 'SB28_1976', 'SB28_1977', 'SB28_1978', 'SB28_1979', 'SB28_198', 'SB28_1980', 'SB28_1981', 'SB28_1982', 'SB28_1983', 'SB28_1984', 'SB28_1985', 'SB28_1986', 'SB28_1987', 'SB28_1988', 'SB28_1989', 'SB28_199', 'SB28_1990', 'SB28_1991', 'SB28_1992', 'SB28_1993', 'SB28_1994', 'SB28_1995', 'SB28_1996', 'SB28_1997', 'SB28_1998', 'SB28_1999', 'SB28_2', 'SB28_20', 'SB28_200', 'SB28_2000', 'SB28_2001', 'SB28_2002', 'SB28_2003', 'SB28_2004', 'SB28_2005', 'SB28_2006', 'SB28_2007', 'SB28_2008', 'SB28_2009', 'SB28_201', 'SB28_2010', 'SB28_2011', 'SB28_2012', 'SB28_2013', 'SB28_2014', 'SB28_2015', 'SB28_2016', 'SB28_2017', 'SB28_2018', 'SB28_2019', 'SB28_202', 'SB28_2020', 'SB28_2021', 'SB28_2022', 'SB28_2023', 'SB28_2024', 'SB28_2025', 'SB28_2026', 'SB28_2027', 'SB28_2028', 'SB28_2029', 'SB28_203', 'SB28_2030', 'SB28_2031', 'SB28_2032', 'SB28_2033', 'SB28_2034', 'SB28_2035', 'SB28_2036', 'SB28_2037', 'SB28_2038', 'SB28_2039', 'SB28_204', 'SB28_2040', 'SB28_2041', 'SB28_2042', 'SB28_2043', 'SB28_2044', 'SB28_2045', 'SB28_2046', 'SB28_2047', 'SB28_205', 'SB28_206', 'SB28_207', 'SB28_208', 'SB28_209', 'SB28_21', 'SB28_210', 'SB28_211', 'SB28_212', 'SB28_213', 'SB28_214', 'SB28_215', 'SB28_216', 'SB28_217', 'SB28_218', 'SB28_219', 'SB28_22', 'SB28_220', 'SB28_221', 'SB28_222', 'SB28_223', 'SB28_224', 'SB28_225', 'SB28_226', 'SB28_227', 'SB28_228', 'SB28_229', 'SB28_23', 'SB28_230', 'SB28_231', 'SB28_232', 'SB28_233', 'SB28_234', 'SB28_235', 'SB28_236', 'SB28_237', 'SB28_238', 'SB28_239', 'SB28_24', 'SB28_240', 'SB28_241', 'SB28_242', 'SB28_243', 'SB28_244', 'SB28_245', 'SB28_246', 'SB28_247', 'SB28_248', 'SB28_249', 'SB28_25', 'SB28_250', 'SB28_251', 'SB28_252', 'SB28_253', 'SB28_254', 'SB28_255', 'SB28_256', 'SB28_257', 'SB28_258', 'SB28_259', 'SB28_26', 'SB28_260', 'SB28_261', 'SB28_262', 'SB28_263', 'SB28_264', 'SB28_265', 'SB28_266', 'SB28_267', 'SB28_268', 'SB28_269', 'SB28_27', 'SB28_270', 'SB28_271', 'SB28_272', 'SB28_273', 'SB28_274', 'SB28_275', 'SB28_276', 'SB28_277', 'SB28_278', 'SB28_279', 'SB28_28', 'SB28_280', 'SB28_281', 'SB28_282', 'SB28_283', 'SB28_284', 'SB28_285', 'SB28_286', 'SB28_287', 'SB28_288', 'SB28_289', 'SB28_29', 'SB28_290', 'SB28_291', 'SB28_292', 'SB28_293', 'SB28_294', 'SB28_295', 'SB28_296', 'SB28_297', 'SB28_298', 'SB28_299', 'SB28_3', 'SB28_30', 'SB28_300', 'SB28_301', 'SB28_302', 'SB28_303', 'SB28_304', 'SB28_305', 'SB28_306', 'SB28_307', 'SB28_308', 'SB28_309', 'SB28_31', 'SB28_310', 'SB28_311', 'SB28_312', 'SB28_313', 'SB28_314', 'SB28_315', 'SB28_316', 'SB28_317', 'SB28_318', 'SB28_319', 'SB28_32', 'SB28_320', 'SB28_321', 'SB28_322', 'SB28_323', 'SB28_324', 'SB28_325', 'SB28_326', 'SB28_327', 'SB28_328', 'SB28_329', 'SB28_33', 'SB28_330', 'SB28_331', 'SB28_332', 'SB28_333', 'SB28_334', 'SB28_335', 'SB28_336', 'SB28_337', 'SB28_338', 'SB28_339', 'SB28_34', 'SB28_340', 'SB28_341', 'SB28_342', 'SB28_343', 'SB28_344', 'SB28_345', 'SB28_346', 'SB28_347', 'SB28_348', 'SB28_349', 'SB28_35', 'SB28_350', 'SB28_351', 'SB28_352', 'SB28_353', 'SB28_354', 'SB28_355', 'SB28_356', 'SB28_357', 'SB28_358', 'SB28_359', 'SB28_36', 'SB28_360', 'SB28_361', 'SB28_362', 'SB28_363', 'SB28_364', 'SB28_365', 'SB28_366', 'SB28_367', 'SB28_368', 'SB28_369', 'SB28_37', 'SB28_370', 'SB28_371', 'SB28_372', 'SB28_373', 'SB28_374', 'SB28_375', 'SB28_376', 'SB28_377', 'SB28_378', 'SB28_379', 'SB28_38', 'SB28_380', 'SB28_381', 'SB28_382', 'SB28_383', 'SB28_384', 'SB28_385', 'SB28_386', 'SB28_387', 'SB28_388', 'SB28_389', 'SB28_39', 'SB28_390', 'SB28_391', 'SB28_392', 'SB28_393', 'SB28_394', 'SB28_395', 'SB28_396', 'SB28_397', 'SB28_398', 'SB28_399', 'SB28_4', 'SB28_40', 'SB28_400', 'SB28_401', 'SB28_402', 'SB28_403', 'SB28_404', 'SB28_405', 'SB28_406', 'SB28_407', 'SB28_408', 'SB28_409', 'SB28_41', 'SB28_410', 'SB28_411', 'SB28_412', 'SB28_413', 'SB28_414', 'SB28_415', 'SB28_416', 'SB28_417', 'SB28_418', 'SB28_419', 'SB28_42', 'SB28_420', 'SB28_421', 'SB28_422', 'SB28_423', 'SB28_424', 'SB28_425', 'SB28_426', 'SB28_427', 'SB28_428', 'SB28_429', 'SB28_43', 'SB28_430', 'SB28_431', 'SB28_432', 'SB28_433', 'SB28_434', 'SB28_435', 'SB28_436', 'SB28_437', 'SB28_438', 'SB28_439', 'SB28_44', 'SB28_440', 'SB28_441', 'SB28_442', 'SB28_443', 'SB28_444', 'SB28_445', 'SB28_446', 'SB28_447', 'SB28_448', 'SB28_449', 'SB28_45', 'SB28_450', 'SB28_451', 'SB28_452', 'SB28_453', 'SB28_454', 'SB28_455', 'SB28_456', 'SB28_457', 'SB28_458', 'SB28_459', 'SB28_46', 'SB28_460', 'SB28_461', 'SB28_462', 'SB28_463', 'SB28_464', 'SB28_465', 'SB28_466', 'SB28_467', 'SB28_468', 'SB28_469', 'SB28_47', 'SB28_470', 'SB28_471', 'SB28_472', 'SB28_473', 'SB28_474', 'SB28_475', 'SB28_476', 'SB28_477', 'SB28_478', 'SB28_479', 'SB28_48', 'SB28_480', 'SB28_481', 'SB28_482', 'SB28_483', 'SB28_484', 'SB28_485', 'SB28_486', 'SB28_487', 'SB28_488', 'SB28_489', 'SB28_49', 'SB28_490', 'SB28_491', 'SB28_492', 'SB28_493', 'SB28_494', 'SB28_495', 'SB28_496', 'SB28_497', 'SB28_498', 'SB28_499', 'SB28_5', 'SB28_50', 'SB28_500', 'SB28_501', 'SB28_502', 'SB28_503', 'SB28_504', 'SB28_505', 'SB28_506', 'SB28_507', 'SB28_508', 'SB28_509', 'SB28_51', 'SB28_510', 'SB28_511', 'SB28_512', 'SB28_513', 'SB28_514', 'SB28_515', 'SB28_516', 'SB28_517', 'SB28_518', 'SB28_519', 'SB28_52', 'SB28_520', 'SB28_521', 'SB28_522', 'SB28_523', 'SB28_524', 'SB28_525', 'SB28_526', 'SB28_527', 'SB28_528', 'SB28_529', 'SB28_53', 'SB28_530', 'SB28_531', 'SB28_532', 'SB28_533', 'SB28_534', 'SB28_535', 'SB28_536', 'SB28_537', 'SB28_538', 'SB28_539', 'SB28_54', 'SB28_540', 'SB28_541', 'SB28_542', 'SB28_543', 'SB28_544', 'SB28_545', 'SB28_546', 'SB28_547', 'SB28_548', 'SB28_549', 'SB28_55', 'SB28_550', 'SB28_551', 'SB28_552', 'SB28_553', 'SB28_554', 'SB28_555', 'SB28_556', 'SB28_557', 'SB28_558', 'SB28_559', 'SB28_56', 'SB28_560', 'SB28_561', 'SB28_562', 'SB28_563', 'SB28_564', 'SB28_565', 'SB28_566', 'SB28_567', 'SB28_568', 'SB28_569', 'SB28_57', 'SB28_570', 'SB28_571', 'SB28_572', 'SB28_573', 'SB28_574', 'SB28_575', 'SB28_576', 'SB28_577', 'SB28_578', 'SB28_579', 'SB28_58', 'SB28_580', 'SB28_581', 'SB28_582', 'SB28_583', 'SB28_584', 'SB28_585', 'SB28_586', 'SB28_587', 'SB28_588', 'SB28_589', 'SB28_59', 'SB28_590', 'SB28_591', 'SB28_592', 'SB28_593', 'SB28_594', 'SB28_595', 'SB28_596', 'SB28_597', 'SB28_598', 'SB28_599', 'SB28_6', 'SB28_60', 'SB28_600', 'SB28_601', 'SB28_602', 'SB28_603', 'SB28_604', 'SB28_605', 'SB28_606', 'SB28_607', 'SB28_608', 'SB28_609', 'SB28_61', 'SB28_610', 'SB28_611', 'SB28_612', 'SB28_613', 'SB28_614', 'SB28_615', 'SB28_616', 'SB28_617', 'SB28_618', 'SB28_619', 'SB28_62', 'SB28_620', 'SB28_621', 'SB28_622', 'SB28_623', 'SB28_624', 'SB28_625', 'SB28_626', 'SB28_627', 'SB28_628', 'SB28_629', 'SB28_63', 'SB28_630', 'SB28_631', 'SB28_632', 'SB28_633', 'SB28_634', 'SB28_635', 'SB28_636', 'SB28_637', 'SB28_638', 'SB28_639', 'SB28_64', 'SB28_640', 'SB28_641', 'SB28_642', 'SB28_643', 'SB28_644', 'SB28_645', 'SB28_646', 'SB28_647', 'SB28_648', 'SB28_649', 'SB28_65', 'SB28_650', 'SB28_651', 'SB28_652', 'SB28_653', 'SB28_654', 'SB28_655', 'SB28_656', 'SB28_657', 'SB28_658', 'SB28_659', 'SB28_66', 'SB28_660', 'SB28_661', 'SB28_662', 'SB28_663', 'SB28_664', 'SB28_665', 'SB28_666', 'SB28_667', 'SB28_668', 'SB28_669', 'SB28_67', 'SB28_670', 'SB28_671', 'SB28_672', 'SB28_673', 'SB28_674', 'SB28_675', 'SB28_676', 'SB28_677', 'SB28_678', 'SB28_679', 'SB28_68', 'SB28_680', 'SB28_681', 'SB28_682', 'SB28_683', 'SB28_684', 'SB28_685', 'SB28_686', 'SB28_687', 'SB28_688', 'SB28_689', 'SB28_69', 'SB28_690', 'SB28_691', 'SB28_692', 'SB28_693', 'SB28_694', 'SB28_695', 'SB28_696', 'SB28_697', 'SB28_698', 'SB28_699', 'SB28_7', 'SB28_70', 'SB28_700', 'SB28_701', 'SB28_702', 'SB28_703', 'SB28_704', 'SB28_705', 'SB28_706', 'SB28_707', 'SB28_708', 'SB28_709', 'SB28_71', 'SB28_710', 'SB28_711', 'SB28_712', 'SB28_713', 'SB28_714', 'SB28_715', 'SB28_716', 'SB28_717', 'SB28_718', 'SB28_719', 'SB28_72', 'SB28_720', 'SB28_721', 'SB28_722', 'SB28_723', 'SB28_724', 'SB28_725', 'SB28_726', 'SB28_727', 'SB28_728', 'SB28_729', 'SB28_73', 'SB28_730', 'SB28_731', 'SB28_732', 'SB28_733', 'SB28_734', 'SB28_735', 'SB28_736', 'SB28_737', 'SB28_738', 'SB28_739', 'SB28_74', 'SB28_740', 'SB28_741', 'SB28_742', 'SB28_743', 'SB28_744', 'SB28_745', 'SB28_746', 'SB28_747', 'SB28_748', 'SB28_749', 'SB28_75', 'SB28_750', 'SB28_751', 'SB28_752', 'SB28_753', 'SB28_754', 'SB28_755', 'SB28_756', 'SB28_757', 'SB28_758', 'SB28_759', 'SB28_76', 'SB28_760', 'SB28_761', 'SB28_762', 'SB28_763', 'SB28_764', 'SB28_765', 'SB28_766', 'SB28_767', 'SB28_768', 'SB28_769', 'SB28_77', 'SB28_770', 'SB28_771', 'SB28_772', 'SB28_773', 'SB28_774', 'SB28_775', 'SB28_776', 'SB28_777', 'SB28_778', 'SB28_779', 'SB28_78', 'SB28_780', 'SB28_781', 'SB28_782', 'SB28_783', 'SB28_784', 'SB28_785', 'SB28_786', 'SB28_787', 'SB28_788', 'SB28_789', 'SB28_79', 'SB28_790', 'SB28_791', 'SB28_792', 'SB28_793', 'SB28_794', 'SB28_795', 'SB28_796', 'SB28_797', 'SB28_798', 'SB28_799', 'SB28_8', 'SB28_80', 'SB28_800', 'SB28_801', 'SB28_802', 'SB28_803', 'SB28_804', 'SB28_805', 'SB28_806', 'SB28_807', 'SB28_808', 'SB28_809', 'SB28_81', 'SB28_810', 'SB28_811', 'SB28_812', 'SB28_813', 'SB28_814', 'SB28_815', 'SB28_816', 'SB28_817', 'SB28_818', 'SB28_819', 'SB28_82', 'SB28_820', 'SB28_821', 'SB28_822', 'SB28_823', 'SB28_824', 'SB28_825', 'SB28_826', 'SB28_827', 'SB28_828', 'SB28_829', 'SB28_83', 'SB28_830', 'SB28_831', 'SB28_832', 'SB28_833', 'SB28_834', 'SB28_835', 'SB28_836', 'SB28_837', 'SB28_838', 'SB28_839', 'SB28_84', 'SB28_840', 'SB28_841', 'SB28_842', 'SB28_843', 'SB28_844', 'SB28_845', 'SB28_846', 'SB28_847', 'SB28_848', 'SB28_849', 'SB28_85', 'SB28_850', 'SB28_851', 'SB28_852', 'SB28_853', 'SB28_854', 'SB28_855', 'SB28_856', 'SB28_857', 'SB28_858', 'SB28_859', 'SB28_86', 'SB28_860', 'SB28_861', 'SB28_862', 'SB28_863', 'SB28_864', 'SB28_865', 'SB28_866', 'SB28_867', 'SB28_868', 'SB28_869', 'SB28_87', 'SB28_870', 'SB28_871', 'SB28_872', 'SB28_873', 'SB28_874', 'SB28_875', 'SB28_876', 'SB28_877', 'SB28_878', 'SB28_879', 'SB28_88', 'SB28_880', 'SB28_881', 'SB28_882', 'SB28_883', 'SB28_884', 'SB28_885', 'SB28_886', 'SB28_887', 'SB28_888', 'SB28_889', 'SB28_89', 'SB28_890', 'SB28_891', 'SB28_892', 'SB28_893', 'SB28_894', 'SB28_895', 'SB28_896', 'SB28_897', 'SB28_898', 'SB28_899', 'SB28_9', 'SB28_90', 'SB28_900', 'SB28_901', 'SB28_902', 'SB28_903', 'SB28_904', 'SB28_905', 'SB28_906', 'SB28_907', 'SB28_908', 'SB28_909', 'SB28_91', 'SB28_910', 'SB28_911', 'SB28_912', 'SB28_913', 'SB28_914', 'SB28_915', 'SB28_916', 'SB28_917', 'SB28_918', 'SB28_919', 'SB28_92', 'SB28_920', 'SB28_921', 'SB28_922', 'SB28_923', 'SB28_924', 'SB28_925', 'SB28_926', 'SB28_927', 'SB28_928', 'SB28_929', 'SB28_93', 'SB28_930', 'SB28_931', 'SB28_932', 'SB28_933', 'SB28_934', 'SB28_935', 'SB28_936', 'SB28_937', 'SB28_938', 'SB28_939', 'SB28_94', 'SB28_940', 'SB28_941', 'SB28_942', 'SB28_943', 'SB28_944', 'SB28_945', 'SB28_946', 'SB28_947', 'SB28_948', 'SB28_949', 'SB28_95', 'SB28_950', 'SB28_951', 'SB28_952', 'SB28_953', 'SB28_954', 'SB28_955', 'SB28_956', 'SB28_957', 'SB28_958', 'SB28_959', 'SB28_96', 'SB28_960', 'SB28_961', 'SB28_962', 'SB28_963', 'SB28_964', 'SB28_965', 'SB28_966', 'SB28_967', 'SB28_968', 'SB28_969', 'SB28_97', 'SB28_970', 'SB28_971', 'SB28_972', 'SB28_973', 'SB28_974', 'SB28_975', 'SB28_976', 'SB28_977', 'SB28_978', 'SB28_979', 'SB28_98', 'SB28_980', 'SB28_981', 'SB28_982', 'SB28_983', 'SB28_984', 'SB28_985', 'SB28_986', 'SB28_987', 'SB28_988', 'SB28_989', 'SB28_99', 'SB28_990', 'SB28_991', 'SB28_992', 'SB28_993', 'SB28_994', 'SB28_995', 'SB28_996', 'SB28_997', 'SB28_998', 'SB28_999']\n",
      "\n",
      "Structure for first simulation:\n",
      "['GALEX FUV', 'GALEX NUV']\n"
     ]
    }
   ],
   "source": [
    "# Quick check of available data\n",
    "with h5py.File(\"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/photometry/alice_galex.h5\", \"r\") as hf:\n",
    "    print(\"Available simulations:\", list(hf.keys()))\n",
    "    sim_key = list(hf.keys())[0]  # First simulation\n",
    "    print(\"\\nStructure for first simulation:\")\n",
    "    print(list(hf[f\"{sim_key}/snap_044/BC03/photometry/luminosity/attenuated\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          #Name    Omega0    sigma8  WindEnergyIn1e51erg  RadioFeedbackFactor  \\\n",
      "0        SB28_0  0.352541  0.694742              3.85743             1.519210   \n",
      "1        SB28_1  0.172430  0.830154              1.03554             0.797734   \n",
      "2        SB28_2  0.234683  0.705844              9.61416             3.380650   \n",
      "3        SB28_3  0.440288  0.969259              2.14363             0.488165   \n",
      "4        SB28_4  0.457152  0.786733              1.38466             0.325727   \n",
      "...         ...       ...       ...                  ...                  ...   \n",
      "2043  SB28_2043  0.457334  0.970226              8.89733             0.607197   \n",
      "2044  SB28_2044  0.440496  0.786137              5.34131             0.880840   \n",
      "2045  SB28_2045  0.234475  0.938760              1.49723             1.873430   \n",
      "2046  SB28_2046  0.172613  0.612888             13.31250             0.363806   \n",
      "2047  SB28_2047  0.352358  0.862216              3.09935             3.331570   \n",
      "\n",
      "      VariableWindVelFactor  RadioFeedbackReiorientationFactor  OmegaBaryon  \\\n",
      "0                   9.09267                            14.2845     0.049404   \n",
      "1                   6.95693                            38.2374     0.031199   \n",
      "2                   3.77681                            24.6592     0.042995   \n",
      "3                  11.49660                            10.9550     0.062295   \n",
      "4                  13.19410                            17.1439     0.044425   \n",
      "...                     ...                                ...          ...   \n",
      "2043                3.75653                            39.5139     0.046041   \n",
      "2044                5.04789                            21.6844     0.063161   \n",
      "2045               13.26680                            13.5869     0.042461   \n",
      "2046                8.64069                            17.7159     0.029257   \n",
      "2047                5.64656                            33.6239     0.051052   \n",
      "\n",
      "      HubbleParam       n_s  ...  WindEnergyReductionExponent  WindDumpFactor  \\\n",
      "0        0.498145  0.795765  ...                      1.61148        0.435971   \n",
      "1        0.683693  1.142160  ...                      2.08951        0.612056   \n",
      "2        0.849506  0.923545  ...                      1.33988        0.878906   \n",
      "3        0.638970  0.964648  ...                      2.86478        0.253435   \n",
      "4        0.773770  0.900256  ...                      2.59030        0.944457   \n",
      "...           ...       ...  ...                          ...             ...   \n",
      "2043     0.513293  0.828903  ...                      1.88784        0.725070   \n",
      "2044     0.748997  1.143290  ...                      1.66098        0.472330   \n",
      "2045     0.534548  0.796116  ...                      2.13803        0.698393   \n",
      "2046     0.600362  0.965127  ...                      1.38572        0.818457   \n",
      "2047     0.789823  0.924799  ...                      2.90964        0.242951   \n",
      "\n",
      "      SeedBlackHoleMass  BlackHoleAccretionFactor  BlackHoleEddingtonFactor  \\\n",
      "0              0.000069                  1.111740                  2.613460   \n",
      "1              0.000094                  0.889955                  0.302262   \n",
      "2              0.000153                  0.364219                  8.235920   \n",
      "3              0.000041                  2.775960                  0.950425   \n",
      "4              0.000049                  0.286789                  1.121900   \n",
      "...                 ...                       ...                       ...   \n",
      "2043           0.000054                  0.327481                  4.790910   \n",
      "2044           0.000038                  2.229260                  0.222157   \n",
      "2045           0.000174                  0.453541                  1.920250   \n",
      "2046           0.000090                  0.718570                  0.698528   \n",
      "2047           0.000070                  1.376900                  6.051620   \n",
      "\n",
      "      BlackHoleFeedbackFactor  BlackHoleRadiativeEfficiency  QuasarThreshold  \\\n",
      "0                    0.039463                      0.225386         0.000269   \n",
      "1                    0.151352                      0.086231         0.022802   \n",
      "2                    0.099772                      0.648096         0.001458   \n",
      "3                    0.349945                      0.126670         0.004242   \n",
      "4                    0.032352                      0.460414         0.007438   \n",
      "...                       ...                           ...              ...   \n",
      "2043                 0.033553                      0.481742         0.001252   \n",
      "2044                 0.397001                      0.109068         0.000416   \n",
      "2045                 0.087029                      0.682619         0.006263   \n",
      "2046                 0.158683                      0.083655         0.000071   \n",
      "2047                 0.038037                      0.256220         0.036493   \n",
      "\n",
      "      QuasarThresholdPower   seed  \n",
      "0                 0.514648  20000  \n",
      "1                 2.620780  20001  \n",
      "2                 3.389560  20002  \n",
      "3                 1.494400  20003  \n",
      "4                 2.299550  20004  \n",
      "...                    ...    ...  \n",
      "2043              2.191720  22043  \n",
      "2044              1.012360  22044  \n",
      "2045              3.117210  22045  \n",
      "2046              2.887940  22046  \n",
      "2047              0.994068  22047  \n",
      "\n",
      "[2048 rows x 30 columns]\n",
      "                            ParamName  AbsMaxDiff  LogFlag  FiducialVal  \\\n",
      "0                              Omega0        0.20        0      0.30000   \n",
      "1                              sigma8        0.20        0      0.80000   \n",
      "2                 WindEnergyIn1e51erg        4.00        1      3.60000   \n",
      "3                 RadioFeedbackFactor        4.00        1      1.00000   \n",
      "4               VariableWindVelFactor        2.00        1      7.40000   \n",
      "5   RadioFeedbackReiorientationFactor        2.00        1     20.00000   \n",
      "6                         OmegaBaryon        0.02        0      0.04900   \n",
      "7                         HubbleParam        0.20        0      0.67110   \n",
      "8                                 n_s        0.20        0      0.96240   \n",
      "9                     MaxSfrTimescale        2.00        1      2.27000   \n",
      "10                 FactorForSofterEQS        3.00        1      0.30000   \n",
      "11                           IMFslope        0.50        0     -2.30000   \n",
      "12                  SNII_MinMass_Msun        4.00        0      8.00000   \n",
      "13                ThermalWindFraction        4.00        1      0.10000   \n",
      "14           VariableWindSpecMomentum     2000.00        0      0.00000   \n",
      "15              WindFreeTravelDensFac       10.00        1      0.05000   \n",
      "16                         MinWindVel      200.00        0    350.00000   \n",
      "17          WindEnergyReductionFactor        4.00        1      0.25000   \n",
      "18     WindEnergyReductionMetallicity        4.00        1      0.00200   \n",
      "19        WindEnergyReductionExponent        1.00        0      2.00000   \n",
      "20                     WindDumpFactor        0.40        0      0.60000   \n",
      "21                  SeedBlackHoleMass        3.16        1      0.00008   \n",
      "22           BlackHoleAccretionFactor        4.00        1      1.00000   \n",
      "23           BlackHoleEddingtonFactor       10.00        1      1.00000   \n",
      "24            BlackHoleFeedbackFactor        4.00        1      0.10000   \n",
      "25       BlackHoleRadiativeEfficiency        4.00        1      0.20000   \n",
      "26                    QuasarThreshold       31.60        1      0.00200   \n",
      "27               QuasarThresholdPower        2.00        0      2.00000   \n",
      "\n",
      "        MinVal       MaxVal                                        Description  \n",
      "0     0.100000     0.500000                                        OmegaMatter  \n",
      "1     0.600000     1.000000                                             sigma8  \n",
      "2     0.900000    14.400000                       ASN1 - galactic winds energy  \n",
      "3     0.250000     4.000000                 AAGN1 - AGN FB kinetic mode energy  \n",
      "4     3.700000    14.800000                        ASN2 - galactic winds speed  \n",
      "5    10.000000    40.000000             AAGN2 - AGN FB kinetic mode burstiness  \n",
      "6     0.029000     0.069000                                        OmegaBaryon  \n",
      "7     0.471100     0.871100                                   Hubble parameter  \n",
      "8     0.762400     1.162400                      power spectrum spectral index  \n",
      "9     1.135000     4.540000                          gas consumption timescale  \n",
      "10    0.100000     0.900000                     softening factor for SH03 eEOS  \n",
      "11   -2.800000    -1.800000                              IMF slope above 1Msun  \n",
      "12    4.000000    12.000000                  minimum stellar mass that goes SN  \n",
      "13    0.025000     0.400000  fraction of wind energy injected thermally (ve...  \n",
      "14    0.000000  4000.000000              wind momentum per unit star-formation  \n",
      "15    0.005000     0.500000                         density of wind recoupling  \n",
      "16  150.000000   550.000000                                   wind speed floor  \n",
      "17    0.062500     1.000000  magnitude of wind energy dependence on metalli...  \n",
      "18    0.000500     0.008000            metallicity at which wind energy pivots  \n",
      "19    1.000000     3.000000  sharpness of wind energy dependence on metalli...  \n",
      "20    0.200000     1.000000                   (1 minus metal loading of winds)  \n",
      "21    0.000025     0.000253                                       seed BH mass  \n",
      "22    0.250000     4.000000                              Bondi rate multiplier  \n",
      "23    0.100000    10.000000  Eddington rate multiplier for the BH accretion...  \n",
      "24    0.025000     0.400000            high-accretion mode feedback efficiency  \n",
      "25    0.050000     0.800000                            BH radiative efficiency  \n",
      "26    0.000063     0.063200  Eddington ratio for transition between BH feed...  \n",
      "27    0.000000     4.000000  power-law in Weinberger+ 2017 eq.5 - steepness...  \n",
      "[[3.52541e-01 6.94742e-01 3.85743e+00 ... 2.25386e-01 2.69356e-04\n",
      "  5.14648e-01]\n",
      " [1.72430e-01 8.30154e-01 1.03554e+00 ... 8.62311e-02 2.28022e-02\n",
      "  2.62078e+00]\n",
      " [2.34683e-01 7.05844e-01 9.61416e+00 ... 6.48096e-01 1.45761e-03\n",
      "  3.38956e+00]\n",
      " ...\n",
      " [2.34475e-01 9.38760e-01 1.49723e+00 ... 6.82619e-01 6.26319e-03\n",
      "  3.11721e+00]\n",
      " [1.72613e-01 6.12888e-01 1.33125e+01 ... 8.36555e-02 7.09853e-05\n",
      "  2.88794e+00]\n",
      " [3.52358e-01 8.62216e-01 3.09935e+00 ... 2.56220e-01 3.64932e-02\n",
      "  9.94068e-01]]\n",
      "(2048, 28)\n",
      "Column names:\n",
      "['#Name', 'Omega0', 'sigma8', 'WindEnergyIn1e51erg', 'RadioFeedbackFactor', 'VariableWindVelFactor', 'RadioFeedbackReiorientationFactor', 'OmegaBaryon', 'HubbleParam', 'n_s', 'MaxSfrTimescale', 'FactorForSofterEQS', 'IMFslope', 'SNII_MinMass_Msun', 'ThermalWindFraction', 'VariableWindSpecMomentum', 'WindFreeTravelDensFac', 'MinWindVel', 'WindEnergyReductionFactor', 'WindEnergyReductionMetallicity', 'WindEnergyReductionExponent', 'WindDumpFactor', 'SeedBlackHoleMass', 'BlackHoleAccretionFactor', 'BlackHoleEddingtonFactor', 'BlackHoleFeedbackFactor', 'BlackHoleRadiativeEfficiency', 'QuasarThreshold', 'QuasarThresholdPower', 'seed']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(theta[:, \u001b[38;5;241m24\u001b[39m])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 23\u001b[0m     theta, x \u001b[38;5;241m=\u001b[39m \u001b[43mget_theta_x_SB\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mluminosity_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mluminosity_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolours\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43muvlf_limits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muvlf_limits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolour_limits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolour_limits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_bins_lf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bins_lf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_bins_colour\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bins_colour\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(theta\u001b[38;5;241m.\u001b[39mshape, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# if __name__ == \"__main__\":\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#     theta, x = get_theta_x_SB(\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#         luminosity_functions=luminosity_functions,\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#         colours=colours  # This will now override the default\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#     print(theta.shape, x.shape)\u001b[39;00m\n",
      "File \u001b[0;32m/disk/xray15/aem2/camels/proj2/SB_sims/../setup_params_SB.py:395\u001b[0m, in \u001b[0;36mget_theta_x_SB\u001b[0;34m(spec_type, snap, sps, model, device, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_theta_x_SB\u001b[39m(\n\u001b[1;32m    387\u001b[0m     spec_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattenuated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    388\u001b[0m     snap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m044\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    393\u001b[0m ):\n\u001b[1;32m    394\u001b[0m     x \u001b[38;5;241m=\u001b[39m get_x_SB(spec_type\u001b[38;5;241m=\u001b[39mspec_type, snap\u001b[38;5;241m=\u001b[39msnap, sps\u001b[38;5;241m=\u001b[39msps, model\u001b[38;5;241m=\u001b[39mmodel, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 395\u001b[0m     theta \u001b[38;5;241m=\u001b[39m \u001b[43mget_theta_SB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Convert x list to proper array format\u001b[39;00m\n\u001b[1;32m    398\u001b[0m     x_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mhstack(_x) \u001b[38;5;28;01mfor\u001b[39;00m _x \u001b[38;5;129;01min\u001b[39;00m x])\n",
      "File \u001b[0;32m/disk/xray15/aem2/camels/proj2/SB_sims/../setup_params_SB.py:235\u001b[0m, in \u001b[0;36mget_theta_SB\u001b[0;34m(model, device)\u001b[0m\n\u001b[1;32m    203\u001b[0m cam \u001b[38;5;241m=\u001b[39m camels(model\u001b[38;5;241m=\u001b[39mmodel, sim_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSB28\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    204\u001b[0m theta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m    205\u001b[0m     cam\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOmega0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,              \u001b[38;5;66;03m# Omega0\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     cam\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma8\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,              \u001b[38;5;66;03m# sigma8\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m     cam\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuasarThresholdPower\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;66;03m# Quasar Threshold Power\u001b[39;00m\n\u001b[1;32m    233\u001b[0m ])\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/xray15/aem2/envs/camels/lib/python3.8/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    292\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 293\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    297\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameter info file (df_info) is used for defining priors\n",
    "# the actual parameter values come from the camels class which reads CosmoAstroSeed_IllustrisTNG_L25n256_SB28.txt\n",
    "\n",
    "#  parameters defined here: /disk/xray15/aem2/data/28pams/IllustrisTNG/SB/CosmoAstroSeed_IllustrisTNG_L25n256_SB28.txt which is used for theta\n",
    "df_pars = pd.read_csv('/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/CosmoAstroSeed_IllustrisTNG_L25n256_SB28.txt', delim_whitespace=True)\n",
    "print(df_pars)\n",
    "\n",
    "\n",
    "# prior values come from this:\n",
    "df_info = pd.read_csv(\"/disk/xray15/aem2/data/28pams/Info_IllustrisTNG_L25n256_28params.txt\")\n",
    "print(df_info)\n",
    "\n",
    "theta = df_pars.iloc[:, 1:29].to_numpy()  # excluding 'name' column and 'seed' column\n",
    "\n",
    "print(theta)\n",
    "print(theta.shape)\n",
    "print(\"Column names:\")\n",
    "print(df_pars.columns.tolist())\n",
    "# plot the first one (omega0) to see shape of prior:\n",
    "plt.hist(theta[:, 24])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    theta, x = get_theta_x_SB(\n",
    "        luminosity_functions=luminosity_functions,\n",
    "        colours=colours,\n",
    "        uvlf_limits=uvlf_limits,\n",
    "        colour_limits=colour_limits,\n",
    "        n_bins_lf=n_bins_lf,\n",
    "        n_bins_colour=n_bins_colour\n",
    "    )\n",
    "    print(theta.shape, x.shape)\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     theta, x = get_theta_x_SB(\n",
    "#         luminosity_functions=luminosity_functions,\n",
    "#         colours=colours  # This will now override the default\n",
    "#     )\n",
    "#     print(theta.shape, x.shape)\n",
    "    \n",
    "if colours:\n",
    "    fig = plot_colour(x)\n",
    "    #plt.savefig('/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/colours_test/colour_check.png')\n",
    "    plt.show()\n",
    "\n",
    "if luminosity_functions:\n",
    "    fig = plot_uvlf(x)\n",
    "    #plt.savefig('/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/LFs_test/uvlf_check.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get the priors and data\n",
    "prior = initialise_priors_SB28(\n",
    "    df=df_info, \n",
    "    device=device,\n",
    "    astro=True,\n",
    "    dust=False  # no dust for testing. set to False to only get the 28 model parameters.\n",
    "    # with dust = True, prior has 32 dimensions (28 parameters + 4 dust parameters) \n",
    ")\n",
    "\n",
    "# process the data\n",
    "x_all = np.array([np.hstack(_x) for _x in x])\n",
    "x_all = torch.tensor(x_all, dtype=torch.float32, device=device)\n",
    "\n",
    "print(\"Theta shape:\", theta.shape)\n",
    "print(\"X shape:\", x_all.shape)\n",
    "\n",
    "\n",
    "# Move data to GPU as early as possible\n",
    "x_all = x_all.to(device)\n",
    "print('x_all:', x_all)\n",
    "\n",
    "theta = torch.tensor(theta, dtype=torch.float32, device=device)\n",
    "print('theta:', theta)\n",
    "\n",
    "# Handle NaN values and normalize while on GPU\n",
    "x_all_cpu = x_all.cpu().numpy()  # Only move to CPU when necessary for sklearn\n",
    "print('x_all_cpu:', x_all_cpu)\n",
    "\n",
    "print(\"Data shape before processing:\", x_all_cpu.shape)\n",
    "print(\"Number of values:\",(x_all_cpu).sum())\n",
    "print(\"Number of NaN values:\", np.isnan(x_all_cpu).sum())\n",
    "print(\"Number of infinite values:\", np.isinf(x_all_cpu).sum())\n",
    "\n",
    "\n",
    "# get rid of NaN/inf values, replace with small random noise\n",
    "nan_mask = np.isnan(x_all_cpu) | np.isinf(x_all_cpu)\n",
    "print('nan_mask:', nan_mask)\n",
    "\n",
    "\n",
    "if nan_mask.any():\n",
    "    x_all_cpu[nan_mask] = np.random.rand(np.sum(nan_mask)) * 1e-10\n",
    "\n",
    "print(\"Data shape before processing:\", x_all_cpu.shape)\n",
    "print(\"Number of NaN values:\", np.isnan(x_all_cpu).sum())\n",
    "print(\"Number of infinite values:\", np.isinf(x_all_cpu).sum())\n",
    "\n",
    "print('x_all_cpu:', x_all_cpu)\n",
    "\n",
    "\n",
    "# Normalize\n",
    "norm = Normalizer()\n",
    "\n",
    "# Option: Add small constant before normalizing\n",
    "epsilon = 1e-10  # Small constant\n",
    "x_all_shifted = x_all_cpu + epsilon\n",
    "x_all_normalized = norm.fit_transform(x_all_shifted)\n",
    "x_all = torch.tensor(x_all_normalized, dtype=torch.float32, device=device)\n",
    "\n",
    "print('x_all:', x_all)\n",
    "\n",
    "# Save normalizer\n",
    "joblib.dump(norm, f'/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/{name}_scaler.save')\n",
    "\n",
    "# Print final check\n",
    "print(\"Any NaN in normalized data:\", torch.isnan(x_all).any().item())\n",
    "print(\"Any inf in normalized data:\", torch.isinf(x_all).any().item())\n",
    "\n",
    "\n",
    "# make test mask\n",
    "test_mask = create_test_mask() # 10% testing\n",
    "train_mask = ~test_mask # 90% for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training arguments\n",
    "train_args = {\n",
    "    \"training_batch_size\": 64, # changed from 4 to 10 as dealing with more sims, want it to be faster for initial testing.\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"stop_after_epochs\": 600, # loss function. waits to see if things improve.\n",
    "    \"validation_fraction\": 0.15,  # creates another split within the training data for validation\n",
    "    \"max_num_epochs\": 2000  # Add maximum epochs\n",
    "}\n",
    "\n",
    "# Configure network\n",
    "hidden_features = 256 #neurons\n",
    "num_transforms = 8 #layers\n",
    "num_nets = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most recent best: uvlfs only\n",
    "# hf20_nt2_bs16_lr1e-05_nets3\n",
    "#renamed for quick screen test:\n",
    "train_args = {\n",
    "    \"training_batch_size\": 64,  # Increase batch size\n",
    "    \"learning_rate\": 1e-3,      # Slightly lower learning rate\n",
    "    \"stop_after_epochs\": 200,   # Stop earlier\n",
    "    \"validation_fraction\": 0.1,  # More validation data\n",
    "    \"max_num_epochs\": 1000,\n",
    "    \"clip_max_norm\": 5.0        # More aggressive gradient clipping\n",
    "}\n",
    "\n",
    "\n",
    "# Reduce model capacity\n",
    "hidden_features = 5  # Back to original\n",
    "num_transforms = 1    # Back to original\n",
    "num_nets = 1       # But keep more ensemble members\n",
    "\n",
    "# Simple base configuration\n",
    "nets = [ili.utils.load_nde_sbi(\n",
    "    engine=\"NPE\",\n",
    "    model=\"nsf\",\n",
    "    hidden_features=hidden_features,\n",
    "    num_transforms=num_transforms,\n",
    ") for _ in range(num_nets)]\n",
    "\n",
    "# Runner setup\n",
    "runner = InferenceRunner.load(\n",
    "    backend=\"sbi\",\n",
    "    engine=\"NPE\",\n",
    "    prior=prior,\n",
    "    nets=nets,\n",
    "    device=device,\n",
    "    train_args=train_args,\n",
    "    proposal=None,\n",
    "    out_dir=model_out_dir,\n",
    "    name=name\n",
    ")\n",
    "\n",
    "# Data loader\n",
    "loader = NumpyLoader(\n",
    "\n",
    "    x=x_all[train_mask].clone().detach(),\n",
    "    theta=theta[train_mask].clone().detach()\n",
    ")\n",
    "\n",
    "posterior_ensemble, summaries = runner(loader=loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# below is the hyper parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use vis.py to see which one is the best model and use the cell above to re run it to plot results (can also pull results from the grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define parameter grid\n",
    "# param_grid = {\n",
    "#     \"hidden_features\": [20, 40, 80],\n",
    "#     \"num_transforms\": [1, 2, 4, 8],\n",
    "#     \"training_batch_size\": [16, 32, 64],\n",
    "#     \"learning_rate\": [1e-4, 5e-5, 1e-5],\n",
    "#     \"num_nets\": [1, 2, 3]\n",
    "# }\n",
    "\n",
    "# # Create output directory with timestamp\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# results_dir = Path(f\"grid_search_results_{timestamp}\")\n",
    "# results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# # Store all results\n",
    "# all_results = []\n",
    "\n",
    "# # Generate all combinations of parameters\n",
    "# param_combinations = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "# def create_run_name(params):\n",
    "#     \"\"\"Create a descriptive run name from parameters\"\"\"\n",
    "#     return f\"hf{params['hidden_features']}_nt{params['num_transforms']}_bs{params['training_batch_size']}_lr{params['learning_rate']:.0e}_nets{params['num_nets']}\"\n",
    "\n",
    "# for i, params in enumerate(param_combinations):\n",
    "\n",
    "#     run_name = create_run_name(params)\n",
    "\n",
    "#     # Update training arguments\n",
    "#     train_args = {\n",
    "#         \"training_batch_size\": params[\"training_batch_size\"],\n",
    "#         \"learning_rate\": params[\"learning_rate\"],\n",
    "#         \"stop_after_epochs\": 100,\n",
    "#         \"validation_fraction\": 0.2,\n",
    "#         \"max_num_epochs\": 1000,\n",
    "#         \"clip_max_norm\": 5.0\n",
    "#     }\n",
    "    \n",
    "#     # Update model parameters\n",
    "#     hidden_features = params[\"hidden_features\"]\n",
    "#     num_transforms = params[\"num_transforms\"]\n",
    "#     num_nets = params[\"num_nets\"]\n",
    "    \n",
    "#     # Create networks\n",
    "#     nets = [ili.utils.load_nde_sbi(\n",
    "#         engine=\"NPE\",\n",
    "#         model=\"nsf\",\n",
    "#         hidden_features=hidden_features,\n",
    "#         num_transforms=num_transforms,\n",
    "#     ) for _ in range(num_nets)]\n",
    "    \n",
    "#     # Setup runner\n",
    "#     run_dir = results_dir / run_name\n",
    "#     run_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "#     runner = InferenceRunner.load(\n",
    "#         backend=\"sbi\",\n",
    "#         engine=\"NPE\",\n",
    "#         prior=prior,\n",
    "#         nets=nets,\n",
    "#         device=device,\n",
    "#         train_args=train_args,\n",
    "#         proposal=None,\n",
    "#         out_dir=run_dir,\n",
    "#         name=run_name\n",
    "#     )\n",
    "    \n",
    "#     # Data loader\n",
    "#     loader = NumpyLoader(\n",
    "#         x=x_all[train_mask].clone().detach(),\n",
    "#         theta=theta[train_mask].clone().detach()\n",
    "#     )\n",
    "    \n",
    "#     # Train model\n",
    "#     posterior_ensemble, summaries = runner(loader=loader)\n",
    "    \n",
    "#     # Store results\n",
    "#     result = {\n",
    "#         \"parameters\": params,\n",
    "#         \"summaries\": summaries,\n",
    "#         \"posterior_ensemble\": posterior_ensemble,\n",
    "#         \"train_args\": train_args\n",
    "#     }\n",
    "    \n",
    "#     all_results.append(result)\n",
    "    \n",
    "#     # Save individual run results\n",
    "#     with open(run_dir / \"results.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(result, f)\n",
    "    \n",
    "#     # Print progress\n",
    "#     print(f\"Completed run {i+1}/{len(param_combinations)}\")\n",
    "#     # print(f\"Parameters: {params}\")\n",
    "#     # print(f\"Validation loss: {summaries.get('validation_loss', 'N/A')}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "# # Save all results to a single file\n",
    "# with open(results_dir / \"all_results.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(all_results, f)\n",
    "\n",
    "# # Create a summary file\n",
    "# # with open(results_dir / \"summary.txt\", \"w\") as f:\n",
    "# #     f.write(\"Grid Search Results Summary\\n\")\n",
    "#     # f.write(f\"Timestamp: {timestamp}\\n\\n\")\n",
    "    \n",
    "#     # for i, result in enumerate(all_results):\n",
    "#         # f.write(f\"Run {i+1}:\\n\")\n",
    "#         # f.write(f\"Parameters: {result['parameters']}\\n\")\n",
    "#         # f.write(f\"Validation loss: {result['summaries'].get('validation_loss', 'N/A')}\\n\")\n",
    "#         # f.write(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hf20_nt2_bs16_lr1e-04_nets1.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_all[train_mask].clone().detach(),\n",
    "theta_train=theta[train_mask].clone().detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training_batch_size\", train_args[ \"training_batch_size\"])\n",
    "print(\"learning_rate\", train_args[ \"learning_rate\"])\n",
    "print(\"stop_after_epochs\", train_args[ \"stop_after_epochs\"])\n",
    "print(\"validation_fraction\", train_args[ \"validation_fraction\"])\n",
    "print(\"hidden_features\", hidden_features)\n",
    "print(\"num_transforms\", num_transforms)\n",
    "print(\"num_nets\", num_nets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_diagnostics(summaries):\n",
    "    \"\"\"Plot training diagnostics without empty subplots\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))  # Changed to 1 row, 2 columns\n",
    "    \n",
    "    # Loss curves\n",
    "    train_losses = summaries[0]['training_log_probs']\n",
    "    val_losses = summaries[0]['validation_log_probs']\n",
    "    epochs = range(len(train_losses))\n",
    "    \n",
    "    ax1.plot(epochs, train_losses, '-', label='Training', color='blue')\n",
    "    ax1.plot(epochs, val_losses, '--', label='Validation', color='red')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Log probability')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overfitting gap\n",
    "    gap = np.array(train_losses) - np.array(val_losses)\n",
    "    ax2.plot(epochs, gap, '-', color='purple')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss difference')\n",
    "    ax2.set_title('Overfitting Gap')\n",
    "    ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# refer back to training args in file name\n",
    "model_params = f\"batch{train_args['training_batch_size']}_\" \\\n",
    "               f\"lr{train_args['learning_rate']:.0e}_\" \\\n",
    "               f\"epochs{train_args['stop_after_epochs']}_\" \\\n",
    "               f\"val{train_args['validation_fraction']}_\" \\\n",
    "               f\"hidden{hidden_features}_\" \\\n",
    "               f\"transforms{num_transforms}\"\n",
    "\n",
    "# Use the function\n",
    "fig = plot_training_diagnostics(summaries)\n",
    "plt.savefig(os.path.join(plots_out_dir, f'loss_overfitting_{model_params}.png'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train/validation loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,4))\n",
    "c = list(mcolors.TABLEAU_COLORS)\n",
    "for i, m in enumerate(summaries):\n",
    "    ax.plot(m['training_log_probs'], ls='-', label=f\"{i}_train\", c=c[i])\n",
    "    ax.plot(m['validation_log_probs'], ls='--', label=f\"{i}_val\", c=c[i])\n",
    "ax.set_xlim(0)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Log probability')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will this work or do we have to use it explicitly?\n",
    "x_train=x_all[train_mask].clone().detach(),\n",
    "theta_train=theta[train_mask].clone().detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing samples from the entire test set to look at overall performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data\n",
    "x_test = x_all[test_mask]\n",
    "theta_test = theta[test_mask]\n",
    "\n",
    "# Number of samples to draw from posterior\n",
    "n_samples = 1000\n",
    "\n",
    "# Storage for predictions\n",
    "all_samples = []\n",
    "all_means = []\n",
    "all_stds = []\n",
    "\n",
    "# Generate posterior samples for each test point\n",
    "for i in range(len(x_test)):\n",
    "    # Get samples from the posterior\n",
    "    samples = posterior_ensemble.sample(\n",
    "        (n_samples,), \n",
    "        x=x_test[i].reshape(1, -1)\n",
    "    ).cpu().numpy()\n",
    "    \n",
    "    # Calculate mean and std of samples\n",
    "    mean = samples.mean(axis=0)\n",
    "    std = samples.std(axis=0)\n",
    "    \n",
    "    all_samples.append(samples)\n",
    "    all_means.append(mean)\n",
    "    all_stds.append(std)\n",
    "\n",
    "all_samples = np.array(all_samples)\n",
    "all_means = np.array(all_means)\n",
    "all_stds = np.array(all_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = df_pars.columns[1:29].tolist()  # excluding 'name' column\n",
    "\n",
    "fig, axes = plt.subplots(7, 4, figsize=(16, 28)) \n",
    "axes = axes.flatten()\n",
    "\n",
    "fontsize = 10  \n",
    "\n",
    "plt.rcParams['figure.constrained_layout.use'] = True  \n",
    "\n",
    "# plot each parameter\n",
    "for i in range(28):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # true vs predicted with error bars\n",
    "    ax.errorbar(\n",
    "        theta_test[:, i].cpu().numpy(),\n",
    "        all_means[:, i],\n",
    "        yerr=all_stds[:, i],\n",
    "        fmt='.',\n",
    "        color='k',\n",
    "        ecolor='blue',\n",
    "        capsize=0,\n",
    "        elinewidth=0.8,  \n",
    "        alpha=0.3,       \n",
    "        markersize=5    \n",
    "    )\n",
    "    \n",
    "    # add true line\n",
    "    lims = [\n",
    "        min(ax.get_xlim()[0], ax.get_ylim()[0]),\n",
    "        max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "    ]\n",
    "    ax.plot(lims, lims, '--', color='black', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    # get metrics\n",
    "    rmse = np.sqrt(np.mean((theta_test[:, i].cpu().numpy() - all_means[:, i])**2))\n",
    "    r2 = np.corrcoef(theta_test[:, i].cpu().numpy(), all_means[:, i])[0, 1]**2\n",
    "    chi2 = np.mean(((theta_test[:, i].cpu().numpy() - all_means[:, i])**2) / (all_stds[:, i]**2))\n",
    "    \n",
    "    # add metrics box in top left corner\n",
    "    stats_text = f'RMSE = {rmse:.2f}\\n' + \\\n",
    "                 f'R = {r2:.2f}\\n' + \\\n",
    "                 f' = {chi2:.2f}'\n",
    "    ax.text(0.05, 0.95, stats_text,\n",
    "            transform=ax.transAxes,\n",
    "            bbox=dict(facecolor='white', alpha=0.8),\n",
    "            verticalalignment='top',\n",
    "            fontsize=fontsize-1)  # Slightly smaller font for stats\n",
    "    \n",
    "    # title: parameter name\n",
    "    ax.set_title(param_names[i], fontsize=fontsize, pad=5)  # Reduced padding\n",
    "    \n",
    "    # axis labels\n",
    "    ax.set_xlabel('True', fontsize=fontsize-1)\n",
    "    ax.set_ylabel('Inferred', fontsize=fontsize-1)\n",
    "    \n",
    "    # tick labels\n",
    "    ax.tick_params(axis='both', which='major', labelsize=fontsize-2)\n",
    "    \n",
    "    # internal padding\n",
    "    ax.margins(x=0.05, y=0.05)\n",
    "\n",
    "# subplot spacing\n",
    "plt.subplots_adjust(\n",
    "    left=0.01,    # Less space on left\n",
    "    right=0.7,   # Less space on right\n",
    "    bottom=0.05,  # Less space at bottom\n",
    "    top=0.7,     # Less space at top\n",
    "    wspace=0.2,   # Less space between plots horizontally\n",
    "    hspace=0.2    # Less space between plots vertically\n",
    ")\n",
    "\n",
    "\n",
    "# save \n",
    "save_path = f'{plots_out_dir}/posterior_predictions_{model_params}.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "print(save_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_str = (f\"batch{train_args['training_batch_size']}_\"\n",
    "             f\"lr{train_args['learning_rate']}_\"\n",
    "             f\"epochs{train_args['stop_after_epochs']}_\"\n",
    "             f\"h{hidden_features}_t{num_transforms}\")\n",
    "\n",
    "# coverage plots\n",
    "metric = PosteriorCoverage(\n",
    "    num_samples=int(4e3),\n",
    "    sample_method='direct',\n",
    "    labels=cam.labels,\n",
    "    plot_list=[\"tarp\"], # \"coverage\", \"histogram\", \"predictions\", \n",
    "    out_dir=plots_out_dir,\n",
    ")\n",
    "\n",
    "# make plots\n",
    "figs = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x=x_all[test_mask].cpu(),\n",
    "    theta=theta[test_mask, :].cpu(),\n",
    "    signature=f\"coverage_{name}_{config_str}_\"  # Add config to filename\n",
    ")\n",
    "\n",
    "config_text = (\n",
    "    f\"Training Config:\\n\"\n",
    "    f\"Batch Size: {train_args['training_batch_size']}\\n\"\n",
    "    f\"Learning Rate: {train_args['learning_rate']}\\n\"\n",
    "    f\"Epochs: {train_args['stop_after_epochs']}\\n\"\n",
    "    f\"Hidden Features: {hidden_features}\\n\"\n",
    "    f\"Num Transforms: {num_transforms}\"\n",
    ")\n",
    "\n",
    "# make each figure\n",
    "for i, fig in enumerate(figs):\n",
    "    plt.figure(fig.number)  # activate the figure\n",
    "    plt.figtext(0.02, 0.98, config_text,\n",
    "                fontsize=8,\n",
    "                bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),\n",
    "                verticalalignment='top')\n",
    "    \n",
    "    # save each figure with type indicator\n",
    "    plot_types = [\"tarp\"] #\"coverage\", \"histogram\", \"predictions\",\n",
    "    plt.savefig(os.path.join(plots_out_dir, \n",
    "                f'metric_{plot_types[i]}_{name}_{config_str}.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at a specific case (one observation randomly set with seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, SBIRunner returns a custom class instance to be able to pass signature strings\n",
    "# 1. prints our info on model configuration and architecture\n",
    "print(posterior_ensemble.signatures)\n",
    "\n",
    "\n",
    "# 2. choose a random input for training\n",
    "seed_in = 49\n",
    "np.random.seed(seed_in) # set seed for reproducability\n",
    "ind = np.random.randint(len(x_train[0])) # choose observation (random index from training data)\n",
    "\n",
    "# 3. generate posterior samples\n",
    "seed_samp = 32\n",
    "torch.manual_seed(seed_samp)# set seed for reproducability\n",
    "# then, for the chosen training sample (as chosen above in 2.)\n",
    "# generate 1000 samples from the posterior distribution using accept/reject sampling\n",
    "samples = posterior_ensemble.sample(\n",
    "    (1000,), \n",
    "    torch.Tensor(x_train[0][ind]).to(device))\n",
    "\n",
    "# 4. calculate the probability densities for each sample\n",
    "# i.e for each generated sample, calculate how likely it is using learned posterior distribution\n",
    "log_prob = posterior_ensemble.log_prob(\n",
    "    samples, # the generated samples from 3.\n",
    "    torch.Tensor(x_train[0][ind]).to(device) # the chosen observation from 2.\n",
    "    )\n",
    "\n",
    "# convert to numpy so can read easier.\n",
    "samples = samples.cpu().numpy()\n",
    "log_prob = log_prob.cpu().numpy()\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "def plot_posterior_samples_grid(samples, log_prob, param_names, df_info, model_name, train_args):\n",
    "   n_params = len(param_names)\n",
    "   n_cols = 4\n",
    "   n_rows = (n_params + n_cols - 1) // n_cols\n",
    "   \n",
    "   fig = plt.figure(figsize=(20, 5*n_rows))\n",
    "   \n",
    "   # Add main title\n",
    "   fig.suptitle('Posterior Probability Distributions', fontsize=16, y=0.98)\n",
    "   \n",
    "   gs = GridSpec(n_rows, n_cols, figure=fig)\n",
    "   \n",
    "   # Model info text\n",
    "   model_info = (\n",
    "       f\"Model Config:\\n\"\n",
    "       f\"Name: {model_name}\\n\"\n",
    "       f\"Hidden Features: {hidden_features}\\n\"\n",
    "       f\"Num Transforms: {num_transforms}\\n\"\n",
    "       f\"\\nTraining Args:\\n\"\n",
    "       f\"Batch Size: {train_args['training_batch_size']}\\n\"\n",
    "       f\"Learning Rate: {train_args['learning_rate']}\\n\"\n",
    "       f\"Stop After Epochs: {train_args['stop_after_epochs']}\"\n",
    "   )\n",
    "   \n",
    "   fig.text(0.02, 0.96, model_info, \n",
    "            fontsize=8,\n",
    "            bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),\n",
    "            verticalalignment='top')\n",
    "   \n",
    "   for i, name in enumerate(param_names):\n",
    "       row = i // n_cols\n",
    "       col = i % n_cols\n",
    "       \n",
    "       ax = fig.add_subplot(gs[row, col])\n",
    "       data = samples[:, i]\n",
    "       param_info = df_info[df_info['ParamName'] == name].iloc[0]\n",
    "       is_log = param_info['LogFlag'] == 1\n",
    "       \n",
    "       if is_log:\n",
    "           ax.hist(data, bins=50, density=True, alpha=0.6)\n",
    "           ax.set_xscale('log')\n",
    "           log_data = np.log10(data)\n",
    "           mean = np.mean(log_data)\n",
    "           std = np.std(log_data)\n",
    "           stats_text = f'Log10 Mean: {mean:.3f}\\nLog10 Std: {std:.3f}'\n",
    "           ax.set_xlabel('Parameter Value (log scale)', fontsize=8)\n",
    "       else:\n",
    "           ax.hist(data, bins=50, density=True, alpha=0.6)\n",
    "           mean = np.mean(data)\n",
    "           std = np.std(data)\n",
    "           stats_text = f'Mean: {mean:.3f}\\nStd: {std:.3f}'\n",
    "           ax.set_xlabel('Parameter Value', fontsize=8)\n",
    "       \n",
    "       ax.set_ylabel('Density', fontsize=8)\n",
    "       \n",
    "       ax.axvline(param_info['MinVal'], color='g', linestyle=':', alpha=0.5, label='Min')\n",
    "       ax.axvline(param_info['MaxVal'], color='g', linestyle=':', alpha=0.5, label='Max')\n",
    "       ax.axvline(param_info['FiducialVal'], color='r', linestyle='--', alpha=0.5, label='Fiducial')\n",
    "       \n",
    "       ax.text(0.02, 0.95, stats_text, transform=ax.transAxes, \n",
    "               verticalalignment='top', fontsize=8, \n",
    "               bbox=dict(facecolor='white', alpha=0.8))\n",
    "       \n",
    "       ax.set_title(f\"{name}\\n{param_info['Description']}\", fontsize=8, pad=5)\n",
    "       ax.tick_params(labelsize=8)\n",
    "       \n",
    "       if i == 0:\n",
    "           ax.legend(loc='upper right', fontsize=8)\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.subplots_adjust(top=0.93)  # Adjusted to make room for main title\n",
    "   return fig\n",
    "\n",
    "# Get all parameter names from df_info\n",
    "param_names = df_info['ParamName'].tolist()\n",
    "\n",
    "# Now try plotting again with the correct parameter names\n",
    "fig = plot_posterior_samples_grid(\n",
    "    samples, \n",
    "    log_prob, \n",
    "    param_names,  # Now contains all 28 parameter names correctly\n",
    "    df_info,\n",
    "    model_name=name,\n",
    "    train_args=train_args\n",
    ")\n",
    "\n",
    "# Save with model config in filename\n",
    "save_name = (f'parameter_posteriors_grid_{name}_'\n",
    "            f'h{hidden_features}_t{num_transforms}_'\n",
    "            f'b{train_args[\"training_batch_size\"]}_'\n",
    "            f'e{train_args[\"stop_after_epochs\"]}.png')\n",
    "\n",
    "os.makedirs(plots_out_dir, exist_ok=True)\n",
    "plt.savefig(os.path.join(plots_out_dir, save_name), \n",
    "            dpi=300, \n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
