{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import GPUtil\n",
    "import itertools\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import joblib\n",
    "import ili  # Import ili for the SBI functionality\n",
    "from ili.dataloaders import NumpyLoader\n",
    "from ili.inference import InferenceRunner\n",
    "from ili.validation.metrics import PosteriorCoverage, PlotSinglePosterior\n",
    "\n",
    "from sbi.utils.user_input_checks import process_prior\n",
    "\n",
    "sys.path.append(\"/disk/xray15/aem2/camels/proj2\")\n",
    "from setup_params_1P import plot_uvlf, plot_colour\n",
    "from setup_params_SB import *\n",
    "from priors_SB import initialise_priors_SB28\n",
    "from variables_config_28 import uvlf_limits, n_bins_lf, colour_limits, n_bins_colour\n",
    "# parameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = \"IllustrisTNG\"\n",
    "spec_type = \"attenuated\"\n",
    "sps = \"BC03\"\n",
    "snap = [\"044\"]\n",
    "bands = \"all\" # or just GALEX?\n",
    "\n",
    "colours = False  \n",
    "luminosity_functions = True\n",
    "name = f\"{model}_{bands}_{sps}_{spec_type}_{n_bins_lf}_{n_bins_colour}\"\n",
    "\n",
    "cam = camels(model=model, sim_set='SB28')\n",
    "\n",
    "if colours and not luminosity_functions:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/colours_only/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/colours_only/\"\n",
    "    \n",
    "elif luminosity_functions and not colours:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/lf_only/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/lfs_only/\"\n",
    "\n",
    "elif colours and luminosity_functions:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/colours_lfs/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/colours_lfs/\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"At least one of colours or luminosity_functions must be True\")\n",
    "\n",
    "print(\"Saving model in \", model_out_dir)\n",
    "print(\"Saving plots in \", plots_out_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import GPUtil  # You already have this imported\n",
    "import torch\n",
    "# Add these imports at the top with your other imports\n",
    "import os\n",
    "from pynvml import *\n",
    "\n",
    "# Add this function at the start, before any GPU operations\n",
    "def select_least_used_gpu():\n",
    "    try:\n",
    "        # Initialize NVIDIA management library\n",
    "        nvmlInit()\n",
    "        \n",
    "        # Get number of GPUs\n",
    "        deviceCount = nvmlDeviceGetCount()\n",
    "        gpu_memory_used = []\n",
    "        \n",
    "        # Check memory usage for each GPU\n",
    "        for i in range(deviceCount):\n",
    "            handle = nvmlDeviceGetHandleByIndex(i)\n",
    "            info = nvmlDeviceGetMemoryInfo(handle)\n",
    "            gpu_memory_used.append((i, info.used))\n",
    "        \n",
    "        # Sort by memory usage and get GPU with least memory used\n",
    "        least_used_gpu = sorted(gpu_memory_used, key=lambda x: x[1])[0][0]\n",
    "        \n",
    "        # Set CUDA device\n",
    "        torch.cuda.set_device(least_used_gpu)\n",
    "        print(f\"Selected GPU {least_used_gpu} with {gpu_memory_used[least_used_gpu][1]/1024**2:.0f}MB used\")\n",
    "        \n",
    "        return least_used_gpu\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error selecting GPU: {e}\")\n",
    "        return 0  # Default to first GPU if there's an error\n",
    "\n",
    "# Add this right after your imports, before any GPU operations\n",
    "# Select GPU and set device\n",
    "gpu_id = select_least_used_gpu()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# The rest of your code remains the same, but remove or comment out your original device line:\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uvlf_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check of available data\n",
    "with h5py.File(\"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/photometry/alice_galex.h5\", \"r\") as hf:\n",
    "    print(\"Available simulations:\", list(hf.keys()))\n",
    "    sim_key = list(hf.keys())[0]  # First simulation\n",
    "    print(\"\\nStructure for first simulation:\")\n",
    "    print(list(hf[f\"{sim_key}/snap_044/BC03/photometry/luminosity/attenuated\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter info file (df_info) is used for defining priors\n",
    "# the actual parameter values come from the camels class which reads CosmoAstroSeed_IllustrisTNG_L25n256_SB28.txt\n",
    "\n",
    "#  parameters defined here: /disk/xray15/aem2/data/28pams/IllustrisTNG/SB/CosmoAstroSeed_IllustrisTNG_L25n256_SB28.txt which is used for theta\n",
    "df_pars = pd.read_csv('/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/CosmoAstroSeed_IllustrisTNG_L25n256_SB28.txt', delim_whitespace=True)\n",
    "# print(df_pars)\n",
    "\n",
    "\n",
    "# prior values come from this:\n",
    "df_info = pd.read_csv(\"/disk/xray15/aem2/data/28pams/Info_IllustrisTNG_L25n256_28params.txt\")\n",
    "# print(df_info)\n",
    "\n",
    "theta = df_pars.iloc[:, 1:29].to_numpy()  # excluding 'name' column and 'seed' column\n",
    "\n",
    "# print(theta)\n",
    "# print(theta.shape)\n",
    "# print(\"Column names:\")\n",
    "# print(df_pars.columns.tolist())\n",
    "# plot the first one (omega0) to see shape of prior:\n",
    "plt.hist(theta[:, 24])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    theta, x = get_theta_x_SB(\n",
    "        luminosity_functions=luminosity_functions,\n",
    "        colours=colours,\n",
    "        uvlf_limits=uvlf_limits,\n",
    "        colour_limits=colour_limits,\n",
    "        n_bins_lf=n_bins_lf,\n",
    "        n_bins_colour=n_bins_colour\n",
    "    )\n",
    "    print(theta.shape, x.shape)\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     theta, x = get_theta_x_SB(\n",
    "#         luminosity_functions=luminosity_functions,\n",
    "#         colours=colours  # This will now override the default\n",
    "#     )\n",
    "#     print(theta.shape, x.shape)\n",
    "    \n",
    "if colours:\n",
    "    fig = plot_colour(x)\n",
    "    plt.savefig('/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/colours_test/colour_check.png')\n",
    "    plt.show()\n",
    "\n",
    "if luminosity_functions:\n",
    "    fig = plot_uvlf(x)\n",
    "    plt.savefig('/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/LFs_test/uvlf_check.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get the priors and data\n",
    "prior = initialise_priors_SB28(\n",
    "    df=df_info, \n",
    "    device=device,\n",
    "    astro=True,\n",
    "    dust=False  # no dust for testing. set to False to only get the 28 model parameters.\n",
    "    # with dust = True, prior has 32 dimensions (28 parameters + 4 dust parameters) \n",
    ")\n",
    "\n",
    "# process the data\n",
    "x_all = np.array([np.hstack(_x) for _x in x])\n",
    "theta = theta.detach().cpu().numpy()\n",
    "\n",
    "print(\"Theta shape:\", theta.shape)\n",
    "print(\"X shape:\", x_all.shape)\n",
    "\n",
    "# make test mask\n",
    "test_mask = create_test_mask() # 10% testing\n",
    "train_mask = ~test_mask # 90% for training\n",
    "\n",
    "# train and test set\n",
    "X_train = x_all[train_mask]\n",
    "theta_train = theta[train_mask]\n",
    "X_test = x_all[test_mask]\n",
    "theta_test = theta[test_mask]\n",
    "\n",
    "# create scalers\n",
    "\n",
    "class GlobalScaler:\n",
    "    def fit_transform(self, x):\n",
    "        self._mean_x = x.mean()\n",
    "        self._std_x = x.std()\n",
    "        return (x - self._mean_x) / self._std_x\n",
    "\n",
    "    def transform(self, x):\n",
    "        return (x - self._mean_x) / self._std_x\n",
    "\n",
    "    def inverse_transform(self, x_norm):\n",
    "        return (x_norm * self._std_x) + self._mean_x\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_scaler = GlobalScaler()  # scales all features by same mean and std  over N samples\n",
    "theta_scaler = StandardScaler()  # scales each feature by its own mean and std over N samples\n",
    "\n",
    "# normalise, fit scaler to the training data\n",
    "X_train = x_scaler.fit_transform(X_train)\n",
    "X_test = x_scaler.transform(X_test)\n",
    "\n",
    "# normalise the theta, each dimension independently\n",
    "theta_train = theta_scaler.fit_transform(theta_train)\n",
    "theta_test = theta_scaler.transform(theta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tensor and to GPU\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "theta_train = torch.tensor(theta_train, dtype=torch.float32, device=device)\n",
    "theta_test = torch.tensor(theta_test, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training arguments\n",
    "train_args = {\n",
    "    \"training_batch_size\": 64, # changed from 4 to 10 as dealing with more sims, want it to be faster for initial testing.\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"stop_after_epochs\": 600, # loss function. waits to see if things improve.\n",
    "    \"validation_fraction\": 0.15,  # creates another split within the training data for validation\n",
    "    \"max_num_epochs\": 2000  # Add maximum epochs\n",
    "}\n",
    "\n",
    "# Configure network\n",
    "hidden_features = 256 #neurons\n",
    "num_transforms = 8 #layers\n",
    "num_nets = 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balanced 'middle' network:\n",
    "\n",
    "# run 1\n",
    "train_args = {\n",
    "   \"training_batch_size\": 16,\n",
    "   \"learning_rate\": 5e-5,   \n",
    "   \"stop_after_epochs\": 100, \n",
    "   \"max_num_epochs\": 1000,\n",
    "   \"validation_fraction\": 0.1,  \n",
    "   \"clip_max_norm\": 5.0        # More aggressive gradient clipping\n",
    "}\n",
    "\n",
    "hidden_features = 128   # Thin network\n",
    "num_transforms = 8    # Deep network\n",
    "num_nets = 1 \n",
    "\n",
    "\n",
    "# run 2\"\n",
    "train_args = {\n",
    "  \"training_batch_size\": 8,\n",
    "  \"learning_rate\": 1e-3,  \n",
    "  \"stop_after_epochs\": 50,\n",
    "  \"max_num_epochs\": 1000,\n",
    "  \"validation_fraction\": 0.1, \n",
    "  \"clip_max_norm\": 5.0        # More aggressive gradient clipping\n",
    "}\n",
    "\n",
    "\n",
    "hidden_features = 128   # Thin network\n",
    "num_transforms = 8    # Deep network\n",
    "num_nets = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIDE but shallow network:\n",
    "train_args = {\n",
    "    \"training_batch_size\": 16,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"stop_after_epochs\": 100, # loss function. waits to see if things improve.\n",
    "    \"max_num_epochs\": 1000,\n",
    "    \"validation_fraction\": 0.1,  # creates another split within the training data for validation\n",
    "    \"clip_max_norm\": 5.0        # More aggressive gradient clipping\n",
    "}\n",
    "\n",
    "hidden_features = 512  # Wide network\n",
    "num_transforms = 3    # Shallow depth\n",
    "num_nets = 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thin but Deep Network:\n",
    "train_args = {\n",
    "    \"training_batch_size\": 16,\n",
    "    \"learning_rate\": 1e-5,   # Slower learning rate for complex architecture\n",
    "    \"stop_after_epochs\": 100, # loss function. waits to see if things improve.\n",
    "    \"max_num_epochs\": 1000,\n",
    "    \"validation_fraction\": 0.1,  # creates another split within the training data for validation\n",
    "    \"clip_max_norm\": 5.0        # More aggressive gradient clipping\n",
    "}\n",
    "\n",
    "\n",
    "hidden_features = 32   # Thin network\n",
    "num_transforms = 15    # Deep network\n",
    "\n",
    "nets = [ili.utils.load_nde_sbi(\n",
    "    engine=\"NPE\",\n",
    "    model=\"nsf\",\n",
    "    hidden_features=hidden_features,\n",
    "    num_transforms=num_transforms\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced middle model:\n",
    "hidden_features = 128  # Medium width\n",
    "num_transforms = 8     # Medium depth\n",
    "learning_rate = 5e-5   # Moderate learning rate\n",
    "\n",
    "nets = [ili.utils.load_nde_sbi(\n",
    "    engine=\"NPE\",\n",
    "    model=\"nsf\",\n",
    "    hidden_features=hidden_features,\n",
    "    num_transforms=num_transforms\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_args = {\n",
    "    \"training_batch_size\": 80,     # Keep this as is\n",
    "    \"learning_rate\": 1e-3,        # Reduce from 5e-6 to slow down learning\n",
    "    \"stop_after_epochs\": 50,      # Increase patience slightly\n",
    "    \"clip_max_norm\": 10,         # Slightly tighter gradient clipping\n",
    "    \"validation_fraction\": 0.1,   # Keep as is\n",
    "    \"max_num_epochs\": 100,       # Allow for longer training with slower learning\n",
    "    \"show_train_summary\": True\n",
    "}\n",
    "\n",
    "hidden_features = 32              # Keep this\n",
    "num_transforms = 2               # Keep this\n",
    "num_nets = 1                      # Consider reducing to 2 for better generalization\n",
    "\n",
    "nets = [ili.utils.load_nde_sbi( # uses sbi.utils.posterior_nn\n",
    "    engine=\"NPE\",\n",
    "    model=\"nsf\",\n",
    "    hidden_features=hidden_features,\n",
    "    num_transforms=num_transforms,\n",
    ") for _ in range(num_nets)]\n",
    "\n",
    "runner = InferenceRunner.load(\n",
    "    backend=\"sbi\",\n",
    "    engine=\"NPE\",\n",
    "    prior=prior,\n",
    "    nets=nets,\n",
    "    device=device,\n",
    "    train_args=train_args,\n",
    "    proposal=None,\n",
    "    out_dir=model_out_dir,\n",
    "    name=name\n",
    ")\n",
    "\n",
    "loader = NumpyLoader(\n",
    "    x=X_train.clone().detach(),\n",
    "    theta=theta_train.clone().detach()\n",
    ")\n",
    "\n",
    "posterior_ensemble, summaries = runner(loader=loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define parameter grid\n",
    "# param_grid = {\n",
    "#     \"hidden_features\": [20, 40, 80],\n",
    "#     \"num_transforms\": [1, 2, 4, 8],\n",
    "#     \"training_batch_size\": [16, 32, 64],\n",
    "#     \"learning_rate\": [1e-4, 5e-5, 1e-5],\n",
    "#     \"num_nets\": [1, 2, 3]\n",
    "# }\n",
    "\n",
    "# # Create output directory with timestamp\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# results_dir = Path(f\"grid_search_results_{timestamp}\")\n",
    "# results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# # Store all results\n",
    "# all_results = []\n",
    "\n",
    "# # Generate all combinations of parameters\n",
    "# param_combinations = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "# def create_run_name(params):\n",
    "#     \"\"\"Create a descriptive run name from parameters\"\"\"\n",
    "#     return f\"hf{params['hidden_features']}_nt{params['num_transforms']}_bs{params['training_batch_size']}_lr{params['learning_rate']:.0e}_nets{params['num_nets']}\"\n",
    "\n",
    "# for i, params in enumerate(param_combinations):\n",
    "\n",
    "#     run_name = create_run_name(params)\n",
    "\n",
    "#     # Update training arguments\n",
    "#     train_args = {\n",
    "#         \"training_batch_size\": params[\"training_batch_size\"],\n",
    "#         \"learning_rate\": params[\"learning_rate\"],\n",
    "#         \"stop_after_epochs\": 100,\n",
    "#         \"validation_fraction\": 0.2,\n",
    "#         \"max_num_epochs\": 1000,\n",
    "#         \"clip_max_norm\": 5.0\n",
    "#     }\n",
    "    \n",
    "#     # Update model parameters\n",
    "#     hidden_features = params[\"hidden_features\"]\n",
    "#     num_transforms = params[\"num_transforms\"]\n",
    "#     num_nets = params[\"num_nets\"]\n",
    "    \n",
    "#     # Create networks\n",
    "#     nets = [ili.utils.load_nde_sbi(\n",
    "#         engine=\"NPE\",\n",
    "#         model=\"nsf\",\n",
    "#         hidden_features=hidden_features,\n",
    "#         num_transforms=num_transforms,\n",
    "#     ) for _ in range(num_nets)]\n",
    "    \n",
    "#     # Setup runner\n",
    "#     run_dir = results_dir / run_name\n",
    "#     run_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "#     runner = InferenceRunner.load(\n",
    "#         backend=\"sbi\",\n",
    "#         engine=\"NPE\",\n",
    "#         prior=prior,\n",
    "#         nets=nets,\n",
    "#         device=device,\n",
    "#         train_args=train_args,\n",
    "#         proposal=None,\n",
    "#         out_dir=run_dir,\n",
    "#         name=run_name\n",
    "#     )\n",
    "    \n",
    "#     # Data loader\n",
    "#     loader = NumpyLoader(\n",
    "#         x=x_all[train_mask].clone().detach(),\n",
    "#         theta=theta[train_mask].clone().detach()\n",
    "#     )\n",
    "    \n",
    "#     # Train model\n",
    "#     posterior_ensemble, summaries = runner(loader=loader)\n",
    "    \n",
    "#     # Store results\n",
    "#     result = {\n",
    "#         \"parameters\": params,\n",
    "#         \"summaries\": summaries,\n",
    "#         \"posterior_ensemble\": posterior_ensemble,\n",
    "#         \"train_args\": train_args\n",
    "#     }\n",
    "    \n",
    "#     all_results.append(result)\n",
    "    \n",
    "#     # Save individual run results\n",
    "#     with open(run_dir / \"results.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(result, f)\n",
    "    \n",
    "#     # Print progress\n",
    "#     print(f\"Completed run {i+1}/{len(param_combinations)}\")\n",
    "#     # print(f\"Parameters: {params}\")\n",
    "#     # print(f\"Validation loss: {summaries.get('validation_loss', 'N/A')}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "# # Save all results to a single file\n",
    "# with open(results_dir / \"all_results.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(all_results, f)\n",
    "\n",
    "# # Create a summary file\n",
    "# # with open(results_dir / \"summary.txt\", \"w\") as f:\n",
    "# #     f.write(\"Grid Search Results Summary\\n\")\n",
    "#     # f.write(f\"Timestamp: {timestamp}\\n\\n\")\n",
    "    \n",
    "#     # for i, result in enumerate(all_results):\n",
    "#         # f.write(f\"Run {i+1}:\\n\")\n",
    "#         # f.write(f\"Parameters: {result['parameters']}\\n\")\n",
    "#         # f.write(f\"Validation loss: {result['summaries'].get('validation_loss', 'N/A')}\\n\")\n",
    "#         # f.write(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hf20_nt2_bs16_lr1e-04_nets1.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training_batch_size\", train_args[ \"training_batch_size\"])\n",
    "print(\"learning_rate\", train_args[ \"learning_rate\"])\n",
    "print(\"stop_after_epochs\", train_args[ \"stop_after_epochs\"])\n",
    "print(\"validation_fraction\", train_args[ \"validation_fraction\"])\n",
    "print(\"hidden_features\", hidden_features)\n",
    "print(\"num_transforms\", num_transforms)\n",
    "print(\"num_nets\", num_nets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_diagnostics(summaries):\n",
    "    \"\"\"Plot training diagnostics without empty subplots\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))  # Changed to 1 row, 2 columns\n",
    "    \n",
    "    # Loss curves\n",
    "    train_losses = summaries[0]['training_log_probs']\n",
    "    val_losses = summaries[0]['validation_log_probs']\n",
    "    epochs = range(len(train_losses))\n",
    "    \n",
    "    ax1.plot(epochs, train_losses, '-', label='Training', color='blue')\n",
    "    ax1.plot(epochs, val_losses, '--', label='Validation', color='red')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Log probability')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overfitting gap\n",
    "    gap = np.array(train_losses) - np.array(val_losses)\n",
    "    ax2.plot(epochs, gap, '-', color='purple')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss difference')\n",
    "    ax2.set_title('Overfitting Gap')\n",
    "    ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# refer back to training args in file name\n",
    "model_params = f\"batch{train_args['training_batch_size']}_\" \\\n",
    "               f\"lr{train_args['learning_rate']:.0e}_\" \\\n",
    "               f\"epochs{train_args['stop_after_epochs']}_\" \\\n",
    "               f\"val{train_args['validation_fraction']}_\" \\\n",
    "               f\"hidden{hidden_features}_\" \\\n",
    "               f\"transforms{num_transforms}\"\n",
    "\n",
    "# Use the function\n",
    "fig = plot_training_diagnostics(summaries)\n",
    "plt.savefig(os.path.join(plots_out_dir, f'loss_overfitting_{model_params}.png'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train/validation loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,4))\n",
    "c = list(mcolors.TABLEAU_COLORS)\n",
    "for i, m in enumerate(summaries):\n",
    "    ax.plot(m['training_log_probs'], ls='-', label=f\"{i}_train\", c=c[i])\n",
    "    ax.plot(m['validation_log_probs'], ls='--', label=f\"{i}_val\", c=c[i])\n",
    "ax.set_xlim(0)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Log probability')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing samples from the entire test set to look at overall performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get test data\n",
    "# x_test = x_all[test_mask]\n",
    "# theta_test = theta[test_mask]\n",
    "\n",
    "# # Number of samples to draw from posterior\n",
    "# n_samples = 1000\n",
    "\n",
    "# # Storage for predictions\n",
    "# all_samples = []\n",
    "# all_means = []\n",
    "# all_stds = []\n",
    "\n",
    "# # Generate posterior samples for each test point\n",
    "# for i in range(len(x_test)):\n",
    "#     # Get samples from the posterior\n",
    "#     samples = posterior_ensemble.sample(\n",
    "#         (n_samples,), \n",
    "#         x=x_test[i].reshape(1, -1)\n",
    "#     ).cpu().numpy()\n",
    "    \n",
    "#     # Calculate mean and std of samples\n",
    "#     mean = samples.mean(axis=0)\n",
    "#     std = samples.std(axis=0)\n",
    "    \n",
    "#     all_samples.append(samples)\n",
    "#     all_means.append(mean)\n",
    "#     all_stds.append(std)\n",
    "\n",
    "# all_samples = np.array(all_samples)\n",
    "# all_means = np.array(all_means)\n",
    "# all_stds = np.array(all_stds)\n",
    "\n",
    "# Get test data\n",
    "\n",
    "# Number of samples to draw from posterior\n",
    "n_samples = 1000\n",
    "\n",
    "# Storage for predictions\n",
    "all_samples = []\n",
    "all_means = []\n",
    "all_stds = []\n",
    "\n",
    "# Suppress the deprecation warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='nflows.transforms.lu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate posterior samples for each test point\n",
    "X_test = X_test.clone().detach()\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    # Get samples from the posterior\n",
    "    with torch.no_grad():  # Add this for efficiency\n",
    "        samples = posterior_ensemble.sample(\n",
    "            (n_samples,), \n",
    "            x=X_test[i].reshape(1, -1)\n",
    "        ).cpu().numpy()\n",
    "    \n",
    "    # Calculate mean and std of samples\n",
    "    mean = samples.mean(axis=0)\n",
    "    std = samples.std(axis=0)\n",
    "    \n",
    "    all_samples.append(samples)\n",
    "    all_means.append(mean)\n",
    "    all_stds.append(std)\n",
    "\n",
    "all_samples = np.array(all_samples)\n",
    "all_means = np.array(all_means)\n",
    "all_stds = np.array(all_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = df_pars.columns[1:29].tolist()  # Excluding 'name' column\n",
    "\n",
    "fig, axes = plt.subplots(7, 4, figsize=(16, 28)) \n",
    "axes = axes.flatten()\n",
    "\n",
    "fontsize = 10  \n",
    "\n",
    "plt.rcParams['figure.constrained_layout.use'] = True  \n",
    "\n",
    "# Plot each parameter\n",
    "for i in range(28):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # True vs predicted with error bars\n",
    "    ax.errorbar(\n",
    "        theta_test[:, i].cpu().numpy(),\n",
    "        all_means[:, i],\n",
    "        yerr=all_stds[:, i],\n",
    "        fmt='.',\n",
    "        color='k',\n",
    "        ecolor='blue',\n",
    "        capsize=0,\n",
    "        elinewidth=0.8,  \n",
    "        alpha=0.3,       \n",
    "        markersize=5    \n",
    "    )\n",
    "    \n",
    "    # Add true line\n",
    "    lims = [\n",
    "        min(ax.get_xlim()[0], ax.get_ylim()[0]),\n",
    "        max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "    ]\n",
    "    ax.plot(lims, lims, '--', color='black', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    # get metrics\n",
    "    rmse = np.sqrt(np.mean((theta_test[:, i].cpu().numpy() - all_means[:, i])**2))\n",
    "    r2 = np.corrcoef(theta_test[:, i].cpu().numpy(), all_means[:, i])[0, 1]**2\n",
    "    chi2 = np.mean(((theta_test[:, i].cpu().numpy() - all_means[:, i])**2) / (all_stds[:, i]**2))\n",
    "    \n",
    "    # add metrics box in top left corner\n",
    "    stats_text = f'RMSE = {rmse:.2f}\\n' + \\\n",
    "                 f'R² = {r2:.2f}\\n' + \\\n",
    "                 f'χ² = {chi2:.2f}'\n",
    "    ax.text(0.05, 0.95, stats_text,\n",
    "            transform=ax.transAxes,\n",
    "            bbox=dict(facecolor='white', alpha=0.8),\n",
    "            verticalalignment='top',\n",
    "            fontsize=fontsize-1)  # Slightly smaller font for stats\n",
    "    \n",
    "    # title: parameter name\n",
    "    ax.set_title(param_names[i], fontsize=fontsize, pad=5)  # Reduced padding\n",
    "    \n",
    "    # axis labels\n",
    "    ax.set_xlabel('True', fontsize=fontsize-1)\n",
    "    ax.set_ylabel('Inferred', fontsize=fontsize-1)\n",
    "    \n",
    "    # tick labels\n",
    "    ax.tick_params(axis='both', which='major', labelsize=fontsize-2)\n",
    "    \n",
    "    # internal padding\n",
    "    ax.margins(x=0.05, y=0.05)\n",
    "\n",
    "# subplot spacing\n",
    "plt.subplots_adjust(\n",
    "    left=0.01,    # Less space on left\n",
    "    right=0.7,   # Less space on right\n",
    "    bottom=0.05,  # Less space at bottom\n",
    "    top=0.7,     # Less space at top\n",
    "    wspace=0.2,   # Less space between plots horizontally\n",
    "    hspace=0.2    # Less space between plots vertically\n",
    ")\n",
    "\n",
    "\n",
    "# Save figure with detailed filename\n",
    "save_path = f'{plots_out_dir}/posterior_predictions_{model_params}.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "print(save_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_str = (f\"batch{train_args['training_batch_size']}_\"\n",
    "             f\"lr{train_args['learning_rate']}_\"\n",
    "             f\"epochs{train_args['stop_after_epochs']}_\"\n",
    "             f\"h{hidden_features}_t{num_transforms}\")\n",
    "\n",
    "# coverage plots\n",
    "metric = PosteriorCoverage(\n",
    "    num_samples=int(4e3),\n",
    "    sample_method='direct',\n",
    "    labels=cam.labels,\n",
    "    plot_list=[\"tarp\"], # \"coverage\", \"histogram\", \"predictions\", \n",
    "    out_dir=plots_out_dir,\n",
    ")\n",
    "\n",
    "# Generate plots\n",
    "figs = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x=x_all[test_mask].cpu(),\n",
    "    theta=theta[test_mask, :].cpu(),\n",
    "    signature=f\"coverage_{name}_{config_str}_\"  # Add config to filename\n",
    ")\n",
    "\n",
    "config_text = (\n",
    "    f\"Training Config:\\n\"\n",
    "    f\"Batch Size: {train_args['training_batch_size']}\\n\"\n",
    "    f\"Learning Rate: {train_args['learning_rate']}\\n\"\n",
    "    f\"Epochs: {train_args['stop_after_epochs']}\\n\"\n",
    "    f\"Hidden Features: {hidden_features}\\n\"\n",
    "    f\"Num Transforms: {num_transforms}\"\n",
    ")\n",
    "\n",
    "# Process each figure\n",
    "for i, fig in enumerate(figs):\n",
    "    plt.figure(fig.number)  # Activate the figure\n",
    "    plt.figtext(0.02, 0.98, config_text,\n",
    "                fontsize=8,\n",
    "                bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),\n",
    "                verticalalignment='top')\n",
    "    \n",
    "    # Save each figure with type indicator\n",
    "    plot_types = [\"tarp\"] #\"coverage\", \"histogram\", \"predictions\",\n",
    "    plt.savefig(os.path.join(plots_out_dir, \n",
    "                f'metric_{plot_types[i]}_{name}_{config_str}.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at a specific case (one observation randomly set with seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, SBIRunner returns a custom class instance to be able to pass signature strings\n",
    "# 1. prints our info on model configuration and architecture\n",
    "print(posterior_ensemble.signatures)\n",
    "\n",
    "\n",
    "# 2. choose a random input for training\n",
    "seed_in = 49\n",
    "np.random.seed(seed_in) # set seed for reproducability\n",
    "ind = np.random.randint(len(x_train[0])) # choose observation (random index from training data)\n",
    "\n",
    "# 3. generate posterior samples\n",
    "seed_samp = 32\n",
    "torch.manual_seed(seed_samp)# set seed for reproducability\n",
    "# then, for the chosen training sample (as chosen above in 2.)\n",
    "# generate 1000 samples from the posterior distribution using accept/reject sampling\n",
    "samples = posterior_ensemble.sample(\n",
    "    (1000,), \n",
    "    torch.Tensor(x_train[0][ind]).to(device))\n",
    "\n",
    "# 4. calculate the probability densities for each sample\n",
    "# i.e for each generated sample, calculate how likely it is using learned posterior distribution\n",
    "log_prob = posterior_ensemble.log_prob(\n",
    "    samples, # the generated samples from 3.\n",
    "    torch.Tensor(x_train[0][ind]).to(device) # the chosen observation from 2.\n",
    "    )\n",
    "\n",
    "# convert to numpy so can read easier.\n",
    "samples = samples.cpu().numpy()\n",
    "log_prob = log_prob.cpu().numpy()\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "def plot_posterior_samples_grid(samples, log_prob, param_names, df_info, model_name, train_args):\n",
    "   n_params = len(param_names)\n",
    "   n_cols = 4\n",
    "   n_rows = (n_params + n_cols - 1) // n_cols\n",
    "   \n",
    "   fig = plt.figure(figsize=(20, 5*n_rows))\n",
    "   \n",
    "   # Add main title\n",
    "   fig.suptitle('Posterior Probability Distributions', fontsize=16, y=0.98)\n",
    "   \n",
    "   gs = GridSpec(n_rows, n_cols, figure=fig)\n",
    "   \n",
    "   # Model info text\n",
    "   model_info = (\n",
    "       f\"Model Config:\\n\"\n",
    "       f\"Name: {model_name}\\n\"\n",
    "       f\"Hidden Features: {hidden_features}\\n\"\n",
    "       f\"Num Transforms: {num_transforms}\\n\"\n",
    "       f\"\\nTraining Args:\\n\"\n",
    "       f\"Batch Size: {train_args['training_batch_size']}\\n\"\n",
    "       f\"Learning Rate: {train_args['learning_rate']}\\n\"\n",
    "       f\"Stop After Epochs: {train_args['stop_after_epochs']}\"\n",
    "   )\n",
    "   \n",
    "   fig.text(0.02, 0.96, model_info, \n",
    "            fontsize=8,\n",
    "            bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),\n",
    "            verticalalignment='top')\n",
    "   \n",
    "   for i, name in enumerate(param_names):\n",
    "       row = i // n_cols\n",
    "       col = i % n_cols\n",
    "       \n",
    "       ax = fig.add_subplot(gs[row, col])\n",
    "       data = samples[:, i]\n",
    "       param_info = df_info[df_info['ParamName'] == name].iloc[0]\n",
    "       is_log = param_info['LogFlag'] == 1\n",
    "       \n",
    "       if is_log:\n",
    "           ax.hist(data, bins=50, density=True, alpha=0.6)\n",
    "           ax.set_xscale('log')\n",
    "           log_data = np.log10(data)\n",
    "           mean = np.mean(log_data)\n",
    "           std = np.std(log_data)\n",
    "           stats_text = f'Log10 Mean: {mean:.3f}\\nLog10 Std: {std:.3f}'\n",
    "           ax.set_xlabel('Parameter Value (log scale)', fontsize=8)\n",
    "       else:\n",
    "           ax.hist(data, bins=50, density=True, alpha=0.6)\n",
    "           mean = np.mean(data)\n",
    "           std = np.std(data)\n",
    "           stats_text = f'Mean: {mean:.3f}\\nStd: {std:.3f}'\n",
    "           ax.set_xlabel('Parameter Value', fontsize=8)\n",
    "       \n",
    "       ax.set_ylabel('Density', fontsize=8)\n",
    "       \n",
    "       ax.axvline(param_info['MinVal'], color='g', linestyle=':', alpha=0.5, label='Min')\n",
    "       ax.axvline(param_info['MaxVal'], color='g', linestyle=':', alpha=0.5, label='Max')\n",
    "       ax.axvline(param_info['FiducialVal'], color='r', linestyle='--', alpha=0.5, label='Fiducial')\n",
    "       \n",
    "       ax.text(0.02, 0.95, stats_text, transform=ax.transAxes, \n",
    "               verticalalignment='top', fontsize=8, \n",
    "               bbox=dict(facecolor='white', alpha=0.8))\n",
    "       \n",
    "       ax.set_title(f\"{name}\\n{param_info['Description']}\", fontsize=8, pad=5)\n",
    "       ax.tick_params(labelsize=8)\n",
    "       \n",
    "       if i == 0:\n",
    "           ax.legend(loc='upper right', fontsize=8)\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.subplots_adjust(top=0.93)  # Adjusted to make room for main title\n",
    "   return fig\n",
    "\n",
    "# Get all parameter names from df_info\n",
    "param_names = df_info['ParamName'].tolist()\n",
    "\n",
    "# Now try plotting again with the correct parameter names\n",
    "fig = plot_posterior_samples_grid(\n",
    "    samples, \n",
    "    log_prob, \n",
    "    param_names,  # Now contains all 28 parameter names correctly\n",
    "    df_info,\n",
    "    model_name=name,\n",
    "    train_args=train_args\n",
    ")\n",
    "\n",
    "# Save with model config in filename\n",
    "save_name = (f'parameter_posteriors_grid_{name}_'\n",
    "            f'h{hidden_features}_t{num_transforms}_'\n",
    "            f'b{train_args[\"training_batch_size\"]}_'\n",
    "            f'e{train_args[\"stop_after_epochs\"]}.png')\n",
    "\n",
    "os.makedirs(plots_out_dir, exist_ok=True)\n",
    "plt.savefig(os.path.join(plots_out_dir, save_name), \n",
    "            dpi=300, \n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (camels)",
   "language": "python",
   "name": "camels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
