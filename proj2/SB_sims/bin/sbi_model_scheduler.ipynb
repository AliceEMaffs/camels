{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model in  /disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/lf_only/\n",
      "Saving plots in  /disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/lfs_only/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import GPUtil\n",
    "import itertools\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import joblib\n",
    "import ili  # Import ili for the SBI functionality\n",
    "from ili.dataloaders import NumpyLoader\n",
    "from ili.inference import InferenceRunner\n",
    "from ili.validation.metrics import PosteriorCoverage, PlotSinglePosterior\n",
    "\n",
    "from sbi.utils.user_input_checks import process_prior\n",
    "from sbi.utils.posterior_ensemble import NeuralPosteriorEnsemble\n",
    "from ili.dataloaders import _BaseLoader\n",
    "from ili.inference.runner_sbi import SBIRunner  # Import the SBIRunner class\n",
    "\n",
    "\n",
    "sys.path.append(\"/disk/xray15/aem2/camels/proj2\")\n",
    "from setup_params_1P import plot_uvlf, plot_colour\n",
    "from setup_params_SB import *\n",
    "from priors_SB import initialise_priors_SB28\n",
    "from variables_config_28 import uvlf_limits, n_bins_lf, colour_limits, n_bins_colour\n",
    "# parameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = \"IllustrisTNG\"\n",
    "spec_type = \"attenuated\"\n",
    "sps = \"BC03\"\n",
    "snap = [\"044\"]\n",
    "bands = \"all\" # or just GALEX?\n",
    "\n",
    "colours = False  \n",
    "luminosity_functions = True\n",
    "name = f\"{model}_{bands}_{sps}_{spec_type}_{n_bins_lf}_{n_bins_colour}\"\n",
    "\n",
    "cam = camels(model=model, sim_set='SB28')\n",
    "\n",
    "if colours and not luminosity_functions:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/colours_only/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/colours_only/\"\n",
    "    \n",
    "elif luminosity_functions and not colours:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/lf_only/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/lfs_only/\"\n",
    "\n",
    "elif colours and luminosity_functions:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/colours_lfs/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/colours_lfs/\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"At least one of colours or luminosity_functions must be True\")\n",
    "\n",
    "print(\"Saving model in \", model_out_dir)\n",
    "print(\"Saving plots in \", plots_out_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the ScheduledSBIRunner class\n",
    "class ScheduledSBIRunner(SBIRunner):\n",
    "    # redefine train round in the SBI runner class:\n",
    "    # we want to hack the loop\n",
    "    def _train_round(self, models, x, theta, proposal): # imported same as from original SBIRunner def\n",
    "        \"\"\"Override _train_round to implement learning rate scheduling\"\"\"\n",
    "        \n",
    "        # SAME:\n",
    "        # append data to models\n",
    "        for model in models:\n",
    "            if (\"NPE\" in self.engine):\n",
    "                model = model.append_simulations(theta, x, proposal=proposal)\n",
    "            else:\n",
    "                model = model.append_simulations(theta, x)\n",
    "\n",
    "        # get all previous simulations\n",
    "        starting_round = 0  # note: won't work for SNPE_A, but we don't use it\n",
    "        x, _, _ = model.get_simulations(starting_round)\n",
    "\n",
    "        # split into training and validation randomly\n",
    "        num_examples = x.shape[0]\n",
    "        permuted_indices = torch.randperm(num_examples)\n",
    "        num_training_examples = int(\n",
    "            (1 - self.train_args['validation_fraction']) * num_examples)\n",
    "        train_indices, val_indices = (\n",
    "            permuted_indices[:num_training_examples],\n",
    "            permuted_indices[num_training_examples:],\n",
    "        )\n",
    "\n",
    "        posteriors, summaries = [], []\n",
    "        for i, model in enumerate(models):\n",
    "            logging.info(f\"Training model {i+1} / {len(models)}.\")\n",
    "\n",
    "            # FIRST CHANGE HERE:\n",
    "            # initialize with base learning rate\n",
    "            current_lr = self.train_args['learning_rate']\n",
    "            \n",
    "            # SAME (except train_args['learning_rate'] replaced with current_lr):\n",
    "            # hack to initialize sbi model without training\n",
    "            first_round = False\n",
    "            if model._neural_net is None:\n",
    "                model.train(learning_rate=current_lr,\n",
    "                          resume_training=False,\n",
    "                          max_num_epochs=-1)\n",
    "                model._epochs_since_last_improvement = 0\n",
    "                first_round = True\n",
    "\n",
    "            # set train/validation splits\n",
    "            model.train_indices = train_indices\n",
    "            model.val_indices = val_indices\n",
    "\n",
    "\n",
    "            # train \n",
    "            \n",
    "            # CHANGED: \n",
    "            # Get learning rate schedule if it exists\n",
    "            lr_schedule = self.train_args.get('lr_schedule', {})\n",
    "            \n",
    "            # put training loop inside no. epochs:\n",
    "            # train whilst not completed epochs:\n",
    "            while model.epoch <= self.train_args.get('max_num_epochs', 4000):\n",
    "                # Check if we need to update learning rate\n",
    "                # learning rate depends on current epoch\n",
    "                if model.epoch in lr_schedule:\n",
    "                    current_lr = lr_schedule[model.epoch]\n",
    "                    for param_group in model.optimizer.param_groups:\n",
    "                        param_group['lr'] = current_lr\n",
    "                \n",
    "                # Train for one epoch\n",
    "                if (\"NPE\" in self.engine) & first_round:\n",
    "                    model.train(max_num_epochs=model.epoch + 1,\n",
    "                              learning_rate=current_lr,\n",
    "                              resume_training=True,\n",
    "                              force_first_round_loss=True)\n",
    "                else:\n",
    "                    model.train(max_num_epochs=model.epoch + 1,\n",
    "                              learning_rate=current_lr,\n",
    "                              resume_training=True)\n",
    "                \n",
    "                # Check convergence\n",
    "                if model._converged(model.epoch, \n",
    "                                  self.train_args.get('stop_after_epochs', 75)):\n",
    "                    break\n",
    "\n",
    "            posteriors.append(model.build_posterior())\n",
    "            summaries.append(model.summary)\n",
    "\n",
    "        # Ensemble all trained models\n",
    "        val_logprob = torch.tensor(\n",
    "            [float(x[\"best_validation_log_prob\"][-1]) for x in summaries]\n",
    "        ).to(self.device)\n",
    "        weights = torch.exp(val_logprob - val_logprob.max())\n",
    "        weights /= weights.sum()\n",
    "\n",
    "        posterior_ensemble = NeuralPosteriorEnsemble(\n",
    "            posteriors=posteriors,\n",
    "            weights=weights,\n",
    "            theta_transform=posteriors[0].theta_transform\n",
    "        )\n",
    "\n",
    "        posterior_ensemble.name = self.name\n",
    "        posterior_ensemble.signatures = self.signatures\n",
    "\n",
    "        return posterior_ensemble, summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveScheduledSBIRunner(SBIRunner):\n",
    "    def _train_round(self, models, x, theta, proposal):\n",
    "        \"\"\"Override _train_round to implement adaptive learning rate scheduling\"\"\"\n",
    "        \n",
    "        # append data to models\n",
    "        for model in models:\n",
    "            if (\"NPE\" in self.engine):\n",
    "                model = model.append_simulations(theta, x, proposal=proposal)\n",
    "            else:\n",
    "                model = model.append_simulations(theta, x)\n",
    "\n",
    "        # get all previous simulations\n",
    "        starting_round = 0\n",
    "        x, _, _ = model.get_simulations(starting_round)\n",
    "\n",
    "        # split into training and validation randomly\n",
    "        num_examples = x.shape[0]\n",
    "        permuted_indices = torch.randperm(num_examples)\n",
    "        num_training_examples = int(\n",
    "            (1 - self.train_args['validation_fraction']) * num_examples)\n",
    "        train_indices, val_indices = (\n",
    "            permuted_indices[:num_training_examples],\n",
    "            permuted_indices[num_training_examples:],\n",
    "        )\n",
    "\n",
    "        posteriors, summaries = [], []\n",
    "        for i, model in enumerate(models):\n",
    "            logging.info(f\"Training model {i+1} / {len(models)}.\")\n",
    "\n",
    "            # Initialize training variables\n",
    "            current_lr = self.train_args['learning_rate']\n",
    "            patience = self.train_args.get('patience', 20)  # Epochs to wait before reducing LR\n",
    "            min_lr = self.train_args.get('min_lr', 1e-7)   # Minimum learning rate\n",
    "            lr_reduction_factor = self.train_args.get('lr_reduction_factor', 0.5)\n",
    "            \n",
    "            # Variables for tracking improvement\n",
    "            best_val_loss = float('inf')\n",
    "            epochs_without_improvement = 0\n",
    "            \n",
    "            first_round = False\n",
    "            if model._neural_net is None:\n",
    "                model.train(learning_rate=current_lr,\n",
    "                          resume_training=False,\n",
    "                          max_num_epochs=-1)\n",
    "                model._epochs_since_last_improvement = 0\n",
    "                first_round = True\n",
    "\n",
    "            # set train/validation splits\n",
    "            model.train_indices = train_indices\n",
    "            model.val_indices = val_indices\n",
    "            \n",
    "            while model.epoch <= self.train_args.get('max_num_epochs', 4000):\n",
    "                prev_epoch = model.epoch\n",
    "                \n",
    "                # Train for one epoch\n",
    "                if (\"NPE\" in self.engine) & first_round:\n",
    "                    model.train(max_num_epochs=model.epoch + 1,\n",
    "                              learning_rate=current_lr,\n",
    "                              resume_training=True,\n",
    "                              force_first_round_loss=True)\n",
    "                else:\n",
    "                    model.train(max_num_epochs=model.epoch + 1,\n",
    "                              learning_rate=current_lr,\n",
    "                              resume_training=True)\n",
    "                \n",
    "                # Get current validation loss\n",
    "                current_val_loss = -float(model.summary[\"validation_log_prob\"][-1])\n",
    "                \n",
    "                # Check if we've improved\n",
    "                if current_val_loss < best_val_loss:\n",
    "                    best_val_loss = current_val_loss\n",
    "                    epochs_without_improvement = 0\n",
    "                    logging.info(f\"Model {i+1} improved: val_loss = {current_val_loss:.6f}, lr = {current_lr:.2e}\")\n",
    "                else:\n",
    "                    epochs_without_improvement += 1\n",
    "                    \n",
    "                # Adaptive learning rate reduction\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    if current_lr > min_lr:\n",
    "                        current_lr *= lr_reduction_factor\n",
    "                        current_lr = max(current_lr, min_lr)\n",
    "                        for param_group in model.optimizer.param_groups:\n",
    "                            param_group['lr'] = current_lr\n",
    "                        epochs_without_improvement = 0  # Reset counter\n",
    "                        logging.info(f\"Model {i+1} reducing lr to {current_lr:.2e} after {patience} epochs without improvement\")\n",
    "                    else:\n",
    "                        logging.info(f\"Model {i+1} reached minimum learning rate, continuing training\")\n",
    "                \n",
    "                # Check for convergence\n",
    "                if model._converged(model.epoch, \n",
    "                                  self.train_args.get('stop_after_epochs', 75)):\n",
    "                    logging.info(f\"Model {i+1} converged after {model.epoch} epochs\")\n",
    "                    break\n",
    "\n",
    "            posteriors.append(model.build_posterior())\n",
    "            summaries.append(model.summary)\n",
    "\n",
    "        # Ensemble creation remains the same\n",
    "        val_logprob = torch.tensor(\n",
    "            [float(x[\"best_validation_log_prob\"][-1]) for x in summaries]\n",
    "        ).to(self.device)\n",
    "        weights = torch.exp(val_logprob - val_logprob.max())\n",
    "        weights /= weights.sum()\n",
    "\n",
    "        posterior_ensemble = NeuralPosteriorEnsemble(\n",
    "            posteriors=posteriors,\n",
    "            weights=weights,\n",
    "            theta_transform=posteriors[0].theta_transform\n",
    "        )\n",
    "\n",
    "        posterior_ensemble.name = self.name\n",
    "        posterior_ensemble.signatures = self.signatures\n",
    "\n",
    "        return posterior_ensemble, summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-25, -14)\n"
     ]
    }
   ],
   "source": [
    "print(uvlf_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available simulations: ['SB28_0', 'SB28_1', 'SB28_10', 'SB28_100', 'SB28_1000', 'SB28_1001', 'SB28_1002', 'SB28_1003', 'SB28_1004', 'SB28_1005', 'SB28_1006', 'SB28_1007', 'SB28_1008', 'SB28_1009', 'SB28_101', 'SB28_1010', 'SB28_1011', 'SB28_1012', 'SB28_1013', 'SB28_1014', 'SB28_1015', 'SB28_1016', 'SB28_1017', 'SB28_1018', 'SB28_1019', 'SB28_102', 'SB28_1020', 'SB28_1021', 'SB28_1022', 'SB28_1023', 'SB28_1024', 'SB28_1025', 'SB28_1026', 'SB28_1027', 'SB28_1028', 'SB28_1029', 'SB28_103', 'SB28_1030', 'SB28_1031', 'SB28_1032', 'SB28_1033', 'SB28_1034', 'SB28_1035', 'SB28_1036', 'SB28_1037', 'SB28_1038', 'SB28_1039', 'SB28_104', 'SB28_1040', 'SB28_1041', 'SB28_1042', 'SB28_1043', 'SB28_1044', 'SB28_1045', 'SB28_1046', 'SB28_1047', 'SB28_1048', 'SB28_1049', 'SB28_105', 'SB28_1050', 'SB28_1051', 'SB28_1052', 'SB28_1053', 'SB28_1054', 'SB28_1055', 'SB28_1056', 'SB28_1057', 'SB28_1058', 'SB28_1059', 'SB28_106', 'SB28_1060', 'SB28_1061', 'SB28_1062', 'SB28_1063', 'SB28_1064', 'SB28_1065', 'SB28_1066', 'SB28_1067', 'SB28_1068', 'SB28_1069', 'SB28_107', 'SB28_1070', 'SB28_1071', 'SB28_1072', 'SB28_1073', 'SB28_1074', 'SB28_1075', 'SB28_1076', 'SB28_1077', 'SB28_1078', 'SB28_1079', 'SB28_108', 'SB28_1080', 'SB28_1081', 'SB28_1082', 'SB28_1083', 'SB28_1084', 'SB28_1085', 'SB28_1086', 'SB28_1087', 'SB28_1088', 'SB28_1089', 'SB28_109', 'SB28_1090', 'SB28_1091', 'SB28_1092', 'SB28_1093', 'SB28_1094', 'SB28_1095', 'SB28_1096', 'SB28_1097', 'SB28_1098', 'SB28_1099', 'SB28_11', 'SB28_110', 'SB28_1100', 'SB28_1101', 'SB28_1102', 'SB28_1103', 'SB28_1104', 'SB28_1105', 'SB28_1106', 'SB28_1107', 'SB28_1108', 'SB28_1109', 'SB28_111', 'SB28_1110', 'SB28_1111', 'SB28_1112', 'SB28_1113', 'SB28_1114', 'SB28_1115', 'SB28_1116', 'SB28_1117', 'SB28_1118', 'SB28_1119', 'SB28_112', 'SB28_1120', 'SB28_1121', 'SB28_1122', 'SB28_1123', 'SB28_1124', 'SB28_1125', 'SB28_1126', 'SB28_1127', 'SB28_1128', 'SB28_1129', 'SB28_113', 'SB28_1130', 'SB28_1131', 'SB28_1132', 'SB28_1133', 'SB28_1134', 'SB28_1135', 'SB28_1136', 'SB28_1137', 'SB28_1138', 'SB28_1139', 'SB28_114', 'SB28_1140', 'SB28_1141', 'SB28_1142', 'SB28_1143', 'SB28_1144', 'SB28_1145', 'SB28_1146', 'SB28_1147', 'SB28_1148', 'SB28_1149', 'SB28_115', 'SB28_1150', 'SB28_1151', 'SB28_1152', 'SB28_1153', 'SB28_1154', 'SB28_1155', 'SB28_1156', 'SB28_1157', 'SB28_1158', 'SB28_1159', 'SB28_116', 'SB28_1160', 'SB28_1161', 'SB28_1162', 'SB28_1163', 'SB28_1164', 'SB28_1165', 'SB28_1166', 'SB28_1167', 'SB28_1168', 'SB28_1169', 'SB28_117', 'SB28_1170', 'SB28_1171', 'SB28_1172', 'SB28_1173', 'SB28_1174', 'SB28_1175', 'SB28_1176', 'SB28_1177', 'SB28_1178', 'SB28_1179', 'SB28_118', 'SB28_1180', 'SB28_1181', 'SB28_1182', 'SB28_1183', 'SB28_1184', 'SB28_1185', 'SB28_1186', 'SB28_1187', 'SB28_1188', 'SB28_1189', 'SB28_119', 'SB28_1190', 'SB28_1191', 'SB28_1192', 'SB28_1193', 'SB28_1194', 'SB28_1195', 'SB28_1196', 'SB28_1197', 'SB28_1198', 'SB28_1199', 'SB28_12', 'SB28_120', 'SB28_1200', 'SB28_1201', 'SB28_1202', 'SB28_1203', 'SB28_1204', 'SB28_1205', 'SB28_1206', 'SB28_1207', 'SB28_1208', 'SB28_1209', 'SB28_121', 'SB28_1210', 'SB28_1211', 'SB28_1212', 'SB28_1213', 'SB28_1214', 'SB28_1215', 'SB28_1216', 'SB28_1217', 'SB28_1218', 'SB28_1219', 'SB28_122', 'SB28_1220', 'SB28_1221', 'SB28_1222', 'SB28_1223', 'SB28_1224', 'SB28_1225', 'SB28_1226', 'SB28_1227', 'SB28_1228', 'SB28_1229', 'SB28_123', 'SB28_1230', 'SB28_1231', 'SB28_1232', 'SB28_1233', 'SB28_1234', 'SB28_1235', 'SB28_1236', 'SB28_1237', 'SB28_1238', 'SB28_1239', 'SB28_124', 'SB28_1240', 'SB28_1241', 'SB28_1242', 'SB28_1243', 'SB28_1244', 'SB28_1245', 'SB28_1246', 'SB28_1247', 'SB28_1248', 'SB28_1249', 'SB28_125', 'SB28_1250', 'SB28_1251', 'SB28_1252', 'SB28_1253', 'SB28_1254', 'SB28_1255', 'SB28_1256', 'SB28_1257', 'SB28_1258', 'SB28_1259', 'SB28_126', 'SB28_1260', 'SB28_1261', 'SB28_1262', 'SB28_1263', 'SB28_1264', 'SB28_1265', 'SB28_1266', 'SB28_1267', 'SB28_1268', 'SB28_1269', 'SB28_127', 'SB28_1270', 'SB28_1271', 'SB28_1272', 'SB28_1273', 'SB28_1274', 'SB28_1275', 'SB28_1276', 'SB28_1277', 'SB28_1278', 'SB28_1279', 'SB28_128', 'SB28_1280', 'SB28_1281', 'SB28_1282', 'SB28_1283', 'SB28_1284', 'SB28_1285', 'SB28_1286', 'SB28_1287', 'SB28_1288', 'SB28_1289', 'SB28_129', 'SB28_1290', 'SB28_1291', 'SB28_1292', 'SB28_1293', 'SB28_1294', 'SB28_1295', 'SB28_1296', 'SB28_1297', 'SB28_1298', 'SB28_1299', 'SB28_13', 'SB28_130', 'SB28_1300', 'SB28_1301', 'SB28_1302', 'SB28_1303', 'SB28_1304', 'SB28_1305', 'SB28_1306', 'SB28_1307', 'SB28_1308', 'SB28_1309', 'SB28_131', 'SB28_1310', 'SB28_1311', 'SB28_1312', 'SB28_1313', 'SB28_1314', 'SB28_1315', 'SB28_1316', 'SB28_1317', 'SB28_1318', 'SB28_1319', 'SB28_132', 'SB28_1320', 'SB28_1321', 'SB28_1322', 'SB28_1323', 'SB28_1324', 'SB28_1325', 'SB28_1326', 'SB28_1327', 'SB28_1328', 'SB28_1329', 'SB28_133', 'SB28_1330', 'SB28_1331', 'SB28_1332', 'SB28_1333', 'SB28_1334', 'SB28_1335', 'SB28_1336', 'SB28_1337', 'SB28_1338', 'SB28_1339', 'SB28_134', 'SB28_1340', 'SB28_1341', 'SB28_1342', 'SB28_1343', 'SB28_1344', 'SB28_1345', 'SB28_1346', 'SB28_1347', 'SB28_1348', 'SB28_1349', 'SB28_135', 'SB28_1350', 'SB28_1351', 'SB28_1352', 'SB28_1353', 'SB28_1354', 'SB28_1355', 'SB28_1356', 'SB28_1357', 'SB28_1358', 'SB28_1359', 'SB28_136', 'SB28_1360', 'SB28_1361', 'SB28_1362', 'SB28_1363', 'SB28_1364', 'SB28_1365', 'SB28_1366', 'SB28_1367', 'SB28_1368', 'SB28_1369', 'SB28_137', 'SB28_1370', 'SB28_1371', 'SB28_1372', 'SB28_1373', 'SB28_1374', 'SB28_1375', 'SB28_1376', 'SB28_1377', 'SB28_1378', 'SB28_1379', 'SB28_138', 'SB28_1380', 'SB28_1381', 'SB28_1382', 'SB28_1383', 'SB28_1384', 'SB28_1385', 'SB28_1386', 'SB28_1387', 'SB28_1388', 'SB28_1389', 'SB28_139', 'SB28_1390', 'SB28_1391', 'SB28_1392', 'SB28_1393', 'SB28_1394', 'SB28_1395', 'SB28_1396', 'SB28_1397', 'SB28_1398', 'SB28_1399', 'SB28_14', 'SB28_140', 'SB28_1400', 'SB28_1401', 'SB28_1402', 'SB28_1403', 'SB28_1404', 'SB28_1405', 'SB28_1406', 'SB28_1407', 'SB28_1408', 'SB28_1409', 'SB28_141', 'SB28_1410', 'SB28_1411', 'SB28_1412', 'SB28_1413', 'SB28_1414', 'SB28_1415', 'SB28_1416', 'SB28_1417', 'SB28_1418', 'SB28_1419', 'SB28_142', 'SB28_1420', 'SB28_1421', 'SB28_1422', 'SB28_1423', 'SB28_1424', 'SB28_1425', 'SB28_1426', 'SB28_1427', 'SB28_1428', 'SB28_1429', 'SB28_143', 'SB28_1430', 'SB28_1431', 'SB28_1432', 'SB28_1433', 'SB28_1434', 'SB28_1435', 'SB28_1436', 'SB28_1437', 'SB28_1438', 'SB28_1439', 'SB28_144', 'SB28_1440', 'SB28_1441', 'SB28_1442', 'SB28_1443', 'SB28_1444', 'SB28_1445', 'SB28_1446', 'SB28_1447', 'SB28_1448', 'SB28_1449', 'SB28_145', 'SB28_1450', 'SB28_1451', 'SB28_1452', 'SB28_1453', 'SB28_1454', 'SB28_1455', 'SB28_1456', 'SB28_1457', 'SB28_1458', 'SB28_1459', 'SB28_146', 'SB28_1460', 'SB28_1461', 'SB28_1462', 'SB28_1463', 'SB28_1464', 'SB28_1465', 'SB28_1466', 'SB28_1467', 'SB28_1468', 'SB28_1469', 'SB28_147', 'SB28_1470', 'SB28_1471', 'SB28_1472', 'SB28_1473', 'SB28_1474', 'SB28_1475', 'SB28_1476', 'SB28_1477', 'SB28_1478', 'SB28_1479', 'SB28_148', 'SB28_1480', 'SB28_1481', 'SB28_1482', 'SB28_1483', 'SB28_1484', 'SB28_1485', 'SB28_1486', 'SB28_1487', 'SB28_1488', 'SB28_1489', 'SB28_149', 'SB28_1490', 'SB28_1491', 'SB28_1492', 'SB28_1493', 'SB28_1494', 'SB28_1495', 'SB28_1496', 'SB28_1497', 'SB28_1498', 'SB28_1499', 'SB28_15', 'SB28_150', 'SB28_1500', 'SB28_1501', 'SB28_1502', 'SB28_1503', 'SB28_1504', 'SB28_1505', 'SB28_1506', 'SB28_1507', 'SB28_1508', 'SB28_1509', 'SB28_151', 'SB28_1510', 'SB28_1511', 'SB28_1512', 'SB28_1513', 'SB28_1514', 'SB28_1515', 'SB28_1516', 'SB28_1517', 'SB28_1518', 'SB28_1519', 'SB28_152', 'SB28_1520', 'SB28_1521', 'SB28_1522', 'SB28_1523', 'SB28_1524', 'SB28_1525', 'SB28_1526', 'SB28_1527', 'SB28_1528', 'SB28_1529', 'SB28_153', 'SB28_1530', 'SB28_1531', 'SB28_1532', 'SB28_1533', 'SB28_1534', 'SB28_1535', 'SB28_1536', 'SB28_1537', 'SB28_1538', 'SB28_1539', 'SB28_154', 'SB28_1540', 'SB28_1541', 'SB28_1542', 'SB28_1543', 'SB28_1544', 'SB28_1545', 'SB28_1546', 'SB28_1547', 'SB28_1548', 'SB28_1549', 'SB28_155', 'SB28_1550', 'SB28_1551', 'SB28_1552', 'SB28_1553', 'SB28_1554', 'SB28_1555', 'SB28_1556', 'SB28_1557', 'SB28_1558', 'SB28_1559', 'SB28_156', 'SB28_1560', 'SB28_1561', 'SB28_1562', 'SB28_1563', 'SB28_1564', 'SB28_1565', 'SB28_1566', 'SB28_1567', 'SB28_1568', 'SB28_1569', 'SB28_157', 'SB28_1570', 'SB28_1571', 'SB28_1572', 'SB28_1573', 'SB28_1574', 'SB28_1575', 'SB28_1576', 'SB28_1577', 'SB28_1578', 'SB28_1579', 'SB28_158', 'SB28_1580', 'SB28_1581', 'SB28_1582', 'SB28_1583', 'SB28_1584', 'SB28_1585', 'SB28_1586', 'SB28_1587', 'SB28_1588', 'SB28_1589', 'SB28_159', 'SB28_1590', 'SB28_1591', 'SB28_1592', 'SB28_1593', 'SB28_1594', 'SB28_1595', 'SB28_1596', 'SB28_1597', 'SB28_1598', 'SB28_1599', 'SB28_16', 'SB28_160', 'SB28_1600', 'SB28_1601', 'SB28_1602', 'SB28_1603', 'SB28_1604', 'SB28_1605', 'SB28_1606', 'SB28_1607', 'SB28_1608', 'SB28_1609', 'SB28_161', 'SB28_1610', 'SB28_1611', 'SB28_1612', 'SB28_1613', 'SB28_1614', 'SB28_1615', 'SB28_1616', 'SB28_1617', 'SB28_1618', 'SB28_1619', 'SB28_162', 'SB28_1620', 'SB28_1621', 'SB28_1622', 'SB28_1623', 'SB28_1624', 'SB28_1625', 'SB28_1626', 'SB28_1627', 'SB28_1628', 'SB28_1629', 'SB28_163', 'SB28_1630', 'SB28_1631', 'SB28_1632', 'SB28_1633', 'SB28_1634', 'SB28_1635', 'SB28_1636', 'SB28_1637', 'SB28_1638', 'SB28_1639', 'SB28_164', 'SB28_1640', 'SB28_1641', 'SB28_1642', 'SB28_1643', 'SB28_1644', 'SB28_1645', 'SB28_1646', 'SB28_1647', 'SB28_1648', 'SB28_1649', 'SB28_165', 'SB28_1650', 'SB28_1651', 'SB28_1652', 'SB28_1653', 'SB28_1654', 'SB28_1655', 'SB28_1656', 'SB28_1657', 'SB28_1658', 'SB28_1659', 'SB28_166', 'SB28_1660', 'SB28_1661', 'SB28_1662', 'SB28_1663', 'SB28_1664', 'SB28_1665', 'SB28_1666', 'SB28_1667', 'SB28_1668', 'SB28_1669', 'SB28_167', 'SB28_1670', 'SB28_1671', 'SB28_1672', 'SB28_1673', 'SB28_1674', 'SB28_1675', 'SB28_1676', 'SB28_1677', 'SB28_1678', 'SB28_1679', 'SB28_168', 'SB28_1680', 'SB28_1681', 'SB28_1682', 'SB28_1683', 'SB28_1684', 'SB28_1685', 'SB28_1686', 'SB28_1687', 'SB28_1688', 'SB28_1689', 'SB28_169', 'SB28_1690', 'SB28_1691', 'SB28_1692', 'SB28_1693', 'SB28_1694', 'SB28_1695', 'SB28_1696', 'SB28_1697', 'SB28_1698', 'SB28_1699', 'SB28_17', 'SB28_170', 'SB28_1700', 'SB28_1701', 'SB28_1702', 'SB28_1703', 'SB28_1704', 'SB28_1705', 'SB28_1706', 'SB28_1707', 'SB28_1708', 'SB28_1709', 'SB28_171', 'SB28_1710', 'SB28_1711', 'SB28_1712', 'SB28_1713', 'SB28_1714', 'SB28_1715', 'SB28_1716', 'SB28_1717', 'SB28_1718', 'SB28_1719', 'SB28_172', 'SB28_1720', 'SB28_1721', 'SB28_1722', 'SB28_1723', 'SB28_1724', 'SB28_1725', 'SB28_1726', 'SB28_1727', 'SB28_1728', 'SB28_1729', 'SB28_173', 'SB28_1730', 'SB28_1731', 'SB28_1732', 'SB28_1733', 'SB28_1734', 'SB28_1735', 'SB28_1736', 'SB28_1737', 'SB28_1738', 'SB28_1739', 'SB28_174', 'SB28_1740', 'SB28_1741', 'SB28_1742', 'SB28_1743', 'SB28_1744', 'SB28_1745', 'SB28_1746', 'SB28_1747', 'SB28_1748', 'SB28_1749', 'SB28_175', 'SB28_1750', 'SB28_1751', 'SB28_1752', 'SB28_1753', 'SB28_1754', 'SB28_1755', 'SB28_1756', 'SB28_1757', 'SB28_1758', 'SB28_1759', 'SB28_176', 'SB28_1760', 'SB28_1761', 'SB28_1762', 'SB28_1763', 'SB28_1764', 'SB28_1765', 'SB28_1766', 'SB28_1767', 'SB28_1768', 'SB28_1769', 'SB28_177', 'SB28_1770', 'SB28_1771', 'SB28_1772', 'SB28_1773', 'SB28_1774', 'SB28_1775', 'SB28_1776', 'SB28_1777', 'SB28_1778', 'SB28_1779', 'SB28_178', 'SB28_1780', 'SB28_1781', 'SB28_1782', 'SB28_1783', 'SB28_1784', 'SB28_1785', 'SB28_1786', 'SB28_1787', 'SB28_1788', 'SB28_1789', 'SB28_179', 'SB28_1790', 'SB28_1791', 'SB28_1792', 'SB28_1793', 'SB28_1794', 'SB28_1795', 'SB28_1796', 'SB28_1797', 'SB28_1798', 'SB28_1799', 'SB28_18', 'SB28_180', 'SB28_1800', 'SB28_1801', 'SB28_1802', 'SB28_1803', 'SB28_1804', 'SB28_1805', 'SB28_1806', 'SB28_1807', 'SB28_1808', 'SB28_1809', 'SB28_181', 'SB28_1810', 'SB28_1811', 'SB28_1812', 'SB28_1813', 'SB28_1814', 'SB28_1815', 'SB28_1816', 'SB28_1817', 'SB28_1818', 'SB28_1819', 'SB28_182', 'SB28_1820', 'SB28_1821', 'SB28_1822', 'SB28_1823', 'SB28_1824', 'SB28_1825', 'SB28_1826', 'SB28_1827', 'SB28_1828', 'SB28_1829', 'SB28_183', 'SB28_1830', 'SB28_1831', 'SB28_1832', 'SB28_1833', 'SB28_1834', 'SB28_1835', 'SB28_1836', 'SB28_1837', 'SB28_1838', 'SB28_1839', 'SB28_184', 'SB28_1840', 'SB28_1841', 'SB28_1842', 'SB28_1843', 'SB28_1844', 'SB28_1845', 'SB28_1846', 'SB28_1847', 'SB28_1848', 'SB28_1849', 'SB28_185', 'SB28_1850', 'SB28_1851', 'SB28_1852', 'SB28_1853', 'SB28_1854', 'SB28_1855', 'SB28_1856', 'SB28_1857', 'SB28_1858', 'SB28_1859', 'SB28_186', 'SB28_1860', 'SB28_1861', 'SB28_1862', 'SB28_1863', 'SB28_1864', 'SB28_1865', 'SB28_1866', 'SB28_1867', 'SB28_1868', 'SB28_1869', 'SB28_187', 'SB28_1870', 'SB28_1871', 'SB28_1872', 'SB28_1873', 'SB28_1874', 'SB28_1875', 'SB28_1876', 'SB28_1877', 'SB28_1878', 'SB28_1879', 'SB28_188', 'SB28_1880', 'SB28_1881', 'SB28_1882', 'SB28_1883', 'SB28_1884', 'SB28_1885', 'SB28_1886', 'SB28_1887', 'SB28_1888', 'SB28_1889', 'SB28_189', 'SB28_1890', 'SB28_1891', 'SB28_1892', 'SB28_1893', 'SB28_1894', 'SB28_1895', 'SB28_1896', 'SB28_1897', 'SB28_1898', 'SB28_1899', 'SB28_19', 'SB28_190', 'SB28_1900', 'SB28_1901', 'SB28_1902', 'SB28_1903', 'SB28_1904', 'SB28_1905', 'SB28_1906', 'SB28_1907', 'SB28_1908', 'SB28_1909', 'SB28_191', 'SB28_1910', 'SB28_1911', 'SB28_1912', 'SB28_1913', 'SB28_1914', 'SB28_1915', 'SB28_1916', 'SB28_1917', 'SB28_1918', 'SB28_1919', 'SB28_192', 'SB28_1920', 'SB28_1921', 'SB28_1922', 'SB28_1923', 'SB28_1924', 'SB28_1925', 'SB28_1926', 'SB28_1927', 'SB28_1928', 'SB28_1929', 'SB28_193', 'SB28_1930', 'SB28_1931', 'SB28_1932', 'SB28_1933', 'SB28_1934', 'SB28_1935', 'SB28_1936', 'SB28_1937', 'SB28_1938', 'SB28_1939', 'SB28_194', 'SB28_1940', 'SB28_1941', 'SB28_1942', 'SB28_1943', 'SB28_1944', 'SB28_1945', 'SB28_1946', 'SB28_1947', 'SB28_1948', 'SB28_1949', 'SB28_195', 'SB28_1950', 'SB28_1951', 'SB28_1952', 'SB28_1953', 'SB28_1954', 'SB28_1955', 'SB28_1956', 'SB28_1957', 'SB28_1958', 'SB28_1959', 'SB28_196', 'SB28_1960', 'SB28_1961', 'SB28_1962', 'SB28_1963', 'SB28_1964', 'SB28_1965', 'SB28_1966', 'SB28_1967', 'SB28_1968', 'SB28_1969', 'SB28_197', 'SB28_1970', 'SB28_1971', 'SB28_1972', 'SB28_1973', 'SB28_1974', 'SB28_1975', 'SB28_1976', 'SB28_1977', 'SB28_1978', 'SB28_1979', 'SB28_198', 'SB28_1980', 'SB28_1981', 'SB28_1982', 'SB28_1983', 'SB28_1984', 'SB28_1985', 'SB28_1986', 'SB28_1987', 'SB28_1988', 'SB28_1989', 'SB28_199', 'SB28_1990', 'SB28_1991', 'SB28_1992', 'SB28_1993', 'SB28_1994', 'SB28_1995', 'SB28_1996', 'SB28_1997', 'SB28_1998', 'SB28_1999', 'SB28_2', 'SB28_20', 'SB28_200', 'SB28_2000', 'SB28_2001', 'SB28_2002', 'SB28_2003', 'SB28_2004', 'SB28_2005', 'SB28_2006', 'SB28_2007', 'SB28_2008', 'SB28_2009', 'SB28_201', 'SB28_2010', 'SB28_2011', 'SB28_2012', 'SB28_2013', 'SB28_2014', 'SB28_2015', 'SB28_2016', 'SB28_2017', 'SB28_2018', 'SB28_2019', 'SB28_202', 'SB28_2020', 'SB28_2021', 'SB28_2022', 'SB28_2023', 'SB28_2024', 'SB28_2025', 'SB28_2026', 'SB28_2027', 'SB28_2028', 'SB28_2029', 'SB28_203', 'SB28_2030', 'SB28_2031', 'SB28_2032', 'SB28_2033', 'SB28_2034', 'SB28_2035', 'SB28_2036', 'SB28_2037', 'SB28_2038', 'SB28_2039', 'SB28_204', 'SB28_2040', 'SB28_2041', 'SB28_2042', 'SB28_2043', 'SB28_2044', 'SB28_2045', 'SB28_2046', 'SB28_2047', 'SB28_205', 'SB28_206', 'SB28_207', 'SB28_208', 'SB28_209', 'SB28_21', 'SB28_210', 'SB28_211', 'SB28_212', 'SB28_213', 'SB28_214', 'SB28_215', 'SB28_216', 'SB28_217', 'SB28_218', 'SB28_219', 'SB28_22', 'SB28_220', 'SB28_221', 'SB28_222', 'SB28_223', 'SB28_224', 'SB28_225', 'SB28_226', 'SB28_227', 'SB28_228', 'SB28_229', 'SB28_23', 'SB28_230', 'SB28_231', 'SB28_232', 'SB28_233', 'SB28_234', 'SB28_235', 'SB28_236', 'SB28_237', 'SB28_238', 'SB28_239', 'SB28_24', 'SB28_240', 'SB28_241', 'SB28_242', 'SB28_243', 'SB28_244', 'SB28_245', 'SB28_246', 'SB28_247', 'SB28_248', 'SB28_249', 'SB28_25', 'SB28_250', 'SB28_251', 'SB28_252', 'SB28_253', 'SB28_254', 'SB28_255', 'SB28_256', 'SB28_257', 'SB28_258', 'SB28_259', 'SB28_26', 'SB28_260', 'SB28_261', 'SB28_262', 'SB28_263', 'SB28_264', 'SB28_265', 'SB28_266', 'SB28_267', 'SB28_268', 'SB28_269', 'SB28_27', 'SB28_270', 'SB28_271', 'SB28_272', 'SB28_273', 'SB28_274', 'SB28_275', 'SB28_276', 'SB28_277', 'SB28_278', 'SB28_279', 'SB28_28', 'SB28_280', 'SB28_281', 'SB28_282', 'SB28_283', 'SB28_284', 'SB28_285', 'SB28_286', 'SB28_287', 'SB28_288', 'SB28_289', 'SB28_29', 'SB28_290', 'SB28_291', 'SB28_292', 'SB28_293', 'SB28_294', 'SB28_295', 'SB28_296', 'SB28_297', 'SB28_298', 'SB28_299', 'SB28_3', 'SB28_30', 'SB28_300', 'SB28_301', 'SB28_302', 'SB28_303', 'SB28_304', 'SB28_305', 'SB28_306', 'SB28_307', 'SB28_308', 'SB28_309', 'SB28_31', 'SB28_310', 'SB28_311', 'SB28_312', 'SB28_313', 'SB28_314', 'SB28_315', 'SB28_316', 'SB28_317', 'SB28_318', 'SB28_319', 'SB28_32', 'SB28_320', 'SB28_321', 'SB28_322', 'SB28_323', 'SB28_324', 'SB28_325', 'SB28_326', 'SB28_327', 'SB28_328', 'SB28_329', 'SB28_33', 'SB28_330', 'SB28_331', 'SB28_332', 'SB28_333', 'SB28_334', 'SB28_335', 'SB28_336', 'SB28_337', 'SB28_338', 'SB28_339', 'SB28_34', 'SB28_340', 'SB28_341', 'SB28_342', 'SB28_343', 'SB28_344', 'SB28_345', 'SB28_346', 'SB28_347', 'SB28_348', 'SB28_349', 'SB28_35', 'SB28_350', 'SB28_351', 'SB28_352', 'SB28_353', 'SB28_354', 'SB28_355', 'SB28_356', 'SB28_357', 'SB28_358', 'SB28_359', 'SB28_36', 'SB28_360', 'SB28_361', 'SB28_362', 'SB28_363', 'SB28_364', 'SB28_365', 'SB28_366', 'SB28_367', 'SB28_368', 'SB28_369', 'SB28_37', 'SB28_370', 'SB28_371', 'SB28_372', 'SB28_373', 'SB28_374', 'SB28_375', 'SB28_376', 'SB28_377', 'SB28_378', 'SB28_379', 'SB28_38', 'SB28_380', 'SB28_381', 'SB28_382', 'SB28_383', 'SB28_384', 'SB28_385', 'SB28_386', 'SB28_387', 'SB28_388', 'SB28_389', 'SB28_39', 'SB28_390', 'SB28_391', 'SB28_392', 'SB28_393', 'SB28_394', 'SB28_395', 'SB28_396', 'SB28_397', 'SB28_398', 'SB28_399', 'SB28_4', 'SB28_40', 'SB28_400', 'SB28_401', 'SB28_402', 'SB28_403', 'SB28_404', 'SB28_405', 'SB28_406', 'SB28_407', 'SB28_408', 'SB28_409', 'SB28_41', 'SB28_410', 'SB28_411', 'SB28_412', 'SB28_413', 'SB28_414', 'SB28_415', 'SB28_416', 'SB28_417', 'SB28_418', 'SB28_419', 'SB28_42', 'SB28_420', 'SB28_421', 'SB28_422', 'SB28_423', 'SB28_424', 'SB28_425', 'SB28_426', 'SB28_427', 'SB28_428', 'SB28_429', 'SB28_43', 'SB28_430', 'SB28_431', 'SB28_432', 'SB28_433', 'SB28_434', 'SB28_435', 'SB28_436', 'SB28_437', 'SB28_438', 'SB28_439', 'SB28_44', 'SB28_440', 'SB28_441', 'SB28_442', 'SB28_443', 'SB28_444', 'SB28_445', 'SB28_446', 'SB28_447', 'SB28_448', 'SB28_449', 'SB28_45', 'SB28_450', 'SB28_451', 'SB28_452', 'SB28_453', 'SB28_454', 'SB28_455', 'SB28_456', 'SB28_457', 'SB28_458', 'SB28_459', 'SB28_46', 'SB28_460', 'SB28_461', 'SB28_462', 'SB28_463', 'SB28_464', 'SB28_465', 'SB28_466', 'SB28_467', 'SB28_468', 'SB28_469', 'SB28_47', 'SB28_470', 'SB28_471', 'SB28_472', 'SB28_473', 'SB28_474', 'SB28_475', 'SB28_476', 'SB28_477', 'SB28_478', 'SB28_479', 'SB28_48', 'SB28_480', 'SB28_481', 'SB28_482', 'SB28_483', 'SB28_484', 'SB28_485', 'SB28_486', 'SB28_487', 'SB28_488', 'SB28_489', 'SB28_49', 'SB28_490', 'SB28_491', 'SB28_492', 'SB28_493', 'SB28_494', 'SB28_495', 'SB28_496', 'SB28_497', 'SB28_498', 'SB28_499', 'SB28_5', 'SB28_50', 'SB28_500', 'SB28_501', 'SB28_502', 'SB28_503', 'SB28_504', 'SB28_505', 'SB28_506', 'SB28_507', 'SB28_508', 'SB28_509', 'SB28_51', 'SB28_510', 'SB28_511', 'SB28_512', 'SB28_513', 'SB28_514', 'SB28_515', 'SB28_516', 'SB28_517', 'SB28_518', 'SB28_519', 'SB28_52', 'SB28_520', 'SB28_521', 'SB28_522', 'SB28_523', 'SB28_524', 'SB28_525', 'SB28_526', 'SB28_527', 'SB28_528', 'SB28_529', 'SB28_53', 'SB28_530', 'SB28_531', 'SB28_532', 'SB28_533', 'SB28_534', 'SB28_535', 'SB28_536', 'SB28_537', 'SB28_538', 'SB28_539', 'SB28_54', 'SB28_540', 'SB28_541', 'SB28_542', 'SB28_543', 'SB28_544', 'SB28_545', 'SB28_546', 'SB28_547', 'SB28_548', 'SB28_549', 'SB28_55', 'SB28_550', 'SB28_551', 'SB28_552', 'SB28_553', 'SB28_554', 'SB28_555', 'SB28_556', 'SB28_557', 'SB28_558', 'SB28_559', 'SB28_56', 'SB28_560', 'SB28_561', 'SB28_562', 'SB28_563', 'SB28_564', 'SB28_565', 'SB28_566', 'SB28_567', 'SB28_568', 'SB28_569', 'SB28_57', 'SB28_570', 'SB28_571', 'SB28_572', 'SB28_573', 'SB28_574', 'SB28_575', 'SB28_576', 'SB28_577', 'SB28_578', 'SB28_579', 'SB28_58', 'SB28_580', 'SB28_581', 'SB28_582', 'SB28_583', 'SB28_584', 'SB28_585', 'SB28_586', 'SB28_587', 'SB28_588', 'SB28_589', 'SB28_59', 'SB28_590', 'SB28_591', 'SB28_592', 'SB28_593', 'SB28_594', 'SB28_595', 'SB28_596', 'SB28_597', 'SB28_598', 'SB28_599', 'SB28_6', 'SB28_60', 'SB28_600', 'SB28_601', 'SB28_602', 'SB28_603', 'SB28_604', 'SB28_605', 'SB28_606', 'SB28_607', 'SB28_608', 'SB28_609', 'SB28_61', 'SB28_610', 'SB28_611', 'SB28_612', 'SB28_613', 'SB28_614', 'SB28_615', 'SB28_616', 'SB28_617', 'SB28_618', 'SB28_619', 'SB28_62', 'SB28_620', 'SB28_621', 'SB28_622', 'SB28_623', 'SB28_624', 'SB28_625', 'SB28_626', 'SB28_627', 'SB28_628', 'SB28_629', 'SB28_63', 'SB28_630', 'SB28_631', 'SB28_632', 'SB28_633', 'SB28_634', 'SB28_635', 'SB28_636', 'SB28_637', 'SB28_638', 'SB28_639', 'SB28_64', 'SB28_640', 'SB28_641', 'SB28_642', 'SB28_643', 'SB28_644', 'SB28_645', 'SB28_646', 'SB28_647', 'SB28_648', 'SB28_649', 'SB28_65', 'SB28_650', 'SB28_651', 'SB28_652', 'SB28_653', 'SB28_654', 'SB28_655', 'SB28_656', 'SB28_657', 'SB28_658', 'SB28_659', 'SB28_66', 'SB28_660', 'SB28_661', 'SB28_662', 'SB28_663', 'SB28_664', 'SB28_665', 'SB28_666', 'SB28_667', 'SB28_668', 'SB28_669', 'SB28_67', 'SB28_670', 'SB28_671', 'SB28_672', 'SB28_673', 'SB28_674', 'SB28_675', 'SB28_676', 'SB28_677', 'SB28_678', 'SB28_679', 'SB28_68', 'SB28_680', 'SB28_681', 'SB28_682', 'SB28_683', 'SB28_684', 'SB28_685', 'SB28_686', 'SB28_687', 'SB28_688', 'SB28_689', 'SB28_69', 'SB28_690', 'SB28_691', 'SB28_692', 'SB28_693', 'SB28_694', 'SB28_695', 'SB28_696', 'SB28_697', 'SB28_698', 'SB28_699', 'SB28_7', 'SB28_70', 'SB28_700', 'SB28_701', 'SB28_702', 'SB28_703', 'SB28_704', 'SB28_705', 'SB28_706', 'SB28_707', 'SB28_708', 'SB28_709', 'SB28_71', 'SB28_710', 'SB28_711', 'SB28_712', 'SB28_713', 'SB28_714', 'SB28_715', 'SB28_716', 'SB28_717', 'SB28_718', 'SB28_719', 'SB28_72', 'SB28_720', 'SB28_721', 'SB28_722', 'SB28_723', 'SB28_724', 'SB28_725', 'SB28_726', 'SB28_727', 'SB28_728', 'SB28_729', 'SB28_73', 'SB28_730', 'SB28_731', 'SB28_732', 'SB28_733', 'SB28_734', 'SB28_735', 'SB28_736', 'SB28_737', 'SB28_738', 'SB28_739', 'SB28_74', 'SB28_740', 'SB28_741', 'SB28_742', 'SB28_743', 'SB28_744', 'SB28_745', 'SB28_746', 'SB28_747', 'SB28_748', 'SB28_749', 'SB28_75', 'SB28_750', 'SB28_751', 'SB28_752', 'SB28_753', 'SB28_754', 'SB28_755', 'SB28_756', 'SB28_757', 'SB28_758', 'SB28_759', 'SB28_76', 'SB28_760', 'SB28_761', 'SB28_762', 'SB28_763', 'SB28_764', 'SB28_765', 'SB28_766', 'SB28_767', 'SB28_768', 'SB28_769', 'SB28_77', 'SB28_770', 'SB28_771', 'SB28_772', 'SB28_773', 'SB28_774', 'SB28_775', 'SB28_776', 'SB28_777', 'SB28_778', 'SB28_779', 'SB28_78', 'SB28_780', 'SB28_781', 'SB28_782', 'SB28_783', 'SB28_784', 'SB28_785', 'SB28_786', 'SB28_787', 'SB28_788', 'SB28_789', 'SB28_79', 'SB28_790', 'SB28_791', 'SB28_792', 'SB28_793', 'SB28_794', 'SB28_795', 'SB28_796', 'SB28_797', 'SB28_798', 'SB28_799', 'SB28_8', 'SB28_80', 'SB28_800', 'SB28_801', 'SB28_802', 'SB28_803', 'SB28_804', 'SB28_805', 'SB28_806', 'SB28_807', 'SB28_808', 'SB28_809', 'SB28_81', 'SB28_810', 'SB28_811', 'SB28_812', 'SB28_813', 'SB28_814', 'SB28_815', 'SB28_816', 'SB28_817', 'SB28_818', 'SB28_819', 'SB28_82', 'SB28_820', 'SB28_821', 'SB28_822', 'SB28_823', 'SB28_824', 'SB28_825', 'SB28_826', 'SB28_827', 'SB28_828', 'SB28_829', 'SB28_83', 'SB28_830', 'SB28_831', 'SB28_832', 'SB28_833', 'SB28_834', 'SB28_835', 'SB28_836', 'SB28_837', 'SB28_838', 'SB28_839', 'SB28_84', 'SB28_840', 'SB28_841', 'SB28_842', 'SB28_843', 'SB28_844', 'SB28_845', 'SB28_846', 'SB28_847', 'SB28_848', 'SB28_849', 'SB28_85', 'SB28_850', 'SB28_851', 'SB28_852', 'SB28_853', 'SB28_854', 'SB28_855', 'SB28_856', 'SB28_857', 'SB28_858', 'SB28_859', 'SB28_86', 'SB28_860', 'SB28_861', 'SB28_862', 'SB28_863', 'SB28_864', 'SB28_865', 'SB28_866', 'SB28_867', 'SB28_868', 'SB28_869', 'SB28_87', 'SB28_870', 'SB28_871', 'SB28_872', 'SB28_873', 'SB28_874', 'SB28_875', 'SB28_876', 'SB28_877', 'SB28_878', 'SB28_879', 'SB28_88', 'SB28_880', 'SB28_881', 'SB28_882', 'SB28_883', 'SB28_884', 'SB28_885', 'SB28_886', 'SB28_887', 'SB28_888', 'SB28_889', 'SB28_89', 'SB28_890', 'SB28_891', 'SB28_892', 'SB28_893', 'SB28_894', 'SB28_895', 'SB28_896', 'SB28_897', 'SB28_898', 'SB28_899', 'SB28_9', 'SB28_90', 'SB28_900', 'SB28_901', 'SB28_902', 'SB28_903', 'SB28_904', 'SB28_905', 'SB28_906', 'SB28_907', 'SB28_908', 'SB28_909', 'SB28_91', 'SB28_910', 'SB28_911', 'SB28_912', 'SB28_913', 'SB28_914', 'SB28_915', 'SB28_916', 'SB28_917', 'SB28_918', 'SB28_919', 'SB28_92', 'SB28_920', 'SB28_921', 'SB28_922', 'SB28_923', 'SB28_924', 'SB28_925', 'SB28_926', 'SB28_927', 'SB28_928', 'SB28_929', 'SB28_93', 'SB28_930', 'SB28_931', 'SB28_932', 'SB28_933', 'SB28_934', 'SB28_935', 'SB28_936', 'SB28_937', 'SB28_938', 'SB28_939', 'SB28_94', 'SB28_940', 'SB28_941', 'SB28_942', 'SB28_943', 'SB28_944', 'SB28_945', 'SB28_946', 'SB28_947', 'SB28_948', 'SB28_949', 'SB28_95', 'SB28_950', 'SB28_951', 'SB28_952', 'SB28_953', 'SB28_954', 'SB28_955', 'SB28_956', 'SB28_957', 'SB28_958', 'SB28_959', 'SB28_96', 'SB28_960', 'SB28_961', 'SB28_962', 'SB28_963', 'SB28_964', 'SB28_965', 'SB28_966', 'SB28_967', 'SB28_968', 'SB28_969', 'SB28_97', 'SB28_970', 'SB28_971', 'SB28_972', 'SB28_973', 'SB28_974', 'SB28_975', 'SB28_976', 'SB28_977', 'SB28_978', 'SB28_979', 'SB28_98', 'SB28_980', 'SB28_981', 'SB28_982', 'SB28_983', 'SB28_984', 'SB28_985', 'SB28_986', 'SB28_987', 'SB28_988', 'SB28_989', 'SB28_99', 'SB28_990', 'SB28_991', 'SB28_992', 'SB28_993', 'SB28_994', 'SB28_995', 'SB28_996', 'SB28_997', 'SB28_998', 'SB28_999']\n",
      "\n",
      "Structure for first simulation:\n",
      "['GALEX FUV', 'GALEX NUV']\n"
     ]
    }
   ],
   "source": [
    "# Quick check of available data\n",
    "with h5py.File(\"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/photometry/alice_galex.h5\", \"r\") as hf:\n",
    "    print(\"Available simulations:\", list(hf.keys()))\n",
    "    sim_key = list(hf.keys())[0]  # First simulation\n",
    "    print(\"\\nStructure for first simulation:\")\n",
    "    print(list(hf[f\"{sim_key}/snap_044/BC03/photometry/luminosity/attenuated\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          #Name    Omega0    sigma8  WindEnergyIn1e51erg  RadioFeedbackFactor  \\\n",
      "0        SB28_0  0.352541  0.694742              3.85743             1.519210   \n",
      "1        SB28_1  0.172430  0.830154              1.03554             0.797734   \n",
      "2        SB28_2  0.234683  0.705844              9.61416             3.380650   \n",
      "3        SB28_3  0.440288  0.969259              2.14363             0.488165   \n",
      "4        SB28_4  0.457152  0.786733              1.38466             0.325727   \n",
      "...         ...       ...       ...                  ...                  ...   \n",
      "2043  SB28_2043  0.457334  0.970226              8.89733             0.607197   \n",
      "2044  SB28_2044  0.440496  0.786137              5.34131             0.880840   \n",
      "2045  SB28_2045  0.234475  0.938760              1.49723             1.873430   \n",
      "2046  SB28_2046  0.172613  0.612888             13.31250             0.363806   \n",
      "2047  SB28_2047  0.352358  0.862216              3.09935             3.331570   \n",
      "\n",
      "      VariableWindVelFactor  RadioFeedbackReiorientationFactor  OmegaBaryon  \\\n",
      "0                   9.09267                            14.2845     0.049404   \n",
      "1                   6.95693                            38.2374     0.031199   \n",
      "2                   3.77681                            24.6592     0.042995   \n",
      "3                  11.49660                            10.9550     0.062295   \n",
      "4                  13.19410                            17.1439     0.044425   \n",
      "...                     ...                                ...          ...   \n",
      "2043                3.75653                            39.5139     0.046041   \n",
      "2044                5.04789                            21.6844     0.063161   \n",
      "2045               13.26680                            13.5869     0.042461   \n",
      "2046                8.64069                            17.7159     0.029257   \n",
      "2047                5.64656                            33.6239     0.051052   \n",
      "\n",
      "      HubbleParam       n_s  ...  WindEnergyReductionExponent  WindDumpFactor  \\\n",
      "0        0.498145  0.795765  ...                      1.61148        0.435971   \n",
      "1        0.683693  1.142160  ...                      2.08951        0.612056   \n",
      "2        0.849506  0.923545  ...                      1.33988        0.878906   \n",
      "3        0.638970  0.964648  ...                      2.86478        0.253435   \n",
      "4        0.773770  0.900256  ...                      2.59030        0.944457   \n",
      "...           ...       ...  ...                          ...             ...   \n",
      "2043     0.513293  0.828903  ...                      1.88784        0.725070   \n",
      "2044     0.748997  1.143290  ...                      1.66098        0.472330   \n",
      "2045     0.534548  0.796116  ...                      2.13803        0.698393   \n",
      "2046     0.600362  0.965127  ...                      1.38572        0.818457   \n",
      "2047     0.789823  0.924799  ...                      2.90964        0.242951   \n",
      "\n",
      "      SeedBlackHoleMass  BlackHoleAccretionFactor  BlackHoleEddingtonFactor  \\\n",
      "0              0.000069                  1.111740                  2.613460   \n",
      "1              0.000094                  0.889955                  0.302262   \n",
      "2              0.000153                  0.364219                  8.235920   \n",
      "3              0.000041                  2.775960                  0.950425   \n",
      "4              0.000049                  0.286789                  1.121900   \n",
      "...                 ...                       ...                       ...   \n",
      "2043           0.000054                  0.327481                  4.790910   \n",
      "2044           0.000038                  2.229260                  0.222157   \n",
      "2045           0.000174                  0.453541                  1.920250   \n",
      "2046           0.000090                  0.718570                  0.698528   \n",
      "2047           0.000070                  1.376900                  6.051620   \n",
      "\n",
      "      BlackHoleFeedbackFactor  BlackHoleRadiativeEfficiency  QuasarThreshold  \\\n",
      "0                    0.039463                      0.225386         0.000269   \n",
      "1                    0.151352                      0.086231         0.022802   \n",
      "2                    0.099772                      0.648096         0.001458   \n",
      "3                    0.349945                      0.126670         0.004242   \n",
      "4                    0.032352                      0.460414         0.007438   \n",
      "...                       ...                           ...              ...   \n",
      "2043                 0.033553                      0.481742         0.001252   \n",
      "2044                 0.397001                      0.109068         0.000416   \n",
      "2045                 0.087029                      0.682619         0.006263   \n",
      "2046                 0.158683                      0.083655         0.000071   \n",
      "2047                 0.038037                      0.256220         0.036493   \n",
      "\n",
      "      QuasarThresholdPower   seed  \n",
      "0                 0.514648  20000  \n",
      "1                 2.620780  20001  \n",
      "2                 3.389560  20002  \n",
      "3                 1.494400  20003  \n",
      "4                 2.299550  20004  \n",
      "...                    ...    ...  \n",
      "2043              2.191720  22043  \n",
      "2044              1.012360  22044  \n",
      "2045              3.117210  22045  \n",
      "2046              2.887940  22046  \n",
      "2047              0.994068  22047  \n",
      "\n",
      "[2048 rows x 30 columns]\n",
      "                            ParamName  AbsMaxDiff  LogFlag  FiducialVal  \\\n",
      "0                              Omega0        0.20        0      0.30000   \n",
      "1                              sigma8        0.20        0      0.80000   \n",
      "2                 WindEnergyIn1e51erg        4.00        1      3.60000   \n",
      "3                 RadioFeedbackFactor        4.00        1      1.00000   \n",
      "4               VariableWindVelFactor        2.00        1      7.40000   \n",
      "5   RadioFeedbackReiorientationFactor        2.00        1     20.00000   \n",
      "6                         OmegaBaryon        0.02        0      0.04900   \n",
      "7                         HubbleParam        0.20        0      0.67110   \n",
      "8                                 n_s        0.20        0      0.96240   \n",
      "9                     MaxSfrTimescale        2.00        1      2.27000   \n",
      "10                 FactorForSofterEQS        3.00        1      0.30000   \n",
      "11                           IMFslope        0.50        0     -2.30000   \n",
      "12                  SNII_MinMass_Msun        4.00        0      8.00000   \n",
      "13                ThermalWindFraction        4.00        1      0.10000   \n",
      "14           VariableWindSpecMomentum     2000.00        0      0.00000   \n",
      "15              WindFreeTravelDensFac       10.00        1      0.05000   \n",
      "16                         MinWindVel      200.00        0    350.00000   \n",
      "17          WindEnergyReductionFactor        4.00        1      0.25000   \n",
      "18     WindEnergyReductionMetallicity        4.00        1      0.00200   \n",
      "19        WindEnergyReductionExponent        1.00        0      2.00000   \n",
      "20                     WindDumpFactor        0.40        0      0.60000   \n",
      "21                  SeedBlackHoleMass        3.16        1      0.00008   \n",
      "22           BlackHoleAccretionFactor        4.00        1      1.00000   \n",
      "23           BlackHoleEddingtonFactor       10.00        1      1.00000   \n",
      "24            BlackHoleFeedbackFactor        4.00        1      0.10000   \n",
      "25       BlackHoleRadiativeEfficiency        4.00        1      0.20000   \n",
      "26                    QuasarThreshold       31.60        1      0.00200   \n",
      "27               QuasarThresholdPower        2.00        0      2.00000   \n",
      "\n",
      "        MinVal       MaxVal                                        Description  \n",
      "0     0.100000     0.500000                                        OmegaMatter  \n",
      "1     0.600000     1.000000                                             sigma8  \n",
      "2     0.900000    14.400000                       ASN1 - galactic winds energy  \n",
      "3     0.250000     4.000000                 AAGN1 - AGN FB kinetic mode energy  \n",
      "4     3.700000    14.800000                        ASN2 - galactic winds speed  \n",
      "5    10.000000    40.000000             AAGN2 - AGN FB kinetic mode burstiness  \n",
      "6     0.029000     0.069000                                        OmegaBaryon  \n",
      "7     0.471100     0.871100                                   Hubble parameter  \n",
      "8     0.762400     1.162400                      power spectrum spectral index  \n",
      "9     1.135000     4.540000                          gas consumption timescale  \n",
      "10    0.100000     0.900000                     softening factor for SH03 eEOS  \n",
      "11   -2.800000    -1.800000                              IMF slope above 1Msun  \n",
      "12    4.000000    12.000000                  minimum stellar mass that goes SN  \n",
      "13    0.025000     0.400000  fraction of wind energy injected thermally (ve...  \n",
      "14    0.000000  4000.000000              wind momentum per unit star-formation  \n",
      "15    0.005000     0.500000                         density of wind recoupling  \n",
      "16  150.000000   550.000000                                   wind speed floor  \n",
      "17    0.062500     1.000000  magnitude of wind energy dependence on metalli...  \n",
      "18    0.000500     0.008000            metallicity at which wind energy pivots  \n",
      "19    1.000000     3.000000  sharpness of wind energy dependence on metalli...  \n",
      "20    0.200000     1.000000                   (1 minus metal loading of winds)  \n",
      "21    0.000025     0.000253                                       seed BH mass  \n",
      "22    0.250000     4.000000                              Bondi rate multiplier  \n",
      "23    0.100000    10.000000  Eddington rate multiplier for the BH accretion...  \n",
      "24    0.025000     0.400000            high-accretion mode feedback efficiency  \n",
      "25    0.050000     0.800000                            BH radiative efficiency  \n",
      "26    0.000063     0.063200  Eddington ratio for transition between BH feed...  \n",
      "27    0.000000     4.000000  power-law in Weinberger+ 2017 eq.5 - steepness...  \n",
      "[[3.52541e-01 6.94742e-01 3.85743e+00 ... 2.25386e-01 2.69356e-04\n",
      "  5.14648e-01]\n",
      " [1.72430e-01 8.30154e-01 1.03554e+00 ... 8.62311e-02 2.28022e-02\n",
      "  2.62078e+00]\n",
      " [2.34683e-01 7.05844e-01 9.61416e+00 ... 6.48096e-01 1.45761e-03\n",
      "  3.38956e+00]\n",
      " ...\n",
      " [2.34475e-01 9.38760e-01 1.49723e+00 ... 6.82619e-01 6.26319e-03\n",
      "  3.11721e+00]\n",
      " [1.72613e-01 6.12888e-01 1.33125e+01 ... 8.36555e-02 7.09853e-05\n",
      "  2.88794e+00]\n",
      " [3.52358e-01 8.62216e-01 3.09935e+00 ... 2.56220e-01 3.64932e-02\n",
      "  9.94068e-01]]\n",
      "(2048, 28)\n",
      "Column names:\n",
      "['#Name', 'Omega0', 'sigma8', 'WindEnergyIn1e51erg', 'RadioFeedbackFactor', 'VariableWindVelFactor', 'RadioFeedbackReiorientationFactor', 'OmegaBaryon', 'HubbleParam', 'n_s', 'MaxSfrTimescale', 'FactorForSofterEQS', 'IMFslope', 'SNII_MinMass_Msun', 'ThermalWindFraction', 'VariableWindSpecMomentum', 'WindFreeTravelDensFac', 'MinWindVel', 'WindEnergyReductionFactor', 'WindEnergyReductionMetallicity', 'WindEnergyReductionExponent', 'WindDumpFactor', 'SeedBlackHoleMass', 'BlackHoleAccretionFactor', 'BlackHoleEddingtonFactor', 'BlackHoleFeedbackFactor', 'BlackHoleRadiativeEfficiency', 'QuasarThreshold', 'QuasarThresholdPower', 'seed']\n",
      "torch.Size([2048, 28]) (2048, 24)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Omega0:\n",
      "processing Omega0 is linear with min 0.1 and max 0.5:\n",
      "processing sigma8:\n",
      "processing sigma8 is linear with min 0.6 and max 1.0:\n",
      "processing WindEnergyIn1e51erg:\n",
      "processing WindEnergyIn1e51erg is logarithmic with min 0.9 and max 14.4:\n",
      "processing RadioFeedbackFactor:\n",
      "processing RadioFeedbackFactor is logarithmic with min 0.25 and max 4.0:\n",
      "processing VariableWindVelFactor:\n",
      "processing VariableWindVelFactor is logarithmic with min 3.7 and max 14.8:\n",
      "processing RadioFeedbackReiorientationFactor:\n",
      "processing RadioFeedbackReiorientationFactor is logarithmic with min 10.0 and max 40.0:\n",
      "processing OmegaBaryon:\n",
      "processing OmegaBaryon is linear with min 0.029 and max 0.069:\n",
      "processing HubbleParam:\n",
      "processing HubbleParam is linear with min 0.4711 and max 0.8711:\n",
      "processing n_s:\n",
      "processing n_s is linear with min 0.7624 and max 1.1624:\n",
      "processing MaxSfrTimescale:\n",
      "processing MaxSfrTimescale is logarithmic with min 1.135 and max 4.54:\n",
      "processing FactorForSofterEQS:\n",
      "processing FactorForSofterEQS is logarithmic with min 0.1 and max 0.9:\n",
      "processing IMFslope:\n",
      "processing IMFslope is linear with min -2.8 and max -1.8:\n",
      "processing SNII_MinMass_Msun:\n",
      "processing SNII_MinMass_Msun is linear with min 4.0 and max 12.0:\n",
      "processing ThermalWindFraction:\n",
      "processing ThermalWindFraction is logarithmic with min 0.025 and max 0.4:\n",
      "processing VariableWindSpecMomentum:\n",
      "processing VariableWindSpecMomentum is linear with min 0.0 and max 4000.0:\n",
      "processing WindFreeTravelDensFac:\n",
      "processing WindFreeTravelDensFac is logarithmic with min 0.005 and max 0.5:\n",
      "processing MinWindVel:\n",
      "processing MinWindVel is linear with min 150.0 and max 550.0:\n",
      "processing WindEnergyReductionFactor:\n",
      "processing WindEnergyReductionFactor is logarithmic with min 0.0625 and max 1.0:\n",
      "processing WindEnergyReductionMetallicity:\n",
      "processing WindEnergyReductionMetallicity is logarithmic with min 0.0005 and max 0.008:\n",
      "processing WindEnergyReductionExponent:\n",
      "processing WindEnergyReductionExponent is linear with min 1.0 and max 3.0:\n",
      "processing WindDumpFactor:\n",
      "processing WindDumpFactor is linear with min 0.2 and max 1.0:\n",
      "processing SeedBlackHoleMass:\n",
      "processing SeedBlackHoleMass is logarithmic with min 2.5316e-05 and max 0.0002528:\n",
      "processing BlackHoleAccretionFactor:\n",
      "processing BlackHoleAccretionFactor is logarithmic with min 0.25 and max 4.0:\n",
      "processing BlackHoleEddingtonFactor:\n",
      "processing BlackHoleEddingtonFactor is logarithmic with min 0.1 and max 10.0:\n",
      "processing BlackHoleFeedbackFactor:\n",
      "processing BlackHoleFeedbackFactor is logarithmic with min 0.025 and max 0.4:\n",
      "processing BlackHoleRadiativeEfficiency:\n",
      "processing BlackHoleRadiativeEfficiency is logarithmic with min 0.05 and max 0.8:\n",
      "processing QuasarThreshold:\n",
      "processing QuasarThreshold is logarithmic with min 6.33e-05 and max 0.0632:\n",
      "processing QuasarThresholdPower:\n",
      "processing QuasarThresholdPower is linear with min 0.0 and max 4.0:\n",
      "Theta shape: torch.Size([2048, 28])\n",
      "X shape: torch.Size([2048, 24])\n",
      "x_all: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/xray15/aem2/envs/camels/lib/python3.8/site-packages/sbi/utils/user_input_checks.py:76: UserWarning: Prior was provided as a sequence of 28 priors. They will be\n",
      "            interpreted as independent of each other and matched in order to the\n",
      "            components of the parameter.\n",
      "  warnings.warn(\n",
      "/disk/xray15/aem2/envs/camels/lib/python3.8/site-packages/sbi/utils/torchutils.py:27: UserWarning: GPU was selected as a device for training the neural network. Note that we expect no significant speed ups in training for the default architectures we provide. Using the GPU will be effective only for large neural networks with operations that are fast on the GPU, e.g., for a CNN or RNN `embedding_net`.\n",
      "  warnings.warn(\n",
      "/disk/xray15/aem2/envs/camels/lib/python3.8/site-packages/sbi/utils/user_input_checks.py:209: UserWarning: Casting 1D Uniform prior to BoxUniform to match sbi batch requirements.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.9912, -5.9912, -5.9912,  ..., -2.9198, -2.9514, -2.9944],\n",
      "        [-5.9673, -5.9673, -5.9673,  ..., -2.5617, -2.4853, -2.3535],\n",
      "        [-5.9923, -5.9923, -4.6757,  ..., -2.6586, -2.7463, -2.7463],\n",
      "        ...,\n",
      "        [-5.9837, -5.9837, -5.9837,  ..., -2.6188, -2.6934, -2.6303],\n",
      "        [-5.9594, -5.9594, -5.9594,  ..., -3.5295, -3.5617, -3.5617],\n",
      "        [-5.9929, -5.9929, -5.9929,  ..., -2.3514, -2.3834, -2.4853]],\n",
      "       device='cuda:0')\n",
      "theta: tensor([[3.5254e-01, 6.9474e-01, 3.8574e+00,  ..., 2.2539e-01, 2.6936e-04,\n",
      "         5.1465e-01],\n",
      "        [1.7243e-01, 8.3015e-01, 1.0355e+00,  ..., 8.6231e-02, 2.2802e-02,\n",
      "         2.6208e+00],\n",
      "        [2.3468e-01, 7.0584e-01, 9.6142e+00,  ..., 6.4810e-01, 1.4576e-03,\n",
      "         3.3896e+00],\n",
      "        ...,\n",
      "        [2.3448e-01, 9.3876e-01, 1.4972e+00,  ..., 6.8262e-01, 6.2632e-03,\n",
      "         3.1172e+00],\n",
      "        [1.7261e-01, 6.1289e-01, 1.3312e+01,  ..., 8.3655e-02, 7.0985e-05,\n",
      "         2.8879e+00],\n",
      "        [3.5236e-01, 8.6222e-01, 3.0993e+00,  ..., 2.5622e-01, 3.6493e-02,\n",
      "         9.9407e-01]], device='cuda:0')\n",
      "x_all_cpu: [[-5.991244  -5.991244  -5.991244  ... -2.9197948 -2.9513938 -2.9944284]\n",
      " [-5.967319  -5.967319  -5.967319  ... -2.5617263 -2.485338  -2.3534505]\n",
      " [-5.992274  -5.992274  -4.6756697 ... -2.6586363 -2.7462509 -2.7462509]\n",
      " ...\n",
      " [-5.983681  -5.983681  -5.983681  ... -2.6187649 -2.6933985 -2.6303468]\n",
      " [-5.959369  -5.959369  -5.959369  ... -3.5295417 -3.5617263 -3.5617263]\n",
      " [-5.9929414 -5.9929414 -5.9929414 ... -2.3513873 -2.3834138 -2.485338 ]]\n",
      "Data shape before processing: (2048, 24)\n",
      "Number of values: -192639.19\n",
      "Number of NaN values: 0\n",
      "Number of infinite values: 0\n",
      "nan_mask: [[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n",
      "Data shape before processing: (2048, 24)\n",
      "Number of NaN values: 0\n",
      "Number of infinite values: 0\n",
      "x_all_cpu: [[-5.991244  -5.991244  -5.991244  ... -2.9197948 -2.9513938 -2.9944284]\n",
      " [-5.967319  -5.967319  -5.967319  ... -2.5617263 -2.485338  -2.3534505]\n",
      " [-5.992274  -5.992274  -4.6756697 ... -2.6586363 -2.7462509 -2.7462509]\n",
      " ...\n",
      " [-5.983681  -5.983681  -5.983681  ... -2.6187649 -2.6933985 -2.6303468]\n",
      " [-5.959369  -5.959369  -5.959369  ... -3.5295417 -3.5617263 -3.5617263]\n",
      " [-5.9929414 -5.9929414 -5.9929414 ... -2.3513873 -2.3834138 -2.485338 ]]\n",
      "x_all: tensor([[-0.2761, -0.2761, -0.2761,  ..., -0.1346, -0.1360, -0.1380],\n",
      "        [-0.2791, -0.2791, -0.2791,  ..., -0.1198, -0.1162, -0.1101],\n",
      "        [-0.3052, -0.3052, -0.2381,  ..., -0.1354, -0.1399, -0.1399],\n",
      "        ...,\n",
      "        [-0.2908, -0.2908, -0.2908,  ..., -0.1273, -0.1309, -0.1278],\n",
      "        [-0.2427, -0.2427, -0.2427,  ..., -0.1437, -0.1450, -0.1450],\n",
      "        [-0.2807, -0.2807, -0.2807,  ..., -0.1102, -0.1117, -0.1164]],\n",
      "       device='cuda:0')\n",
      "Any NaN in normalized data: False\n",
      "Any inf in normalized data: False\n",
      "Total simulations: 2048\n",
      "Test set size: 203\n",
      "Training set size: 1845\n",
      "Test fraction: 0.099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3018464/2970234998.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  theta = torch.tensor(theta, dtype=torch.float32, device=device)\n"
     ]
    }
   ],
   "source": [
    "# parameter info file (df_info) is used for defining priors\n",
    "# the actual parameter values come from the camels class which reads CosmoAstroSeed_IllustrisTNG_L25n256_SB28.txt\n",
    "\n",
    "#  parameters defined here: /disk/xray15/aem2/data/28pams/IllustrisTNG/SB/CosmoAstroSeed_IllustrisTNG_L25n256_SB28.txt which is used for theta\n",
    "df_pars = pd.read_csv('/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/CosmoAstroSeed_IllustrisTNG_L25n256_SB28.txt', delim_whitespace=True)\n",
    "print(df_pars)\n",
    "\n",
    "\n",
    "# prior values come from this:\n",
    "df_info = pd.read_csv(\"/disk/xray15/aem2/data/28pams/Info_IllustrisTNG_L25n256_28params.txt\")\n",
    "print(df_info)\n",
    "\n",
    "theta = df_pars.iloc[:, 1:29].to_numpy()  # excluding 'name' column and 'seed' column\n",
    "\n",
    "print(theta)\n",
    "print(theta.shape)\n",
    "print(\"Column names:\")\n",
    "print(df_pars.columns.tolist())\n",
    "# plot the first one (omega0) to see shape of prior:\n",
    "plt.hist(theta[:, 24])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    theta, x = get_theta_x_SB(\n",
    "        luminosity_functions=luminosity_functions,\n",
    "        colours=colours,\n",
    "        uvlf_limits=uvlf_limits,\n",
    "        colour_limits=colour_limits,\n",
    "        n_bins_lf=n_bins_lf,\n",
    "        n_bins_colour=n_bins_colour\n",
    "    )\n",
    "    print(theta.shape, x.shape)\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     theta, x = get_theta_x_SB(\n",
    "#         luminosity_functions=luminosity_functions,\n",
    "#         colours=colours  # This will now override the default\n",
    "#     )\n",
    "#     print(theta.shape, x.shape)\n",
    "    \n",
    "if colours:\n",
    "    fig = plot_colour(x)\n",
    "    plt.savefig('/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/colours_test/colour_check.png')\n",
    "    plt.show()\n",
    "\n",
    "if luminosity_functions:\n",
    "    fig = plot_uvlf(x)\n",
    "    plt.savefig('/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/LFs_test/uvlf_check.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get the priors and data\n",
    "prior = initialise_priors_SB28(\n",
    "    df=df_info, \n",
    "    device=device,\n",
    "    astro=True,\n",
    "    dust=False  # no dust for testing. set to False to only get the 28 model parameters.\n",
    "    # with dust = True, prior has 32 dimensions (28 parameters + 4 dust parameters) \n",
    ")\n",
    "\n",
    "# process the data\n",
    "x_all = np.array([np.hstack(_x) for _x in x])\n",
    "x_all = torch.tensor(x_all, dtype=torch.float32, device=device)\n",
    "\n",
    "print(\"Theta shape:\", theta.shape)\n",
    "print(\"X shape:\", x_all.shape)\n",
    "\n",
    "\n",
    "# Move data to GPU as early as possible\n",
    "x_all = x_all.to(device)\n",
    "print('x_all:', x_all)\n",
    "\n",
    "theta = torch.tensor(theta, dtype=torch.float32, device=device)\n",
    "print('theta:', theta)\n",
    "\n",
    "# Handle NaN values and normalize while on GPU\n",
    "x_all_cpu = x_all.cpu().numpy()  # Only move to CPU when necessary for sklearn\n",
    "print('x_all_cpu:', x_all_cpu)\n",
    "\n",
    "print(\"Data shape before processing:\", x_all_cpu.shape)\n",
    "print(\"Number of values:\",(x_all_cpu).sum())\n",
    "print(\"Number of NaN values:\", np.isnan(x_all_cpu).sum())\n",
    "print(\"Number of infinite values:\", np.isinf(x_all_cpu).sum())\n",
    "\n",
    "\n",
    "# get rid of NaN/inf values, replace with small random noise\n",
    "nan_mask = np.isnan(x_all_cpu) | np.isinf(x_all_cpu)\n",
    "print('nan_mask:', nan_mask)\n",
    "\n",
    "\n",
    "if nan_mask.any():\n",
    "    x_all_cpu[nan_mask] = np.random.rand(np.sum(nan_mask)) * 1e-10\n",
    "\n",
    "print(\"Data shape before processing:\", x_all_cpu.shape)\n",
    "print(\"Number of NaN values:\", np.isnan(x_all_cpu).sum())\n",
    "print(\"Number of infinite values:\", np.isinf(x_all_cpu).sum())\n",
    "\n",
    "print('x_all_cpu:', x_all_cpu)\n",
    "\n",
    "\n",
    "# Normalize\n",
    "norm = Normalizer()\n",
    "\n",
    "# Option: Add small constant before normalizing\n",
    "epsilon = 1e-10  # Small constant\n",
    "x_all_shifted = x_all_cpu + epsilon\n",
    "x_all_normalized = norm.fit_transform(x_all_shifted)\n",
    "x_all = torch.tensor(x_all_normalized, dtype=torch.float32, device=device)\n",
    "\n",
    "print('x_all:', x_all)\n",
    "\n",
    "# Save normalizer\n",
    "joblib.dump(norm, f'/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/{name}_scaler.save')\n",
    "\n",
    "# Print final check\n",
    "print(\"Any NaN in normalized data:\", torch.isnan(x_all).any().item())\n",
    "print(\"Any inf in normalized data:\", torch.isinf(x_all).any().item())\n",
    "\n",
    "\n",
    "# make test mask\n",
    "test_mask = create_test_mask() # 10% testing\n",
    "train_mask = ~test_mask # 90% for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training arguments\n",
    "train_args = {\n",
    "    \"training_batch_size\": 64, # changed from 4 to 10 as dealing with more sims, want it to be faster for initial testing.\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"stop_after_epochs\": 600, # loss function. waits to see if things improve.\n",
    "    \"validation_fraction\": 0.15,  # creates another split within the training data for validation\n",
    "    \"max_num_epochs\": 2000  # Add maximum epochs\n",
    "}\n",
    "\n",
    "# Configure network\n",
    "hidden_features = 256 #neurons\n",
    "num_transforms = 8 #layers\n",
    "num_nets = 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balanced 'middle' network:\n",
    "\n",
    "# run 1\n",
    "train_args = {\n",
    "   \"training_batch_size\": 16,\n",
    "   \"learning_rate\": 5e-5,   \n",
    "   \"stop_after_epochs\": 100, \n",
    "   \"max_num_epochs\": 1000,\n",
    "   \"validation_fraction\": 0.1,  \n",
    "   \"clip_max_norm\": 5.0        # More aggressive gradient clipping\n",
    "}\n",
    "\n",
    "hidden_features = 128   # Thin network\n",
    "num_transforms = 8    # Deep network\n",
    "num_nets = 1 \n",
    "\n",
    "\n",
    "# run 2\"\n",
    "train_args = {\n",
    "  \"training_batch_size\": 8,\n",
    "  \"learning_rate\": 1e-3,  \n",
    "  \"stop_after_epochs\": 50,\n",
    "  \"max_num_epochs\": 1000,\n",
    "  \"validation_fraction\": 0.1, \n",
    "  \"clip_max_norm\": 5.0        # More aggressive gradient clipping\n",
    "}\n",
    "\n",
    "\n",
    "hidden_features = 128   # Thin network\n",
    "num_transforms = 8    # Deep network\n",
    "num_nets = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIDE but shallow network:\n",
    "train_args = {\n",
    "    \"training_batch_size\": 16,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"stop_after_epochs\": 100, # loss function. waits to see if things improve.\n",
    "    \"max_num_epochs\": 1000,\n",
    "    \"validation_fraction\": 0.1,  # creates another split within the training data for validation\n",
    "    \"clip_max_norm\": 5.0        # More aggressive gradient clipping\n",
    "}\n",
    "\n",
    "hidden_features = 512  # Wide network\n",
    "num_transforms = 3    # Shallow depth\n",
    "num_nets = 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thin but Deep Network:\n",
    "train_args = {\n",
    "    \"training_batch_size\": 16,\n",
    "    \"learning_rate\": 1e-5,   # Slower learning rate for complex architecture\n",
    "    \"stop_after_epochs\": 100, # loss function. waits to see if things improve.\n",
    "    \"max_num_epochs\": 1000,\n",
    "    \"validation_fraction\": 0.1,  # creates another split within the training data for validation\n",
    "    \"clip_max_norm\": 5.0        # More aggressive gradient clipping\n",
    "}\n",
    "\n",
    "\n",
    "hidden_features = 32   # Thin network\n",
    "num_transforms = 15    # Deep network\n",
    "\n",
    "nets = [ili.utils.load_nde_sbi(\n",
    "    engine=\"NPE\",\n",
    "    model=\"nsf\",\n",
    "    hidden_features=hidden_features,\n",
    "    num_transforms=num_transforms\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced middle model:\n",
    "hidden_features = 128  # Medium width\n",
    "num_transforms = 8     # Medium depth\n",
    "learning_rate = 5e-5   # Moderate learning rate\n",
    "\n",
    "nets = [ili.utils.load_nde_sbi(\n",
    "    engine=\"NPE\",\n",
    "    model=\"nsf\",\n",
    "    hidden_features=hidden_features,\n",
    "    num_transforms=num_transforms\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:MODEL INFERENCE CLASS: NPE\n",
      "/disk/xray15/aem2/envs/camels/lib/python3.8/site-packages/sbi/utils/torchutils.py:27: UserWarning: GPU was selected as a device for training the neural network. Note that we expect no significant speed ups in training for the default architectures we provide. Using the GPU will be effective only for large neural networks with operations that are fast on the GPU, e.g., for a CNN or RNN `embedding_net`.\n",
      "  warnings.warn(\n",
      "INFO:root:Training model 1 / 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training neural network. Epochs trained: 2"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'validation_log_prob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 65\u001b[0m\n\u001b[1;32m     49\u001b[0m runner \u001b[38;5;241m=\u001b[39m AdaptiveScheduledSBIRunner(\n\u001b[1;32m     50\u001b[0m     engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     51\u001b[0m     prior\u001b[38;5;241m=\u001b[39mprior,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     name\u001b[38;5;241m=\u001b[39mname\n\u001b[1;32m     58\u001b[0m )\n\u001b[1;32m     60\u001b[0m loader \u001b[38;5;241m=\u001b[39m NumpyLoader(\n\u001b[1;32m     61\u001b[0m     x\u001b[38;5;241m=\u001b[39mx_all[train_mask]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach(),\n\u001b[1;32m     62\u001b[0m     theta\u001b[38;5;241m=\u001b[39mtheta[train_mask]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     63\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m posterior_ensemble, summaries \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/xray15/aem2/envs/camels/src/ltu-ili/ili/inference/runner_sbi.py:301\u001b[0m, in \u001b[0;36mSBIRunner.__call__\u001b[0;34m(self, loader, seed)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# train a single round of inference\u001b[39;00m\n\u001b[1;32m    300\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 301\u001b[0m posterior_ensemble, summaries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_round\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproposal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproposal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt0\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds to train models.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# save if output path is specified\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 67\u001b[0m, in \u001b[0;36mAdaptiveScheduledSBIRunner._train_round\u001b[0;34m(self, models, x, theta, proposal)\u001b[0m\n\u001b[1;32m     62\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain(max_num_epochs\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     63\u001b[0m               learning_rate\u001b[38;5;241m=\u001b[39mcurrent_lr,\n\u001b[1;32m     64\u001b[0m               resume_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Get current validation loss\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m current_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation_log_prob\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Check if we've improved\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation_log_prob'"
     ]
    }
   ],
   "source": [
    "\n",
    "train_args = {\n",
    "    \"training_batch_size\": 64,\n",
    "    \"learning_rate\": 3e-6,        # Initial learning rate\n",
    "    \"stop_after_epochs\": 100,     # Increased patience for convergence check\n",
    "    \"clip_max_norm\": 0.3,\n",
    "    \"validation_fraction\": 0.15,\n",
    "    \"max_num_epochs\": 4000,\n",
    "    \"show_train_summary\": True,\n",
    "    # New adaptive learning rate parameters\n",
    "    \"patience\": 30,               # Epochs to wait before reducing LR\n",
    "    \"min_lr\": 1e-7,              # Minimum learning rate\n",
    "    \"lr_reduction_factor\": 0.5    # Factor to reduce LR by when plateauing\n",
    "}\n",
    "\n",
    "hidden_features = 64              # Keep this\n",
    "num_transforms = 4                # Keep this\n",
    "num_nets = 2                      # Consider reducing to 2 for better generalization\n",
    "\n",
    "# Simple base configuration\n",
    "nets = [ili.utils.load_nde_sbi( # uses sbi.utils.posterior_nn\n",
    "    engine=\"NPE\",\n",
    "    model=\"nsf\",\n",
    "    hidden_features=hidden_features,\n",
    "    num_transforms=num_transforms,\n",
    ") for _ in range(num_nets)]\n",
    "\n",
    "# # Create the runner with the scheduled learning rate\n",
    "# runner = ScheduledSBIRunner(\n",
    "#     backend=\"sbi\",\n",
    "#     engine=\"NPE\",\n",
    "#     prior=prior,\n",
    "#     nets=nets,\n",
    "#     device=device,\n",
    "#     train_args=train_args,\n",
    "#     proposal=None,\n",
    "#     out_dir=model_out_dir,\n",
    "#     name=name\n",
    "# )\n",
    "\n",
    "# loader = NumpyLoader(\n",
    "\n",
    "#     x=x_all[train_mask].clone().detach(),\n",
    "#     theta=theta[train_mask].clone().detach()\n",
    "# )\n",
    "# posterior_ensemble, summaries = runner(loader=loader)\n",
    "\n",
    "\n",
    "# Then create the runner without the 'backend' parameter\n",
    "runner = AdaptiveScheduledSBIRunner(\n",
    "    engine=\"NPE\",\n",
    "    prior=prior,\n",
    "    nets=nets,\n",
    "    device=device,\n",
    "    train_args=train_args,\n",
    "    proposal=None,\n",
    "    out_dir=model_out_dir,\n",
    "    name=name\n",
    ")\n",
    "\n",
    "loader = NumpyLoader(\n",
    "    x=x_all[train_mask].clone().detach(),\n",
    "    theta=theta[train_mask].clone().detach()\n",
    ")\n",
    "\n",
    "posterior_ensemble, summaries = runner(loader=loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define parameter grid\n",
    "# param_grid = {\n",
    "#     \"hidden_features\": [20, 40, 80],\n",
    "#     \"num_transforms\": [1, 2, 4, 8],\n",
    "#     \"training_batch_size\": [16, 32, 64],\n",
    "#     \"learning_rate\": [1e-4, 5e-5, 1e-5],\n",
    "#     \"num_nets\": [1, 2, 3]\n",
    "# }\n",
    "\n",
    "# # Create output directory with timestamp\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# results_dir = Path(f\"grid_search_results_{timestamp}\")\n",
    "# results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# # Store all results\n",
    "# all_results = []\n",
    "\n",
    "# # Generate all combinations of parameters\n",
    "# param_combinations = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "# def create_run_name(params):\n",
    "#     \"\"\"Create a descriptive run name from parameters\"\"\"\n",
    "#     return f\"hf{params['hidden_features']}_nt{params['num_transforms']}_bs{params['training_batch_size']}_lr{params['learning_rate']:.0e}_nets{params['num_nets']}\"\n",
    "\n",
    "# for i, params in enumerate(param_combinations):\n",
    "\n",
    "#     run_name = create_run_name(params)\n",
    "\n",
    "#     # Update training arguments\n",
    "#     train_args = {\n",
    "#         \"training_batch_size\": params[\"training_batch_size\"],\n",
    "#         \"learning_rate\": params[\"learning_rate\"],\n",
    "#         \"stop_after_epochs\": 100,\n",
    "#         \"validation_fraction\": 0.2,\n",
    "#         \"max_num_epochs\": 1000,\n",
    "#         \"clip_max_norm\": 5.0\n",
    "#     }\n",
    "    \n",
    "#     # Update model parameters\n",
    "#     hidden_features = params[\"hidden_features\"]\n",
    "#     num_transforms = params[\"num_transforms\"]\n",
    "#     num_nets = params[\"num_nets\"]\n",
    "    \n",
    "#     # Create networks\n",
    "#     nets = [ili.utils.load_nde_sbi(\n",
    "#         engine=\"NPE\",\n",
    "#         model=\"nsf\",\n",
    "#         hidden_features=hidden_features,\n",
    "#         num_transforms=num_transforms,\n",
    "#     ) for _ in range(num_nets)]\n",
    "    \n",
    "#     # Setup runner\n",
    "#     run_dir = results_dir / run_name\n",
    "#     run_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "#     runner = InferenceRunner.load(\n",
    "#         backend=\"sbi\",\n",
    "#         engine=\"NPE\",\n",
    "#         prior=prior,\n",
    "#         nets=nets,\n",
    "#         device=device,\n",
    "#         train_args=train_args,\n",
    "#         proposal=None,\n",
    "#         out_dir=run_dir,\n",
    "#         name=run_name\n",
    "#     )\n",
    "    \n",
    "#     # Data loader\n",
    "#     loader = NumpyLoader(\n",
    "#         x=x_all[train_mask].clone().detach(),\n",
    "#         theta=theta[train_mask].clone().detach()\n",
    "#     )\n",
    "    \n",
    "#     # Train model\n",
    "#     posterior_ensemble, summaries = runner(loader=loader)\n",
    "    \n",
    "#     # Store results\n",
    "#     result = {\n",
    "#         \"parameters\": params,\n",
    "#         \"summaries\": summaries,\n",
    "#         \"posterior_ensemble\": posterior_ensemble,\n",
    "#         \"train_args\": train_args\n",
    "#     }\n",
    "    \n",
    "#     all_results.append(result)\n",
    "    \n",
    "#     # Save individual run results\n",
    "#     with open(run_dir / \"results.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(result, f)\n",
    "    \n",
    "#     # Print progress\n",
    "#     print(f\"Completed run {i+1}/{len(param_combinations)}\")\n",
    "#     # print(f\"Parameters: {params}\")\n",
    "#     # print(f\"Validation loss: {summaries.get('validation_loss', 'N/A')}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "# # Save all results to a single file\n",
    "# with open(results_dir / \"all_results.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(all_results, f)\n",
    "\n",
    "# # Create a summary file\n",
    "# # with open(results_dir / \"summary.txt\", \"w\") as f:\n",
    "# #     f.write(\"Grid Search Results Summary\\n\")\n",
    "#     # f.write(f\"Timestamp: {timestamp}\\n\\n\")\n",
    "    \n",
    "#     # for i, result in enumerate(all_results):\n",
    "#         # f.write(f\"Run {i+1}:\\n\")\n",
    "#         # f.write(f\"Parameters: {result['parameters']}\\n\")\n",
    "#         # f.write(f\"Validation loss: {result['summaries'].get('validation_loss', 'N/A')}\\n\")\n",
    "#         # f.write(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hf20_nt2_bs16_lr1e-04_nets1.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_all[train_mask].clone().detach(),\n",
    "theta_train=theta[train_mask].clone().detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training_batch_size\", train_args[ \"training_batch_size\"])\n",
    "print(\"learning_rate\", train_args[ \"learning_rate\"])\n",
    "print(\"stop_after_epochs\", train_args[ \"stop_after_epochs\"])\n",
    "print(\"validation_fraction\", train_args[ \"validation_fraction\"])\n",
    "print(\"hidden_features\", hidden_features)\n",
    "print(\"num_transforms\", num_transforms)\n",
    "print(\"num_nets\", num_nets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_diagnostics(summaries):\n",
    "    \"\"\"Plot training diagnostics without empty subplots\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))  # Changed to 1 row, 2 columns\n",
    "    \n",
    "    # Loss curves\n",
    "    train_losses = summaries[0]['training_log_probs']\n",
    "    val_losses = summaries[0]['validation_log_probs']\n",
    "    epochs = range(len(train_losses))\n",
    "    \n",
    "    ax1.plot(epochs, train_losses, '-', label='Training', color='blue')\n",
    "    ax1.plot(epochs, val_losses, '--', label='Validation', color='red')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Log probability')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overfitting gap\n",
    "    gap = np.array(train_losses) - np.array(val_losses)\n",
    "    ax2.plot(epochs, gap, '-', color='purple')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss difference')\n",
    "    ax2.set_title('Overfitting Gap')\n",
    "    ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# refer back to training args in file name\n",
    "model_params = f\"batch{train_args['training_batch_size']}_\" \\\n",
    "               f\"lr{train_args['learning_rate']:.0e}_\" \\\n",
    "               f\"epochs{train_args['stop_after_epochs']}_\" \\\n",
    "               f\"val{train_args['validation_fraction']}_\" \\\n",
    "               f\"hidden{hidden_features}_\" \\\n",
    "               f\"transforms{num_transforms}\"\n",
    "\n",
    "# Use the function\n",
    "fig = plot_training_diagnostics(summaries)\n",
    "plt.savefig(os.path.join(plots_out_dir, f'loss_overfitting_{model_params}.png'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train/validation loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,4))\n",
    "c = list(mcolors.TABLEAU_COLORS)\n",
    "for i, m in enumerate(summaries):\n",
    "    ax.plot(m['training_log_probs'], ls='-', label=f\"{i}_train\", c=c[i])\n",
    "    ax.plot(m['validation_log_probs'], ls='--', label=f\"{i}_val\", c=c[i])\n",
    "ax.set_xlim(0)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Log probability')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will this work or do we have to use it explicitly?\n",
    "x_train=x_all[train_mask].clone().detach(),\n",
    "theta_train=theta[train_mask].clone().detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing samples from the entire test set to look at overall performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get test data\n",
    "# x_test = x_all[test_mask]\n",
    "# theta_test = theta[test_mask]\n",
    "\n",
    "# # Number of samples to draw from posterior\n",
    "# n_samples = 1000\n",
    "\n",
    "# # Storage for predictions\n",
    "# all_samples = []\n",
    "# all_means = []\n",
    "# all_stds = []\n",
    "\n",
    "# # Generate posterior samples for each test point\n",
    "# for i in range(len(x_test)):\n",
    "#     # Get samples from the posterior\n",
    "#     samples = posterior_ensemble.sample(\n",
    "#         (n_samples,), \n",
    "#         x=x_test[i].reshape(1, -1)\n",
    "#     ).cpu().numpy()\n",
    "    \n",
    "#     # Calculate mean and std of samples\n",
    "#     mean = samples.mean(axis=0)\n",
    "#     std = samples.std(axis=0)\n",
    "    \n",
    "#     all_samples.append(samples)\n",
    "#     all_means.append(mean)\n",
    "#     all_stds.append(std)\n",
    "\n",
    "# all_samples = np.array(all_samples)\n",
    "# all_means = np.array(all_means)\n",
    "# all_stds = np.array(all_stds)\n",
    "\n",
    "# Get test data\n",
    "x_test = x_all[test_mask]\n",
    "theta_test = theta[test_mask]\n",
    "\n",
    "# Number of samples to draw from posterior\n",
    "n_samples = 1000\n",
    "\n",
    "# Storage for predictions\n",
    "all_samples = []\n",
    "all_means = []\n",
    "all_stds = []\n",
    "\n",
    "# Suppress the deprecation warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='nflows.transforms.lu')\n",
    "\n",
    "# Generate posterior samples for each test point\n",
    "for i in range(len(x_test)):\n",
    "    # Get samples from the posterior\n",
    "    with torch.no_grad():  # Add this for efficiency\n",
    "        samples = posterior_ensemble.sample(\n",
    "            (n_samples,), \n",
    "            x=x_test[i].reshape(1, -1)\n",
    "        ).cpu().numpy()\n",
    "    \n",
    "    # Calculate mean and std of samples\n",
    "    mean = samples.mean(axis=0)\n",
    "    std = samples.std(axis=0)\n",
    "    \n",
    "    all_samples.append(samples)\n",
    "    all_means.append(mean)\n",
    "    all_stds.append(std)\n",
    "\n",
    "all_samples = np.array(all_samples)\n",
    "all_means = np.array(all_means)\n",
    "all_stds = np.array(all_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = df_pars.columns[1:29].tolist()  # Excluding 'name' column\n",
    "\n",
    "fig, axes = plt.subplots(7, 4, figsize=(16, 28)) \n",
    "axes = axes.flatten()\n",
    "\n",
    "fontsize = 10  \n",
    "\n",
    "plt.rcParams['figure.constrained_layout.use'] = True  \n",
    "\n",
    "# Plot each parameter\n",
    "for i in range(28):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # True vs predicted with error bars\n",
    "    ax.errorbar(\n",
    "        theta_test[:, i].cpu().numpy(),\n",
    "        all_means[:, i],\n",
    "        yerr=all_stds[:, i],\n",
    "        fmt='.',\n",
    "        color='k',\n",
    "        ecolor='blue',\n",
    "        capsize=0,\n",
    "        elinewidth=0.8,  \n",
    "        alpha=0.3,       \n",
    "        markersize=5    \n",
    "    )\n",
    "    \n",
    "    # Add true line\n",
    "    lims = [\n",
    "        min(ax.get_xlim()[0], ax.get_ylim()[0]),\n",
    "        max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "    ]\n",
    "    ax.plot(lims, lims, '--', color='black', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    # get metrics\n",
    "    rmse = np.sqrt(np.mean((theta_test[:, i].cpu().numpy() - all_means[:, i])**2))\n",
    "    r2 = np.corrcoef(theta_test[:, i].cpu().numpy(), all_means[:, i])[0, 1]**2\n",
    "    chi2 = np.mean(((theta_test[:, i].cpu().numpy() - all_means[:, i])**2) / (all_stds[:, i]**2))\n",
    "    \n",
    "    # add metrics box in top left corner\n",
    "    stats_text = f'RMSE = {rmse:.2f}\\n' + \\\n",
    "                 f'R² = {r2:.2f}\\n' + \\\n",
    "                 f'χ² = {chi2:.2f}'\n",
    "    ax.text(0.05, 0.95, stats_text,\n",
    "            transform=ax.transAxes,\n",
    "            bbox=dict(facecolor='white', alpha=0.8),\n",
    "            verticalalignment='top',\n",
    "            fontsize=fontsize-1)  # Slightly smaller font for stats\n",
    "    \n",
    "    # title: parameter name\n",
    "    ax.set_title(param_names[i], fontsize=fontsize, pad=5)  # Reduced padding\n",
    "    \n",
    "    # axis labels\n",
    "    ax.set_xlabel('True', fontsize=fontsize-1)\n",
    "    ax.set_ylabel('Inferred', fontsize=fontsize-1)\n",
    "    \n",
    "    # tick labels\n",
    "    ax.tick_params(axis='both', which='major', labelsize=fontsize-2)\n",
    "    \n",
    "    # internal padding\n",
    "    ax.margins(x=0.05, y=0.05)\n",
    "\n",
    "# subplot spacing\n",
    "plt.subplots_adjust(\n",
    "    left=0.01,    # Less space on left\n",
    "    right=0.7,   # Less space on right\n",
    "    bottom=0.05,  # Less space at bottom\n",
    "    top=0.7,     # Less space at top\n",
    "    wspace=0.2,   # Less space between plots horizontally\n",
    "    hspace=0.2    # Less space between plots vertically\n",
    ")\n",
    "\n",
    "\n",
    "# Save figure with detailed filename\n",
    "save_path = f'{plots_out_dir}/posterior_predictions_{model_params}.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "print(save_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_str = (f\"batch{train_args['training_batch_size']}_\"\n",
    "             f\"lr{train_args['learning_rate']}_\"\n",
    "             f\"epochs{train_args['stop_after_epochs']}_\"\n",
    "             f\"h{hidden_features}_t{num_transforms}\")\n",
    "\n",
    "# coverage plots\n",
    "metric = PosteriorCoverage(\n",
    "    num_samples=int(4e3),\n",
    "    sample_method='direct',\n",
    "    labels=cam.labels,\n",
    "    plot_list=[\"tarp\"], # \"coverage\", \"histogram\", \"predictions\", \n",
    "    out_dir=plots_out_dir,\n",
    ")\n",
    "\n",
    "# Generate plots\n",
    "figs = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x=x_all[test_mask].cpu(),\n",
    "    theta=theta[test_mask, :].cpu(),\n",
    "    signature=f\"coverage_{name}_{config_str}_\"  # Add config to filename\n",
    ")\n",
    "\n",
    "config_text = (\n",
    "    f\"Training Config:\\n\"\n",
    "    f\"Batch Size: {train_args['training_batch_size']}\\n\"\n",
    "    f\"Learning Rate: {train_args['learning_rate']}\\n\"\n",
    "    f\"Epochs: {train_args['stop_after_epochs']}\\n\"\n",
    "    f\"Hidden Features: {hidden_features}\\n\"\n",
    "    f\"Num Transforms: {num_transforms}\"\n",
    ")\n",
    "\n",
    "# Process each figure\n",
    "for i, fig in enumerate(figs):\n",
    "    plt.figure(fig.number)  # Activate the figure\n",
    "    plt.figtext(0.02, 0.98, config_text,\n",
    "                fontsize=8,\n",
    "                bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),\n",
    "                verticalalignment='top')\n",
    "    \n",
    "    # Save each figure with type indicator\n",
    "    plot_types = [\"tarp\"] #\"coverage\", \"histogram\", \"predictions\",\n",
    "    plt.savefig(os.path.join(plots_out_dir, \n",
    "                f'metric_{plot_types[i]}_{name}_{config_str}.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at a specific case (one observation randomly set with seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now, SBIRunner returns a custom class instance to be able to pass signature strings\n",
    "# # 1. prints our info on model configuration and architecture\n",
    "# print(posterior_ensemble.signatures)\n",
    "\n",
    "\n",
    "# # 2. choose a random input for training\n",
    "# seed_in = 49\n",
    "# np.random.seed(seed_in) # set seed for reproducability\n",
    "# ind = np.random.randint(len(x_train[0])) # choose observation (random index from training data)\n",
    "\n",
    "# # 3. generate posterior samples\n",
    "# seed_samp = 32\n",
    "# torch.manual_seed(seed_samp)# set seed for reproducability\n",
    "# # then, for the chosen training sample (as chosen above in 2.)\n",
    "# # generate 1000 samples from the posterior distribution using accept/reject sampling\n",
    "# samples = posterior_ensemble.sample(\n",
    "#     (1000,), \n",
    "#     torch.Tensor(x_train[0][ind]).to(device))\n",
    "\n",
    "# # 4. calculate the probability densities for each sample\n",
    "# # i.e for each generated sample, calculate how likely it is using learned posterior distribution\n",
    "# log_prob = posterior_ensemble.log_prob(\n",
    "#     samples, # the generated samples from 3.\n",
    "#     torch.Tensor(x_train[0][ind]).to(device) # the chosen observation from 2.\n",
    "#     )\n",
    "\n",
    "# # convert to numpy so can read easier.\n",
    "# samples = samples.cpu().numpy()\n",
    "# log_prob = log_prob.cpu().numpy()\n",
    "\n",
    "# from matplotlib.gridspec import GridSpec\n",
    "# def plot_posterior_samples_grid(samples, log_prob, param_names, df_info, model_name, train_args):\n",
    "#    n_params = len(param_names)\n",
    "#    n_cols = 4\n",
    "#    n_rows = (n_params + n_cols - 1) // n_cols\n",
    "   \n",
    "#    fig = plt.figure(figsize=(20, 5*n_rows))\n",
    "   \n",
    "#    # Add main title\n",
    "#    fig.suptitle('Posterior Probability Distributions', fontsize=16, y=0.98)\n",
    "   \n",
    "#    gs = GridSpec(n_rows, n_cols, figure=fig)\n",
    "   \n",
    "#    # Model info text\n",
    "#    model_info = (\n",
    "#        f\"Model Config:\\n\"\n",
    "#        f\"Name: {model_name}\\n\"\n",
    "#        f\"Hidden Features: {hidden_features}\\n\"\n",
    "#        f\"Num Transforms: {num_transforms}\\n\"\n",
    "#        f\"\\nTraining Args:\\n\"\n",
    "#        f\"Batch Size: {train_args['training_batch_size']}\\n\"\n",
    "#        f\"Learning Rate: {train_args['learning_rate']}\\n\"\n",
    "#        f\"Stop After Epochs: {train_args['stop_after_epochs']}\"\n",
    "#    )\n",
    "   \n",
    "#    fig.text(0.02, 0.96, model_info, \n",
    "#             fontsize=8,\n",
    "#             bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),\n",
    "#             verticalalignment='top')\n",
    "   \n",
    "#    for i, name in enumerate(param_names):\n",
    "#        row = i // n_cols\n",
    "#        col = i % n_cols\n",
    "       \n",
    "#        ax = fig.add_subplot(gs[row, col])\n",
    "#        data = samples[:, i]\n",
    "#        param_info = df_info[df_info['ParamName'] == name].iloc[0]\n",
    "#        is_log = param_info['LogFlag'] == 1\n",
    "       \n",
    "#        if is_log:\n",
    "#            ax.hist(data, bins=50, density=True, alpha=0.6)\n",
    "#            ax.set_xscale('log')\n",
    "#            log_data = np.log10(data)\n",
    "#            mean = np.mean(log_data)\n",
    "#            std = np.std(log_data)\n",
    "#            stats_text = f'Log10 Mean: {mean:.3f}\\nLog10 Std: {std:.3f}'\n",
    "#            ax.set_xlabel('Parameter Value (log scale)', fontsize=8)\n",
    "#        else:\n",
    "#            ax.hist(data, bins=50, density=True, alpha=0.6)\n",
    "#            mean = np.mean(data)\n",
    "#            std = np.std(data)\n",
    "#            stats_text = f'Mean: {mean:.3f}\\nStd: {std:.3f}'\n",
    "#            ax.set_xlabel('Parameter Value', fontsize=8)\n",
    "       \n",
    "#        ax.set_ylabel('Density', fontsize=8)\n",
    "       \n",
    "#        ax.axvline(param_info['MinVal'], color='g', linestyle=':', alpha=0.5, label='Min')\n",
    "#        ax.axvline(param_info['MaxVal'], color='g', linestyle=':', alpha=0.5, label='Max')\n",
    "#        ax.axvline(param_info['FiducialVal'], color='r', linestyle='--', alpha=0.5, label='Fiducial')\n",
    "       \n",
    "#        ax.text(0.02, 0.95, stats_text, transform=ax.transAxes, \n",
    "#                verticalalignment='top', fontsize=8, \n",
    "#                bbox=dict(facecolor='white', alpha=0.8))\n",
    "       \n",
    "#        ax.set_title(f\"{name}\\n{param_info['Description']}\", fontsize=8, pad=5)\n",
    "#        ax.tick_params(labelsize=8)\n",
    "       \n",
    "#        if i == 0:\n",
    "#            ax.legend(loc='upper right', fontsize=8)\n",
    "   \n",
    "#    plt.tight_layout()\n",
    "#    plt.subplots_adjust(top=0.93)  # Adjusted to make room for main title\n",
    "#    return fig\n",
    "\n",
    "# # Get all parameter names from df_info\n",
    "# param_names = df_info['ParamName'].tolist()\n",
    "\n",
    "# # Now try plotting again with the correct parameter names\n",
    "# fig = plot_posterior_samples_grid(\n",
    "#     samples, \n",
    "#     log_prob, \n",
    "#     param_names,  # Now contains all 28 parameter names correctly\n",
    "#     df_info,\n",
    "#     model_name=name,\n",
    "#     train_args=train_args\n",
    "# )\n",
    "\n",
    "# # Save with model config in filename\n",
    "# save_name = (f'parameter_posteriors_grid_{name}_'\n",
    "#             f'h{hidden_features}_t{num_transforms}_'\n",
    "#             f'b{train_args[\"training_batch_size\"]}_'\n",
    "#             f'e{train_args[\"stop_after_epochs\"]}.png')\n",
    "\n",
    "# os.makedirs(plots_out_dir, exist_ok=True)\n",
    "# plt.savefig(os.path.join(plots_out_dir, save_name), \n",
    "#             dpi=300, \n",
    "#             bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (camels)",
   "language": "python",
   "name": "camels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
