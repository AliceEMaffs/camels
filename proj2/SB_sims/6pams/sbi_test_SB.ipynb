{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgridspec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSpec\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmcolors\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Normalizer\n",
      "File \u001b[0;32m/disk/xray15/aem2/envs/camels/lib/python3.8/site-packages/torch/__init__.py:1481\u001b[0m\n\u001b[1;32m   1475\u001b[0m __all__\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewaxis\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;66;03m# Define Storage and Tensor classes\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m-> 1481\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _StorageBase, TypedStorage, _LegacyStorage, UntypedStorage, _warn_typed_storage_removal\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;66;03m# NOTE: New <type>Storage classes should never be added. When adding a new\u001b[39;00m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;66;03m# dtype, use torch.storage.TypedStorage directly.\u001b[39;00m\n",
      "File \u001b[0;32m/disk/xray15/aem2/envs/camels/lib/python3.8/site-packages/torch/_tensor.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_C\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhooks\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_namedtensor_internals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     check_serializing_named_tensor,\n\u001b[1;32m     16\u001b[0m     is_ellipsis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     update_names,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     get_default_nowrap_functions,\n\u001b[1;32m     24\u001b[0m     handle_torch_function,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     has_torch_function_variadic,\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m/disk/xray15/aem2/envs/camels/lib/python3.8/site-packages/torch/utils/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mthroughput_benchmark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThroughputBenchmark\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_backtrace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cpp_backtrace\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rename_privateuse1_backend, generate_methods_for_privateuse1_backend\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deterministic\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:839\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:934\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import joblib\n",
    "import ili\n",
    "from ili.dataloaders import NumpyLoader\n",
    "from ili.inference import InferenceRunner\n",
    "from ili.validation.metrics import PosteriorCoverage\n",
    "\n",
    "from sbi.utils.user_input_checks import process_prior\n",
    "\n",
    "sys.path.append(\"/disk/xray15/aem2/camels/proj2\")\n",
    "from setup_params_1P import plot_uvlf, plot_colour\n",
    "from setup_params_SB import *\n",
    "# from setup_params import *\n",
    "from priors_SB import initialise_priors_SB28\n",
    "from variables_config_28 import uvlf_limits, n_bins_lf, colour_limits, n_bins_colour\n",
    "\n",
    "# parameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = \"IllustrisTNG\"\n",
    "spec_type = \"attenuated\"\n",
    "sps = \"BC03\"\n",
    "snap = [\"044\"]\n",
    "bands = \"all\" # or just GALEX?\n",
    "\n",
    "colours = True  \n",
    "luminosity_functions = True\n",
    "name = f\"{model}_{bands}_{sps}_{spec_type}_{n_bins_lf}_{n_bins_colour}\"\n",
    "\n",
    "snap_str = str(snap[0])\n",
    "\n",
    "cam = camels(model=model, sim_set='SB28')\n",
    "\n",
    "if colours and not luminosity_functions:\n",
    "    model_out_dir = os.path.join(\"/disk/xray15/aem2/data/6pams/SB/IllustrisTNG/models/colours_only/\", f\"{snap_str}\")\n",
    "    plots_out_dir = os.path.join(\"/disk/xray15/aem2/plots/6pams/SB/IllustrisTNG/test/sbi_plots/colours_only/\", f\"{snap_str}\")\n",
    "    \n",
    "elif luminosity_functions and not colours:\n",
    "    model_out_dir = os.path.join(\"/disk/xray15/aem2/data/6pams/SB/IllustrisTNG/models/lf_only/\", f\"{snap_str}\")\n",
    "    plots_out_dir = os.path.join(\"/disk/xray15/aem2/plots/6pams/SB/IllustrisTNG/test/sbi_plots/lf_only/\", f\"{snap_str}\")\n",
    "\n",
    "elif colours and luminosity_functions:\n",
    "    model_out_dir = os.path.join(\"/disk/xray15/aem2/data/6pams/SB/IllustrisTNG/models/colours_lfs/\", f\"{snap_str}\")\n",
    "    plots_out_dir = os.path.join(\"/disk/xray15/aem2/plots/6pams/SB/IllustrisTNG/test/sbi_plots/colours_lfs/\", f\"{snap_str}\")\n",
    "\n",
    "# You might want to add an else for safety:\n",
    "else:\n",
    "    raise ValueError(\"At least one of colours or luminosity_functions must be True\")\n",
    "\n",
    "# number of astronomy and cosmology parameters we want to use.\n",
    "num_asco_pams = 6\n",
    "\n",
    "\n",
    "print(\"Saving model in \", model_out_dir)\n",
    "print(\"Saving plots in \", plots_out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import GPUtil  # You already have this imported\n",
    "import torch\n",
    "# Add these imports at the top with your other imports\n",
    "import os\n",
    "from pynvml import *\n",
    "\n",
    "# Add this function at the start, before any GPU operations\n",
    "def select_least_used_gpu():\n",
    "    try:\n",
    "        # Initialize NVIDIA management library\n",
    "        nvmlInit()\n",
    "        \n",
    "        # Get number of GPUs\n",
    "        deviceCount = nvmlDeviceGetCount()\n",
    "        gpu_memory_used = []\n",
    "        \n",
    "        # Check memory usage for each GPU\n",
    "        for i in range(deviceCount):\n",
    "            handle = nvmlDeviceGetHandleByIndex(i)\n",
    "            info = nvmlDeviceGetMemoryInfo(handle)\n",
    "            gpu_memory_used.append((i, info.used))\n",
    "        \n",
    "        # Sort by memory usage and get GPU with least memory used\n",
    "        least_used_gpu = sorted(gpu_memory_used, key=lambda x: x[1])[0][0]\n",
    "        \n",
    "        # Set CUDA device\n",
    "        torch.cuda.set_device(least_used_gpu)\n",
    "        print(f\"Selected GPU {least_used_gpu} with {gpu_memory_used[least_used_gpu][1]/1024**2:.0f}MB used\")\n",
    "        \n",
    "        return least_used_gpu\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error selecting GPU: {e}\")\n",
    "        return 0  # Default to first GPU if there's an error\n",
    "\n",
    "# Add this right after your imports, before any GPU operations\n",
    "# Select GPU and set device\n",
    "gpu_id = select_least_used_gpu()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# The rest of your code remains the same, but remove or comment out your original device line:\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cam = camels(model=model, sim_set='SB28')\n",
    "cam.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cam.labels[0:num_asco_pams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chris version of sbi:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# prior values come from this:\n",
    "df_info = pd.read_csv(\"/disk/xray15/aem2/data/28pams/Info_IllustrisTNG_L25n256_28params.txt\")\n",
    "df_info6 = df_info[0:num_asco_pams]\n",
    "df_info6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# get the priors and data\n",
    "prior = initialise_priors_SB28(\n",
    "    df=df_info6, \n",
    "    device=device,\n",
    "    astro=True,\n",
    "    dust=False  # no dust for testing. set to False to only get the 28 model parameters.\n",
    "    # with dust = True, prior has 32 dimensions (28 parameters + 4 dust parameters) \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    theta, x = get_theta_x_SB(\n",
    "        photo_dir=f\"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/photometry\", #/alice_galex.h5\",\n",
    "        spec_type=spec_type,\n",
    "        model=model,\n",
    "        snap=snap,\n",
    "        sps=sps,\n",
    "        n_bins_lf=13,\n",
    "        n_bins_colour=n_bins_colour,\n",
    "        colours=colours,\n",
    "        luminosity_functions=luminosity_functions,\n",
    "        device=device,\n",
    "    )\n",
    "    print(theta.shape, x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if luminosity_functions:\n",
    "    fig = plot_uvlf(x)\n",
    "    plt.savefig('/disk/xray15/aem2/plots/6pams/SB/IllustrisTNG/test/LFs_test/lf_check.png')\n",
    "    plt.title('UVLF Before any scaling/normalisation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if colours:\n",
    "    fig = plot_colour(x, n_bins=n_bins_colour, n_sims_to_plot=15)\n",
    "    plt.savefig('/disk/xray15/aem2/plots/6pams/SB/IllustrisTNG/test/colours_test/colour_check.png')\n",
    "    plt.title('Colour Before any scaling/normalisation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "x_all = np.array([np.hstack(_x) for _x in x])\n",
    "\n",
    "norm = Normalizer()\n",
    "x_all = torch.tensor(\n",
    "    norm.fit_transform(X=x_all),\n",
    "    # x_all,\n",
    "    dtype=torch.float32,\n",
    "    device=device, \n",
    ")\n",
    "\n",
    "# make test mask\n",
    "test_mask = create_test_mask() # 10% testing\n",
    "# train_mask = ~test_mask # 90% for training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# # Network architecture improvements\n",
    "hidden_features = 80  # Chris HF\n",
    "num_transforms = 4    # Chris NT\n",
    "num_nets = 3 # increased from 2\n",
    "# # num_bins = 10 # spline bins, this is default in sbi package anyway.\n",
    "\n",
    "# best model so far:\n",
    "# batch4_lr0.0005_epochs20_max_num_epochs100_validation_fraction0.1_clip_max_norm1_h60_t4_nn3\n",
    "train_args = {\n",
    "    \"training_batch_size\": 4,\n",
    "    \"learning_rate\": 5e-4,      # Chris LR\n",
    "    \"stop_after_epochs\": 20,     \n",
    "    \"max_num_epochs\": 100,       \n",
    "    \"clip_max_norm\": 1,       # try best with clipping at lower val (default is 5 in sbi)\n",
    "    \"validation_fraction\": 0.1, # Keep current validation split\n",
    "    \"use_combined_loss\": True,\n",
    "    \"show_train_summary\": True,\n",
    "    \"dataloader_kwargs\": {\n",
    "        \"num_workers\": 0,\n",
    "        \"pin_memory\": False\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define config string for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Create config string for filenames\n",
    "config_str = (f\"batch{train_args['training_batch_size']}_\"\n",
    "             f\"lr{train_args['learning_rate']}_\"\n",
    "             f\"epochs{train_args['stop_after_epochs']}_\"\n",
    "             f\"max_num_epochs{train_args['max_num_epochs']}_\"\n",
    "             f\"validation_fraction{train_args['validation_fraction']}_\"\n",
    "             f\"clip_max_norm{train_args['clip_max_norm']}_\"\n",
    "             f\"h{hidden_features}_t{num_transforms}_nn{num_nets}\")\n",
    "\n",
    "# Create a new directory with the config string\n",
    "config_plots_dir = os.path.join(plots_out_dir, config_str)\n",
    "config_model_dir = os.path.join(model_out_dir, config_str)\n",
    "os.makedirs(config_plots_dir, exist_ok=True)\n",
    "os.makedirs(config_model_dir, exist_ok=True)\n",
    "print(\"Model will be saved to: \", config_model_dir)\n",
    "print(\"Plots will be saved to: \", config_plots_dir)\n",
    "\n",
    "# save normaliser to config dir.\n",
    "joblib.dump(norm, os.path.join(config_model_dir, f'data_normaliser_{name}_scaler.save'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Increase ensemble size and capacity slightly\n",
    "nets = [\n",
    "    ili.utils.load_nde_sbi(\n",
    "        engine=\"NPE\",\n",
    "        model=\"nsf\", \n",
    "        hidden_features=hidden_features,      # Reduce further for better σ8 and ASN1/2\n",
    "        num_transforms=num_transforms,        # Reduce transforms for simpler model\n",
    "    ) for _ in range(num_nets)         # Keep ensemble size\n",
    "]\n",
    "\n",
    "# Keep the existing loader setup with scaled data\n",
    "loader = NumpyLoader(\n",
    "    x=x_all[~test_mask],\n",
    "    theta=torch.tensor(theta[~test_mask, :], device=device)\n",
    ")\n",
    "\n",
    "\n",
    "runner = InferenceRunner.load(\n",
    "    backend=\"sbi\",\n",
    "    engine=\"NPE\",\n",
    "    prior=prior,\n",
    "    nets=nets,\n",
    "    device=device,\n",
    "    train_args=train_args,\n",
    "    proposal=None,\n",
    "    out_dir=config_model_dir,\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "print('Training with ', config_str)\n",
    "\n",
    "posterior_ensemble, summaries = runner(loader=loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create comprehensive results dictionary\n",
    "results = {\n",
    "    \"posterior_ensemble\": posterior_ensemble,\n",
    "    \"summaries\": summaries,\n",
    "    \"config\": {\n",
    "        \"hidden_features\": hidden_features,\n",
    "        \"num_transforms\": num_transforms,\n",
    "        \"num_nets\": num_nets,\n",
    "        \"train_args\": train_args,\n",
    "        \"config_str\": config_str\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the complete results in the config directory\n",
    "\n",
    "# Save using pickle for the complete object\n",
    "with open(os.path.join(config_model_dir, f\"{name}_complete.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "# Also save the posterior ensemble using torch.save for compatibility\n",
    "torch.save(posterior_ensemble, os.path.join(config_model_dir, f\"{name}_posterior.pkl\"))\n",
    "\n",
    "# Copy or move the summary file if it was created in the original directory\n",
    "summary_file = os.path.join(model_out_dir, f\"{name}summary.json\")\n",
    "if os.path.exists(summary_file):\n",
    "    import shutil\n",
    "    shutil.copy2(summary_file, os.path.join(config_model_dir, f\"{name}summary.json\"))\n",
    "    \n",
    "print(f\"Complete model saved to: {os.path.join(config_model_dir, f'{name}_complete.pkl')}\")\n",
    "print(f\"Posterior ensemble saved to: {os.path.join(config_model_dir, f'{name}_posterior.pkl')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to the complete model file\n",
    "complete_model_path = \"/disk/xray15/aem2/data/6pams/SB/IllustrisTNG/models/colours_lfs/044/batch4_lr0.0005_epochs20_max_num_epochs100_validation_fraction0.1_h60_t4_nn2/IllustrisTNG_all_BC03_attenuated_13_13_complete.pkl\"\n",
    "\n",
    "# Try to load the file\n",
    "try:\n",
    "    with open(complete_model_path, 'rb') as f:\n",
    "        complete_model = pickle.load(f)\n",
    "    \n",
    "    # Print the structure of the object\n",
    "    print(\"Type of complete_model:\", type(complete_model))\n",
    "    \n",
    "    if isinstance(complete_model, dict):\n",
    "        print(\"\\nKeys in complete_model:\", complete_model.keys())\n",
    "        \n",
    "        # Check for validation metrics\n",
    "        if 'summaries' in complete_model:\n",
    "            print(\"\\nType of summaries:\", type(complete_model['summaries']))\n",
    "            \n",
    "            if isinstance(complete_model['summaries'], list):\n",
    "                print(\"\\nNumber of items in summaries:\", len(complete_model['summaries']))\n",
    "                print(\"\\nKeys in first summary item:\", complete_model['summaries'][0].keys())\n",
    "                \n",
    "                if 'validation_log_probs' in complete_model['summaries'][0]:\n",
    "                    val_probs = complete_model['summaries'][0]['validation_log_probs']\n",
    "                    print(\"\\nValidation log probabilities:\")\n",
    "                    print(f\"Min: {min(val_probs)}, Max: {max(val_probs)}\")\n",
    "                    print(f\"Best epoch: {val_probs.index(max(val_probs))}\")\n",
    "                    \n",
    "                    # Print a few values\n",
    "                    print(\"\\nFirst few validation log probabilities:\")\n",
    "                    for i, prob in enumerate(val_probs[:5]):\n",
    "                        print(f\"Epoch {i+1}: {prob}\")\n",
    "                    \n",
    "                    print(\"\\nLast few validation log probabilities:\")\n",
    "                    for i, prob in enumerate(val_probs[-5:]):\n",
    "                        print(f\"Epoch {len(val_probs)-4+i}: {prob}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or processing the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "x_test_np = x_all[test_mask]\n",
    "theta_test_np = theta[test_mask].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "type(x_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "type(theta_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# First plot: Training diagnostics with two subplots\n",
    "def plot_training_diagnostics(summaries):\n",
    "    \"\"\"Plot training diagnostics with loss and overfitting gap\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    train_losses = summaries[0]['training_log_probs']\n",
    "    val_losses = summaries[0]['validation_log_probs']\n",
    "    epochs = range(len(train_losses))\n",
    "    \n",
    "    ax1.plot(epochs, train_losses, '-', label='Training', color='blue')\n",
    "    ax1.plot(epochs, val_losses, '--', label='Validation', color='red')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Log probability')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overfitting gap\n",
    "    gap = np.array(train_losses) - np.array(val_losses)\n",
    "    ax2.plot(epochs, gap, '-', color='purple')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss difference')\n",
    "    ax2.set_title('Overfitting Gap')\n",
    "    ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Second plot: Ensemble training curves\n",
    "def plot_ensemble_training(summaries):\n",
    "    \"\"\"Plot training curves for each ensemble member\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6,4))\n",
    "    c = list(mcolors.TABLEAU_COLORS)\n",
    "    for i, m in enumerate(summaries):\n",
    "        ax.plot(m['training_log_probs'], ls='-', label=f\"{i}_train\", c=c[i])\n",
    "        ax.plot(m['validation_log_probs'], ls='--', label=f\"{i}_val\", c=c[i])\n",
    "    ax.set_xlim(0)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Log probability')\n",
    "    ax.legend()\n",
    "    return fig\n",
    "\n",
    "# Save training plots in the new directory\n",
    "fig1 = plot_training_diagnostics(summaries)\n",
    "plt.savefig(os.path.join(config_plots_dir, f'training_analysis_{name}.png'),\n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "fig2 = plot_ensemble_training(summaries)\n",
    "plt.savefig(os.path.join(config_plots_dir, f'ensemble_training_{name}.png'),\n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Create metric and get plots\n",
    "metric = PosteriorCoverage(\n",
    "    num_samples=int(1000),\n",
    "    sample_method='direct',\n",
    "    labels=cam.labels[0:num_asco_pams],\n",
    "    plot_list=[\"coverage\", \"histogram\", \"predictions\", \"tarp\", \"logprob\"], \n",
    ")\n",
    "\n",
    "# Create metric and get plots\n",
    "metric = PosteriorCoverage(\n",
    "    num_samples=int(1000),\n",
    "    sample_method='direct',\n",
    "    labels=cam.labels[0:num_asco_pams],\n",
    "    plot_list=[\"coverage\", \"histogram\", \"predictions\", \"tarp\", \"logprob\"], \n",
    ")\n",
    "\n",
    "# Get the metric plots\n",
    "plot_types = [\"coverage\", \"histogram\", \"predictions\", \"tarp\", \"logprob\"]\n",
    "figs = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x=x_test_np,\n",
    "    theta=theta_test_np,\n",
    "    signature=f\"coverage_{name}_\"\n",
    ")\n",
    "\n",
    "# Save and display the metric plots\n",
    "def save_and_display_metric_plots(figs, plot_types, config_plots_dir, name):\n",
    "    for fig, plot_type in zip(figs, plot_types):\n",
    "        if fig is not None:\n",
    "            # Ensure the figure is a matplotlib figure\n",
    "            if hasattr(fig, 'savefig'):\n",
    "                plot_path = os.path.join(config_plots_dir, f'{plot_type}_{name}.png')\n",
    "                fig.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "                plt.figure(fig.number)  # Make this figure the current figure\n",
    "                plt.show()  # Display the figure\n",
    "                plt.close(fig)  # Close the figure to free up memory\n",
    "            else:\n",
    "                print(f\"Warning: {plot_type} figure is not a standard matplotlib figure\")\n",
    "\n",
    "# Save and display the metric plots\n",
    "save_and_display_metric_plots(figs, plot_types, config_plots_dir, name)\n",
    "\n",
    "print(f\"All plots saved in: {config_plots_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors / Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# first try tweaking the time and then try tweaking the model a little and making it simpler and leting it run for longer.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the metric object\n",
    "metric = PosteriorSamples(\n",
    "    num_samples=int(1e4),  # 10,000 samples like your supervisor used\n",
    "    sample_method=\"direct\",\n",
    ")\n",
    "\n",
    "# Now use it to get posterior samples\n",
    "psamps = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x=x_all[test_mask],\n",
    "    theta=theta[test_mask],\n",
    ")\n",
    "\n",
    "# Calculate the percentiles and metrics\n",
    "perc = np.percentile(psamps, q=[16, 50, 84], axis=0)\n",
    "\n",
    "# Calculate RMSE, epsilon, R², and χ²\n",
    "rmse = np.sqrt(\n",
    "    np.sum((theta.cpu().numpy()[test_mask, :] - perc[1, :, :])**2, axis=0) / \n",
    "    np.sum(test_mask)\n",
    ")\n",
    "\n",
    "# Mean relative error (epsilon)\n",
    "mre = np.sum(\n",
    "    ((perc[2, :, :] - perc[0, :, :]) / 2) / perc[1, :, :], axis=0\n",
    ") / np.sum(test_mask)\n",
    "\n",
    "# R-squared\n",
    "theta_hat = np.sum(theta.cpu().numpy()[test_mask, :], axis=0) / np.sum(test_mask)\n",
    "r2 = 1 - np.sum(\n",
    "    (theta.cpu().numpy()[test_mask, :] - perc[1, :, :])**2, axis=0\n",
    ") / np.sum(\n",
    "    (theta.cpu().numpy()[test_mask, :] - theta_hat)**2, axis=0\n",
    ")\n",
    "\n",
    "# Chi-squared\n",
    "chi2 = np.sum(\n",
    "    (theta.cpu().numpy()[test_mask, :] - perc[1, :, :])**2 /\n",
    "    ((perc[2, :, :] - perc[0, :, :]) / 2)**2, axis=0\n",
    ") / np.sum(test_mask)\n",
    "\n",
    "# Print results for each parameter\n",
    "for i, param in enumerate(cam.labels[0:num_asco_pams]):\n",
    "    print(f\"\\nMetrics for {param}:\")\n",
    "    print(f\"RMSE: {rmse[i]:.4f}\")\n",
    "    print(f\"Epsilon: {mre[i]:.4f}\")\n",
    "    print(f\"R²: {r2[i]:.4f}\")\n",
    "    print(f\"χ²: {chi2[i]:.4f}\")\n",
    "\n",
    "\n",
    "# Define the file path\n",
    "metrics_file = os.path.join(config_plots_dir, f'metrics_{name}.txt')\n",
    "\n",
    "# Write metrics to the file\n",
    "with open(metrics_file, 'w') as f:\n",
    "    for i, param in enumerate(cam.labels[0:num_asco_pams]):\n",
    "        f.write(f\"\\nMetrics for {param}:\\n\")\n",
    "        f.write(f\"RMSE: {rmse[i]:.4f}\\n\")\n",
    "        f.write(f\"Epsilon: {mre[i]:.4f}\\n\")\n",
    "        f.write(f\"R²: {r2[i]:.4f}\\n\")\n",
    "        f.write(f\"χ²: {chi2[i]:.4f}\\n\")\n",
    "\n",
    "print(f\"Metrics saved in: {metrics_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Using variables already defined:\n",
    "# x_all, theta, test_mask, cam, posterior_ensemble\n",
    "\n",
    "def create_corner_plot(samples, true_values, param_names, figsize=(12, 12)):\n",
    "    \"\"\"\n",
    "    Create a corner plot showing marginal and joint distributions of parameter samples.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    samples : array-like\n",
    "        Array of shape (n_samples, n_params) containing posterior samples\n",
    "    true_values : array-like\n",
    "        Array of shape (n_params,) containing true parameter values\n",
    "    param_names : list\n",
    "        List of parameter names\n",
    "    figsize : tuple\n",
    "        Figure size\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib Figure\n",
    "        Corner plot figure\n",
    "    \"\"\"\n",
    "    n_params = samples.shape[1]\n",
    "    fig, axes = plt.subplots(n_params, n_params, figsize=figsize)\n",
    "    \n",
    "    # Set up the axes\n",
    "    for i in range(n_params):\n",
    "        for j in range(n_params):\n",
    "            if i < j:\n",
    "                axes[i, j].set_visible(False)\n",
    "                continue\n",
    "                \n",
    "            if i == j:  # Diagonal: show marginal distributions\n",
    "                axes[i, i].hist(samples[:, i], bins=25, alpha=0.7, density=True)\n",
    "                axes[i, i].axvline(true_values[i], color='red', linestyle='--')\n",
    "                \n",
    "                # Only show x labels on bottom row\n",
    "                if i < n_params - 1:\n",
    "                    axes[i, i].set_xticklabels([])\n",
    "                else:\n",
    "                    axes[i, i].set_xlabel(param_names[i])\n",
    "                \n",
    "                # Remove y ticks for cleaner look\n",
    "                axes[i, i].set_yticks([])\n",
    "                \n",
    "            else:  # Off-diagonal: show joint distributions\n",
    "                axes[i, j].scatter(samples[:, j], samples[:, i], alpha=0.1, s=1)\n",
    "                axes[i, j].scatter(true_values[j], true_values[i], color='red', s=20, marker='*')\n",
    "                \n",
    "                # Only show x labels on bottom row\n",
    "                if i < n_params - 1:\n",
    "                    axes[i, j].set_xticklabels([])\n",
    "                else:\n",
    "                    axes[i, j].set_xlabel(param_names[j])\n",
    "                \n",
    "                # Only show y labels on leftmost column\n",
    "                if j > 0:\n",
    "                    axes[i, j].set_yticklabels([])\n",
    "                else:\n",
    "                    axes[i, j].set_ylabel(param_names[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_parameter_recovery(true_values, estimated_values, uncertainty_low, uncertainty_high, param_names):\n",
    "    \"\"\"\n",
    "    Plot true vs. estimated parameter values with error bars.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_values : array-like\n",
    "        Array of shape (n_samples, n_params) containing true parameter values\n",
    "    estimated_values : array-like\n",
    "        Array of shape (n_samples, n_params) containing estimated (median) values\n",
    "    uncertainty_low : array-like\n",
    "        Array of shape (n_samples, n_params) containing lower bounds\n",
    "    uncertainty_high : array-like\n",
    "        Array of shape (n_samples, n_params) containing upper bounds\n",
    "    param_names : list\n",
    "        List of parameter names\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib Figure\n",
    "        Parameter recovery plot\n",
    "    \"\"\"\n",
    "    n_params = true_values.shape[1]\n",
    "    fig, axes = plt.subplots(1, n_params, figsize=(n_params*4, 4))\n",
    "    \n",
    "    for i in range(n_params):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Calculate value ranges for equal aspect ratio\n",
    "        param_min = min(true_values[:, i].min(), estimated_values[:, i].min())\n",
    "        param_max = max(true_values[:, i].max(), estimated_values[:, i].max())\n",
    "        param_range = param_max - param_min\n",
    "        \n",
    "        # Add some padding\n",
    "        param_min -= 0.1 * param_range\n",
    "        param_max += 0.1 * param_range\n",
    "        \n",
    "        # Plot diagonal line\n",
    "        ax.plot([param_min, param_max], [param_min, param_max], 'k--', alpha=0.5)\n",
    "        \n",
    "        # Plot error bars\n",
    "        for j in range(len(true_values)):\n",
    "            ax.errorbar(\n",
    "                true_values[j, i], \n",
    "                estimated_values[j, i], \n",
    "                yerr=[[estimated_values[j, i] - uncertainty_low[j, i]], \n",
    "                      [uncertainty_high[j, i] - estimated_values[j, i]]],\n",
    "                fmt='o', alpha=0.5, color='blue', capsize=3\n",
    "            )\n",
    "        \n",
    "        # Compute R² for this parameter\n",
    "        true_mean = np.mean(true_values[:, i])\n",
    "        ss_tot = np.sum((true_values[:, i] - true_mean)**2)\n",
    "        ss_res = np.sum((true_values[:, i] - estimated_values[:, i])**2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        # Compute RMSE\n",
    "        rmse = np.sqrt(np.mean((true_values[:, i] - estimated_values[:, i])**2))\n",
    "        \n",
    "        ax.set_xlabel(f'True {param_names[i]}')\n",
    "        ax.set_ylabel(f'Estimated {param_names[i]}')\n",
    "        ax.set_title(f'{param_names[i]}\\nR² = {r2:.3f}, RMSE = {rmse:.3f}')\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlim(param_min, param_max)\n",
    "        ax.set_ylim(param_min, param_max)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def visualize_test_set_results(config_plots_dir, name):\n",
    "    \"\"\"\n",
    "    Generate and save visualization of test set results.\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(config_plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Set up CUDA if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # 1. Get posterior samples for test set\n",
    "    # Move tensors to CPU first\n",
    "    test_x_cpu = x_all[test_mask].cpu()\n",
    "    test_theta_cpu = theta[test_mask].cpu()\n",
    "    \n",
    "    # Number of test samples to visualize (limit to avoid overwhelming plots)\n",
    "    n_test_samples = min(50, test_theta_cpu.shape[0])\n",
    "    \n",
    "    # 2. Get predictions for each test sample\n",
    "    predictions = []\n",
    "    for i in range(n_test_samples):\n",
    "        # Get posterior samples for this test point\n",
    "        x_i = test_x_cpu[i].unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "        samples = posterior_ensemble.sample((10000,), x=x_i).cpu().numpy()\n",
    "        predictions.append(samples)\n",
    "    \n",
    "    # 3. Compute statistics from posterior samples\n",
    "    perc = []\n",
    "    for i in range(n_test_samples):\n",
    "        sample_perc = np.percentile(predictions[i], q=[16, 50, 84], axis=0)\n",
    "        perc.append(sample_perc)\n",
    "    \n",
    "    # Convert to numpy arrays for easier processing\n",
    "    perc = np.array(perc)  # Shape: [n_test_samples, 3 (percentiles), n_params]\n",
    "    test_theta_np = test_theta_cpu.numpy()[:n_test_samples]\n",
    "    \n",
    "    # 4. Create corner plot for a few test examples\n",
    "    for i in range(min(5, n_test_samples)):\n",
    "        fig = create_corner_plot(\n",
    "            samples=predictions[i], \n",
    "            true_values=test_theta_np[i], \n",
    "            param_names=cam.labels[0:num_asco_pams],\n",
    "            figsize=(15, 15)\n",
    "        )\n",
    "        fig.suptitle(f'Posterior Distribution for Test Sample {i+1}', fontsize=16)\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.97])  # Make room for suptitle\n",
    "        plt.savefig(os.path.join(config_plots_dir, f'corner_plot_test_sample_{i+1}_{name}.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # 5. Create parameter recovery plot\n",
    "    medians = perc[:, 1, :]  # Shape: [n_test_samples, n_params]\n",
    "    lower_bounds = perc[:, 0, :]\n",
    "    upper_bounds = perc[:, 2, :]\n",
    "    \n",
    "    fig = plot_parameter_recovery(\n",
    "        true_values=test_theta_np, \n",
    "        estimated_values=medians, \n",
    "        uncertainty_low=lower_bounds, \n",
    "        uncertainty_high=upper_bounds, \n",
    "        param_names=cam.labels[0:num_asco_pams]\n",
    "    )\n",
    "    fig.suptitle('Parameter Recovery on Test Set', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])  # Make room for suptitle\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'parameter_recovery_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 6. Create a table of metrics for each parameter\n",
    "    metrics = []\n",
    "    for i, param in enumerate(cam.labels[0:num_asco_pams]):\n",
    "        # Calculate metrics for this parameter\n",
    "        rmse = np.sqrt(np.mean((test_theta_np[:, i] - medians[:, i])**2))\n",
    "        mre = np.mean(((upper_bounds[:, i] - lower_bounds[:, i]) / 2) / medians[:, i])\n",
    "        true_mean = np.mean(test_theta_np[:, i])\n",
    "        ss_tot = np.sum((test_theta_np[:, i] - true_mean)**2)\n",
    "        ss_res = np.sum((test_theta_np[:, i] - medians[:, i])**2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        chi2 = np.mean((test_theta_np[:, i] - medians[:, i])**2 / \n",
    "                      ((upper_bounds[:, i] - lower_bounds[:, i]) / 2)**2)\n",
    "        \n",
    "        metrics.append({\n",
    "            'Parameter': param,\n",
    "            'RMSE': rmse,\n",
    "            'MRE': mre,\n",
    "            'R²': r2,\n",
    "            'χ²': chi2\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_df.to_csv(os.path.join(config_plots_dir, f'metrics_summary_{name}.csv'), index=False)\n",
    "    \n",
    "    # 7. Visualize color distributions and UVLFs from the test set\n",
    "    # Extract a few test examples to visualize\n",
    "    for i in range(min(3, n_test_samples)):\n",
    "        # Get the original (unscaled) data for this test sample\n",
    "        # Make sure to move tensor to CPU first\n",
    "        x_original = x_all[test_mask].cpu().numpy()[i]\n",
    "        \n",
    "        # Plot UVLF\n",
    "        fig_uvlf = plot_uvlf(np.array([x_original]), n_bins=n_bins_lf)\n",
    "        plt.suptitle(f'UVLF for Test Sample {i+1}', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.savefig(os.path.join(config_plots_dir, f'uvlf_test_sample_{i+1}_{name}.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close(fig_uvlf)\n",
    "        \n",
    "        # Plot Color Distribution\n",
    "        fig_color = plot_colour(np.array([x_original]), n_bins=n_bins_colour, n_sims_to_plot=1)\n",
    "        plt.suptitle(f'Color Distribution for Test Sample {i+1}', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.savefig(os.path.join(config_plots_dir, f'color_test_sample_{i+1}_{name}.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close(fig_color)\n",
    "    \n",
    "    # 8. Create a heatmap of parameter correlations\n",
    "    correlation_matrix = np.zeros((len(cam.labels[0:num_asco_pams]), len(cam.labels[0:num_asco_pams])))\n",
    "    for i in range(len(cam.labels[0:num_asco_pams])):\n",
    "        for j in range(len(cam.labels[0:num_asco_pams])):\n",
    "            r = np.corrcoef(test_theta_np[:, i], medians[:, j])[0, 1]\n",
    "            correlation_matrix[i, j] = r\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    im = ax.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xticks(np.arange(len(cam.labels[0:num_asco_pams])))\n",
    "    ax.set_yticks(np.arange(len(cam.labels[0:num_asco_pams])))\n",
    "    ax.set_xticklabels(cam.labels[0:num_asco_pams])\n",
    "    ax.set_yticklabels(cam.labels[0:num_asco_pams])\n",
    "    \n",
    "    # Rotate x labels for better readability\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Add a colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel('Correlation', rotation=-90, va=\"bottom\")\n",
    "    \n",
    "    # Add correlation values as text\n",
    "    for i in range(len(cam.labels[0:num_asco_pams])):\n",
    "        for j in range(len(cam.labels[0:num_asco_pams])):\n",
    "            text = ax.text(j, i, f\"{correlation_matrix[i, j]:.2f}\",\n",
    "                           ha=\"center\", va=\"center\", color=\"black\" if abs(correlation_matrix[i, j]) < 0.5 else \"white\")\n",
    "    \n",
    "    ax.set_title(\"Correlation between True and Estimated Parameters\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'parameter_correlation_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 9. Create a summary plot for all parameters\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, param in enumerate(cam.labels[0:num_asco_pams]):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot true vs. estimated with error bars\n",
    "        ax.errorbar(\n",
    "            test_theta_np[:, i], \n",
    "            medians[:, i], \n",
    "            yerr=[medians[:, i] - lower_bounds[:, i], upper_bounds[:, i] - medians[:, i]],\n",
    "            fmt='o', alpha=0.5, capsize=3, label='Posterior median with 68% CI'\n",
    "        )\n",
    "        \n",
    "        # Add diagonal line\n",
    "        param_min = min(test_theta_np[:, i].min(), medians[:, i].min())\n",
    "        param_max = max(test_theta_np[:, i].max(), medians[:, i].max())\n",
    "        param_range = param_max - param_min\n",
    "        param_min -= 0.1 * param_range\n",
    "        param_max += 0.1 * param_range\n",
    "        ax.plot([param_min, param_max], [param_min, param_max], 'k--', alpha=0.7, label='Perfect recovery')\n",
    "        \n",
    "        # Compute metrics\n",
    "        true_mean = np.mean(test_theta_np[:, i])\n",
    "        ss_tot = np.sum((test_theta_np[:, i] - true_mean)**2)\n",
    "        ss_res = np.sum((test_theta_np[:, i] - medians[:, i])**2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        rmse = np.sqrt(np.mean((test_theta_np[:, i] - medians[:, i])**2))\n",
    "        \n",
    "        ax.set_xlabel(f'True {param}')\n",
    "        ax.set_ylabel(f'Estimated {param}')\n",
    "        ax.set_title(f'{param}\\nR² = {r2:.3f}, RMSE = {rmse:.3f}')\n",
    "        \n",
    "        if i == 0:  # Only add legend to first subplot\n",
    "            ax.legend()\n",
    "        \n",
    "    # Remove any unused subplots\n",
    "    for i in range(len(cam.labels[0:num_asco_pams]), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "        \n",
    "    fig.suptitle('Parameter Recovery Summary', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'parameter_summary_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"All visualizations saved in: {config_plots_dir}\")\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Call the function to generate visualizations\n",
    "metrics_df = visualize_test_set_results(config_plots_dir, name)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "def compare_train_test_performance():\n",
    "    \"\"\"\n",
    "    Compare the model performance on training and test sets.\n",
    "    \"\"\"\n",
    "    # Set up CUDA if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Get samples for both training and test sets\n",
    "    train_x = x_all[~test_mask]\n",
    "    train_theta = theta[~test_mask]\n",
    "    test_x = x_all[test_mask]\n",
    "    test_theta = theta[test_mask]\n",
    "    \n",
    "    # Use a subsample to keep computation manageable\n",
    "    n_train_samples = min(50, train_theta.shape[0])\n",
    "    n_test_samples = min(50, test_theta.shape[0])\n",
    "    \n",
    "    # Randomly select indices for subsampling\n",
    "    train_indices = np.random.choice(train_theta.shape[0], n_train_samples, replace=False)\n",
    "    \n",
    "    # For test, use all if fewer than n_test_samples, otherwise randomly select\n",
    "    if test_theta.shape[0] <= n_test_samples:\n",
    "        test_indices = np.arange(test_theta.shape[0])\n",
    "    else:\n",
    "        test_indices = np.random.choice(test_theta.shape[0], n_test_samples, replace=False)\n",
    "    \n",
    "    # Get predictions for training samples\n",
    "    train_predictions = []\n",
    "    for i in train_indices:\n",
    "        x_i = train_x[i].unsqueeze(0)  # Add batch dimension\n",
    "        samples = posterior_ensemble.sample((1000,), x=x_i.to(device)).cpu().numpy()\n",
    "        train_predictions.append(samples)\n",
    "    \n",
    "    # Get predictions for test samples\n",
    "    test_predictions = []\n",
    "    for i in test_indices:\n",
    "        x_i = test_x[i].unsqueeze(0)  # Add batch dimension\n",
    "        samples = posterior_ensemble.sample((1000,), x=x_i.to(device)).cpu().numpy()\n",
    "        test_predictions.append(samples)\n",
    "    \n",
    "    # Compute statistics from posterior samples\n",
    "    train_perc = []\n",
    "    for i in range(len(train_indices)):\n",
    "        sample_perc = np.percentile(train_predictions[i], q=[16, 50, 84], axis=0)\n",
    "        train_perc.append(sample_perc)\n",
    "    \n",
    "    test_perc = []\n",
    "    for i in range(len(test_indices)):\n",
    "        sample_perc = np.percentile(test_predictions[i], q=[16, 50, 84], axis=0)\n",
    "        test_perc.append(sample_perc)\n",
    "    \n",
    "    # Convert to numpy arrays for easier processing\n",
    "    train_perc = np.array(train_perc)  # Shape: [n_samples, 3 (percentiles), n_params]\n",
    "    test_perc = np.array(test_perc)\n",
    "    train_theta_np = train_theta.cpu().numpy()[train_indices]\n",
    "    test_theta_np = test_theta.cpu().numpy()[test_indices]\n",
    "    \n",
    "    # Calculate metrics for both sets\n",
    "    train_metrics = []\n",
    "    test_metrics = []\n",
    "    \n",
    "    for i, param in enumerate(cam.labels[0:num_asco_pams]):\n",
    "        # Training metrics\n",
    "        train_rmse = np.sqrt(np.mean((train_theta_np[:, i] - train_perc[:, 1, i])**2))\n",
    "        train_mre = np.mean(((train_perc[:, 2, i] - train_perc[:, 0, i]) / 2) / train_perc[:, 1, i])\n",
    "        train_true_mean = np.mean(train_theta_np[:, i])\n",
    "        train_ss_tot = np.sum((train_theta_np[:, i] - train_true_mean)**2)\n",
    "        train_ss_res = np.sum((train_theta_np[:, i] - train_perc[:, 1, i])**2)\n",
    "        train_r2 = 1 - (train_ss_res / train_ss_tot)\n",
    "        train_chi2 = np.mean((train_theta_np[:, i] - train_perc[:, 1, i])**2 / \n",
    "                          ((train_perc[:, 2, i] - train_perc[:, 0, i]) / 2)**2)\n",
    "        \n",
    "        # Test metrics\n",
    "        test_rmse = np.sqrt(np.mean((test_theta_np[:, i] - test_perc[:, 1, i])**2))\n",
    "        test_mre = np.mean(((test_perc[:, 2, i] - test_perc[:, 0, i]) / 2) / test_perc[:, 1, i])\n",
    "        test_true_mean = np.mean(test_theta_np[:, i])\n",
    "        test_ss_tot = np.sum((test_theta_np[:, i] - test_true_mean)**2)\n",
    "        test_ss_res = np.sum((test_theta_np[:, i] - test_perc[:, 1, i])**2)\n",
    "        test_r2 = 1 - (test_ss_res / test_ss_tot)\n",
    "        test_chi2 = np.mean((test_theta_np[:, i] - test_perc[:, 1, i])**2 / \n",
    "                         ((test_perc[:, 2, i] - test_perc[:, 0, i]) / 2)**2)\n",
    "        \n",
    "        train_metrics.append({\n",
    "            'Parameter': param,\n",
    "            'Dataset': 'Training',\n",
    "            'RMSE': train_rmse,\n",
    "            'MRE': train_mre,\n",
    "            'R²': train_r2,\n",
    "            'χ²': train_chi2\n",
    "        })\n",
    "        \n",
    "        test_metrics.append({\n",
    "            'Parameter': param,\n",
    "            'Dataset': 'Test',\n",
    "            'RMSE': test_rmse,\n",
    "            'MRE': test_mre,\n",
    "            'R²': test_r2,\n",
    "            'χ²': test_chi2\n",
    "        })\n",
    "    \n",
    "    # Combine metrics\n",
    "    all_metrics = pd.DataFrame(train_metrics + test_metrics)\n",
    "    \n",
    "    # Create directory for plots\n",
    "    os.makedirs(config_plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    all_metrics.to_csv(os.path.join(config_plots_dir, f'train_test_metrics_comparison_{name}.csv'), index=False)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    # 1. Barplot comparisons\n",
    "    metrics_to_plot = ['RMSE', 'R²', 'MRE', 'χ²']\n",
    "    \n",
    "    for metric in metrics_to_plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = sns.barplot(x='Parameter', y=metric, hue='Dataset', data=all_metrics)\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, p in enumerate(ax.patches):\n",
    "            height = p.get_height()\n",
    "            ax.text(p.get_x() + p.get_width()/2., height + 0.01,\n",
    "                   f'{height:.3f}', ha=\"center\", fontsize=9)\n",
    "        \n",
    "        plt.title(f'Comparison of {metric} Between Training and Test Sets')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config_plots_dir, f'train_test_{metric}_comparison_{name}.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    # 2. Create a combined visualization for all metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        ax = axes[i]\n",
    "        sns.barplot(x='Parameter', y=metric, hue='Dataset', data=all_metrics, ax=ax)\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for j, p in enumerate(ax.patches):\n",
    "            height = p.get_height()\n",
    "            ax.text(p.get_x() + p.get_width()/2., height + 0.01,\n",
    "                   f'{height:.3f}', ha=\"center\", fontsize=8)\n",
    "        \n",
    "        ax.set_title(f'Comparison of {metric}')\n",
    "        \n",
    "    fig.suptitle('Performance Metrics: Training vs Test Set', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'train_test_metrics_summary_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 3. Create a 2D comparison plot for each parameter\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, param in enumerate(cam.labels[0:num_asco_pams]):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot training samples\n",
    "        ax.errorbar(\n",
    "            train_theta_np[:, i], \n",
    "            train_perc[:, 1, i], \n",
    "            yerr=[train_perc[:, 1, i] - train_perc[:, 0, i], train_perc[:, 2, i] - train_perc[:, 1, i]],\n",
    "            fmt='o', alpha=0.4, capsize=3, label='Training', color='blue'\n",
    "        )\n",
    "        \n",
    "        # Plot test samples\n",
    "        ax.errorbar(\n",
    "            test_theta_np[:, i], \n",
    "            test_perc[:, 1, i], \n",
    "            yerr=[test_perc[:, 1, i] - test_perc[:, 0, i], test_perc[:, 2, i] - test_perc[:, 1, i]],\n",
    "            fmt='o', alpha=0.4, capsize=3, label='Test', color='red'\n",
    "        )\n",
    "        \n",
    "        # Add diagonal line\n",
    "        all_theta = np.concatenate([train_theta_np[:, i], test_theta_np[:, i]])\n",
    "        all_medians = np.concatenate([train_perc[:, 1, i], test_perc[:, 1, i]])\n",
    "        param_min = min(all_theta.min(), all_medians.min())\n",
    "        param_max = max(all_theta.max(), all_medians.max())\n",
    "        param_range = param_max - param_min\n",
    "        param_min -= 0.1 * param_range\n",
    "        param_max += 0.1 * param_range\n",
    "        ax.plot([param_min, param_max], [param_min, param_max], 'k--', alpha=0.7)\n",
    "        \n",
    "        # Get Train and Test R² for title\n",
    "        train_r2 = all_metrics[(all_metrics['Parameter'] == param) & \n",
    "                               (all_metrics['Dataset'] == 'Training')]['R²'].values[0]\n",
    "        test_r2 = all_metrics[(all_metrics['Parameter'] == param) & \n",
    "                             (all_metrics['Dataset'] == 'Test')]['R²'].values[0]\n",
    "        \n",
    "        ax.set_xlabel(f'True {param}')\n",
    "        ax.set_ylabel(f'Estimated {param}')\n",
    "        ax.set_title(f'{param}\\nTrain R² = {train_r2:.3f}, Test R² = {test_r2:.3f}')\n",
    "        \n",
    "        if i == 0:  # Only add legend to first subplot\n",
    "            ax.legend()\n",
    "        \n",
    "    # Remove any unused subplots\n",
    "    for i in range(len(cam.labels[0:num_asco_pams]), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "        \n",
    "    fig.suptitle('Parameter Recovery: Training vs Test Set', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'train_test_recovery_comparison_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"Training vs. Test comparison visualizations saved in: {config_plots_dir}\")\n",
    "    \n",
    "    return all_metrics\n",
    "\n",
    "# Call the function to generate the comparison\n",
    "comparison_metrics = compare_train_test_performance()\n",
    "print(comparison_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "def conduct_posterior_predictive_checks():\n",
    "    \"\"\"\n",
    "    Perform posterior predictive checks to validate model performance.\n",
    "    \n",
    "    This involves:\n",
    "    1. Sampling from the posterior distributions for test set observations\n",
    "    2. Using these parameter samples to simulate data (forward model)\n",
    "    3. Comparing simulated data to observed data\n",
    "    \n",
    "    Since we don't have direct access to the forward model, we'll:\n",
    "    - Analyze posterior coverage and calibration\n",
    "    - Check parameter correlations and degeneracies\n",
    "    - Evaluate how well the constraints match expected values\n",
    "    \"\"\"\n",
    "    # Set up CUDA if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Get test set data\n",
    "    test_x = x_all[test_mask]\n",
    "    test_theta = theta[test_mask]\n",
    "    \n",
    "    # Number of test samples to analyze\n",
    "    n_test_samples = min(50, test_theta.shape[0])\n",
    "    test_indices = np.random.choice(test_theta.shape[0], n_test_samples, replace=False)\n",
    "    \n",
    "    # Create directory for outputs\n",
    "    os.makedirs(config_plots_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Analyze posterior width vs error\n",
    "    # This checks if posterior width (uncertainty) correlates with actual error\n",
    "    \n",
    "    # Get samples for each test point\n",
    "    all_samples = []\n",
    "    for i in test_indices:\n",
    "        x_i = test_x[i].unsqueeze(0)\n",
    "        samples = posterior_ensemble.sample((1000,), x=x_i.to(device)).cpu().numpy()\n",
    "        all_samples.append(samples)\n",
    "    \n",
    "    all_samples = np.array(all_samples)\n",
    "    test_theta_selected = test_theta[test_indices].cpu().numpy()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    medians = np.median(all_samples, axis=1)\n",
    "    lower_bounds = np.percentile(all_samples, 16, axis=1)\n",
    "    upper_bounds = np.percentile(all_samples, 84, axis=1)\n",
    "    \n",
    "    # Calculate errors and uncertainties\n",
    "    errors = np.abs(medians - test_theta_selected)\n",
    "    uncertainties = (upper_bounds - lower_bounds) / 2\n",
    "    \n",
    "    # Visualize error vs uncertainty\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, param in enumerate(cam.labels[0:num_asco_pams]):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot error vs uncertainty\n",
    "        ax.scatter(uncertainties[:, i], errors[:, i], alpha=0.7)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        corr = np.corrcoef(uncertainties[:, i], errors[:, i])[0, 1]\n",
    "        \n",
    "        # Add a diagonal line for reference\n",
    "        max_val = max(uncertainties[:, i].max(), errors[:, i].max()) * 1.1\n",
    "        ax.plot([0, max_val], [0, max_val], 'k--', alpha=0.5)\n",
    "        \n",
    "        ax.set_xlabel(f'Posterior Uncertainty (68% CI width/2) for {param}')\n",
    "        ax.set_ylabel(f'Absolute Error for {param}')\n",
    "        ax.set_title(f'{param}: Error vs. Uncertainty\\nCorrelation: {corr:.3f}')\n",
    "        \n",
    "    # Remove any unused subplots\n",
    "    for i in range(len(cam.labels[0:num_asco_pams]), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "        \n",
    "    fig.suptitle('Posterior Calibration: Error vs. Uncertainty', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'error_vs_uncertainty_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 2. Check coverage: what fraction of true values fall within different CI levels?\n",
    "    # Ideal: the 68% CI should contain the true value 68% of the time\n",
    "    \n",
    "    # Check coverage at different percentile levels\n",
    "    percentile_levels = [50, 68, 90, 95, 99]\n",
    "    coverage_results = []\n",
    "    \n",
    "    for i, param in enumerate(cam.labels[0:num_asco_pams]):\n",
    "        param_coverage = []\n",
    "        \n",
    "        for level in percentile_levels:\n",
    "            # Calculate lower and upper bounds for this confidence level\n",
    "            half_width = level / 2\n",
    "            lower = np.percentile(all_samples, 50 - half_width, axis=1)[:, i]\n",
    "            upper = np.percentile(all_samples, 50 + half_width, axis=1)[:, i]\n",
    "            \n",
    "            # Check if true values fall within bounds\n",
    "            within_bounds = np.logical_and(\n",
    "                test_theta_selected[:, i] >= lower,\n",
    "                test_theta_selected[:, i] <= upper\n",
    "            )\n",
    "            coverage = np.mean(within_bounds) * 100\n",
    "            \n",
    "            param_coverage.append({\n",
    "                'Parameter': param,\n",
    "                'Confidence Level': f'{level}%',\n",
    "                'Expected Coverage': level,\n",
    "                'Actual Coverage': coverage\n",
    "            })\n",
    "        \n",
    "        coverage_results.extend(param_coverage)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    coverage_df = pd.DataFrame(coverage_results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    coverage_df.to_csv(os.path.join(config_plots_dir, f'coverage_analysis_{name}.csv'), index=False)\n",
    "    \n",
    "    # Visualize coverage\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    for param in cam.labels[0:num_asco_pams]:\n",
    "        param_data = coverage_df[coverage_df['Parameter'] == param]\n",
    "        ax.plot(param_data['Expected Coverage'], param_data['Actual Coverage'], \n",
    "               'o-', label=param)\n",
    "    \n",
    "    # Add diagonal line for perfect coverage\n",
    "    ax.plot([0, 100], [0, 100], 'k--', alpha=0.7, label='Perfect Coverage')\n",
    "    \n",
    "    ax.set_xlabel('Expected Coverage (%)')\n",
    "    ax.set_ylabel('Actual Coverage (%)')\n",
    "    ax.set_title('Posterior Coverage Analysis')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.savefig(os.path.join(config_plots_dir, f'coverage_analysis_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 3. Look for parameter degeneracies in the posterior\n",
    "    # For a randomly selected test point, visualize full posterior\n",
    "    selected_test_idx = test_indices[0]\n",
    "    x_selected = test_x[selected_test_idx].unsqueeze(0)\n",
    "    samples_selected = posterior_ensemble.sample((5000,), x=x_selected.to(device)).cpu().numpy()\n",
    "    \n",
    "    # Create corner plot\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    n_params = len(cam.labels[0:num_asco_pams])\n",
    "    gs = GridSpec(n_params, n_params, figure=fig)\n",
    "    \n",
    "    for i in range(n_params):\n",
    "        for j in range(n_params):\n",
    "            if i < j:  # Upper triangle - leave empty\n",
    "                continue\n",
    "                \n",
    "            ax = fig.add_subplot(gs[i, j])\n",
    "            \n",
    "            if i == j:  # Diagonal - plot marginal distributions\n",
    "                ax.hist(samples_selected[:, i], bins=30, alpha=0.7, density=True)\n",
    "                ax.axvline(test_theta[selected_test_idx, i].item(), color='red', linestyle='--')\n",
    "                \n",
    "                if i < n_params - 1:\n",
    "                    ax.set_xticklabels([])\n",
    "                else:\n",
    "                    ax.set_xlabel(cam.labels[0:num_asco_pams][i])\n",
    "                \n",
    "                ax.set_yticks([])\n",
    "                \n",
    "            else:  # Lower triangle - plot 2D distributions\n",
    "                ax.scatter(samples_selected[:, j], samples_selected[:, i], alpha=0.1, s=1)\n",
    "                ax.scatter(test_theta[selected_test_idx, j].item(), \n",
    "                          test_theta[selected_test_idx, i].item(), \n",
    "                          color='red', marker='*', s=100)\n",
    "                \n",
    "                if i < n_params - 1:\n",
    "                    ax.set_xticklabels([])\n",
    "                else:\n",
    "                    ax.set_xlabel(cam.labels[0:num_asco_pams][j])\n",
    "                    \n",
    "                if j > 0:\n",
    "                    ax.set_yticklabels([])\n",
    "                else:\n",
    "                    ax.set_ylabel(cam.labels[0:num_asco_pams][i])\n",
    "    \n",
    "    fig.suptitle('Posterior Parameter Degeneracies for a Test Point', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'parameter_degeneracies_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 4. Calculate Kullback-Leibler divergence between prior and posterior\n",
    "    # This measures how much information we've gained from the data\n",
    "    \n",
    "    # Generate prior samples\n",
    "    prior_samples = prior.sample((10000,)).cpu().numpy()\n",
    "    \n",
    "    # Calculate KL divergence for each parameter and each test point\n",
    "    kl_divergences = []\n",
    "    \n",
    "    # Use only a few test points to avoid excessive computation\n",
    "    for i in test_indices[:5]:\n",
    "        x_i = test_x[i].unsqueeze(0)\n",
    "        posterior_samples = posterior_ensemble.sample((10000,), x=x_i.to(device)).cpu().numpy()\n",
    "        \n",
    "        for j, param in enumerate(cam.labels[0:num_asco_pams]):\n",
    "            # Use histogram approximation for KL divergence\n",
    "            bins = 50\n",
    "            prior_hist, bin_edges = np.histogram(prior_samples[:, j], bins=bins, density=True)\n",
    "            posterior_hist, _ = np.histogram(posterior_samples[:, j], bins=bin_edges, density=True)\n",
    "            \n",
    "            # Avoid division by zero and log of zero\n",
    "            prior_hist = np.maximum(prior_hist, 1e-10)\n",
    "            posterior_hist = np.maximum(posterior_hist, 1e-10)\n",
    "            \n",
    "            # KL divergence: sum(p(x) * log(p(x)/q(x)))\n",
    "            kl = np.sum(posterior_hist * np.log(posterior_hist / prior_hist)) * (bin_edges[1] - bin_edges[0])\n",
    "            \n",
    "            kl_divergences.append({\n",
    "                'Test Point': i,\n",
    "                'Parameter': param,\n",
    "                'KL Divergence': kl\n",
    "            })\n",
    "    \n",
    "    kl_df = pd.DataFrame(kl_divergences)\n",
    "    \n",
    "    # Visualize KL divergence\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Parameter', y='KL Divergence', data=kl_df)\n",
    "    plt.title('Information Gain (KL Divergence) from Prior to Posterior')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'kl_divergence_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Compute credible interval width for each parameter\n",
    "    # This shows which parameters are well-constrained vs poorly-constrained\n",
    "    \n",
    "    ci_widths = []\n",
    "    \n",
    "    for i, param in enumerate(cam.labels[0:num_asco_pams]):\n",
    "        for ci_level in [68, 95]:\n",
    "            half_width = ci_level / 2\n",
    "            lower_percentile = 50 - half_width\n",
    "            upper_percentile = 50 + half_width\n",
    "            \n",
    "            # Calculate width for prior\n",
    "            prior_lower = np.percentile(prior_samples[:, i], lower_percentile)\n",
    "            prior_upper = np.percentile(prior_samples[:, i], upper_percentile)\n",
    "            prior_width = prior_upper - prior_lower\n",
    "            \n",
    "            # Calculate width for each test point\n",
    "            posterior_widths = []\n",
    "            \n",
    "            for j in test_indices:\n",
    "                x_j = test_x[j].unsqueeze(0)\n",
    "                posterior_samples = posterior_ensemble.sample((1000,), x=x_j.to(device)).cpu().numpy()\n",
    "                \n",
    "                posterior_lower = np.percentile(posterior_samples[:, i], lower_percentile)\n",
    "                posterior_upper = np.percentile(posterior_samples[:, i], upper_percentile)\n",
    "                posterior_width = posterior_upper - posterior_lower\n",
    "                \n",
    "                posterior_widths.append(posterior_width)\n",
    "            \n",
    "            # Average width across test points\n",
    "            avg_posterior_width = np.mean(posterior_widths)\n",
    "            \n",
    "            # Width reduction (prior to posterior)\n",
    "            width_reduction = 1 - (avg_posterior_width / prior_width)\n",
    "            \n",
    "            ci_widths.append({\n",
    "                'Parameter': param,\n",
    "                'CI Level': f'{ci_level}%',\n",
    "                'Prior Width': prior_width,\n",
    "                'Posterior Width': avg_posterior_width,\n",
    "                'Width Reduction': width_reduction * 100  # as percentage\n",
    "            })\n",
    "    \n",
    "    ci_width_df = pd.DataFrame(ci_widths)\n",
    "    \n",
    "    # Save to CSV\n",
    "    ci_width_df.to_csv(os.path.join(config_plots_dir, f'credible_interval_analysis_{name}.csv'), index=False)\n",
    "    \n",
    "    # Visualize width reduction\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Only use 68% CI for clarity\n",
    "    plot_data = ci_width_df[ci_width_df['CI Level'] == '68%']\n",
    "    \n",
    "    sns.barplot(x='Parameter', y='Width Reduction', data=plot_data)\n",
    "    plt.title('Parameter Constraint Improvement (Prior to Posterior)')\n",
    "    plt.ylabel('Width Reduction (%)')\n",
    "    plt.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'constraint_improvement_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Rank parameters by recoverability\n",
    "    # Combine multiple metrics to assess overall parameter recovery quality\n",
    "    \n",
    "    # Use metrics we've already calculated\n",
    "    param_metrics = []\n",
    "    \n",
    "    for i, param in enumerate(cam.labels[0:num_asco_pams]):\n",
    "        # Average error\n",
    "        avg_error = np.mean(errors[:, i])\n",
    "        \n",
    "        # Average uncertainty\n",
    "        avg_uncertainty = np.mean(uncertainties[:, i])\n",
    "        \n",
    "        # Error-to-uncertainty ratio (closer to 1 is better)\n",
    "        err_unc_ratio = avg_error / avg_uncertainty\n",
    "        \n",
    "        # Width reduction from prior\n",
    "        width_red = plot_data[plot_data['Parameter'] == param]['Width Reduction'].values[0]\n",
    "        \n",
    "        # Actual vs. expected coverage at 68%\n",
    "        coverage_at_68 = coverage_df[(coverage_df['Parameter'] == param) & \n",
    "                                    (coverage_df['Confidence Level'] == '68%')]['Actual Coverage'].values[0]\n",
    "        coverage_error = abs(coverage_at_68 - 68)\n",
    "        \n",
    "        # Combined score (lower is better)\n",
    "        # Normalize each component to [0,1] range before combining\n",
    "        combined_score = (\n",
    "            err_unc_ratio / 2 +  # Penalize ratio far from 1 (0 is perfect)\n",
    "            (1 - width_red / 100) +  # Higher width reduction is better\n",
    "            coverage_error / 68  # Lower coverage error is better\n",
    "        ) / 3\n",
    "        \n",
    "        param_metrics.append({\n",
    "            'Parameter': param,\n",
    "            'Avg Error': avg_error,\n",
    "            'Avg Uncertainty': avg_uncertainty,\n",
    "            'Error/Uncertainty Ratio': err_unc_ratio,\n",
    "            'Width Reduction (%)': width_red,\n",
    "            'Coverage Error': coverage_error,\n",
    "            'Combined Score': combined_score\n",
    "        })\n",
    "    \n",
    "    param_metrics_df = pd.DataFrame(param_metrics)\n",
    "    param_metrics_df = param_metrics_df.sort_values('Combined Score')\n",
    "    \n",
    "    # Save metrics\n",
    "    param_metrics_df.to_csv(os.path.join(config_plots_dir, f'parameter_ranking_{name}.csv'), index=False)\n",
    "    \n",
    "    # Visualization of parameter ranking\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Parameter', y='Combined Score', data=param_metrics_df, \n",
    "               order=param_metrics_df['Parameter'])\n",
    "    plt.title('Parameter Recovery Quality (Lower Score is Better)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config_plots_dir, f'parameter_ranking_{name}.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Final summary table\n",
    "    summary = pd.DataFrame({\n",
    "        'Parameter': cam.labels[0:num_asco_pams],\n",
    "        'Best Constrained?': param_metrics_df['Parameter'].iloc[0] == np.array(cam.labels[0:num_asco_pams]),\n",
    "        'Width Reduction (%)': [param_metrics_df[param_metrics_df['Parameter'] == p]['Width Reduction (%)'].values[0] \n",
    "                               for p in cam.labels[0:num_asco_pams]],\n",
    "        'Coverage at 68% CI': [coverage_df[(coverage_df['Parameter'] == p) & \n",
    "                                         (coverage_df['Confidence Level'] == '68%')]['Actual Coverage'].values[0] \n",
    "                              for p in cam.labels[0:num_asco_pams]],\n",
    "        'Error/Uncertainty Ratio': [param_metrics_df[param_metrics_df['Parameter'] == p]['Error/Uncertainty Ratio'].values[0] \n",
    "                                   for p in cam.labels[0:num_asco_pams]]\n",
    "    })\n",
    "    \n",
    "    summary.to_csv(os.path.join(config_plots_dir, f'parameter_summary_{name}.csv'), index=False)\n",
    "    print(f\"Posterior predictive checks completed. Results saved in: {config_plots_dir}\")\n",
    "    \n",
    "    return summary, coverage_df, param_metrics_df\n",
    "\n",
    "# Execute the analysis\n",
    "summary, coverage_df, param_metrics_df = conduct_posterior_predictive_checks()\n",
    "print(\"\\nParameter Performance Summary:\")\n",
    "print(summary)\n",
    "\n",
    "print(\"\\nBest recovered parameters (ranked):\")\n",
    "print(param_metrics_df[['Parameter', 'Combined Score']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
