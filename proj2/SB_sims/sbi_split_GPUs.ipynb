{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import GPUtil\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import joblib\n",
    "import os \n",
    "import ili\n",
    "from ili.dataloaders import NumpyLoader\n",
    "from ili.inference import InferenceRunner\n",
    "import h5py\n",
    "\n",
    "# Import your custom modules\n",
    "sys.path.append(\"/disk/xray15/aem2/camels/proj2\")\n",
    "from setup_params_1P import plot_uvlf, plot_colour\n",
    "from setup_params_SB import *\n",
    "from priors_SB import initialise_priors_SB28_splitGPU\n",
    "from variables_config_28 import uvlf_limits, n_bins_lf, colour_limits, n_bins_colour\n",
    "colours = False\n",
    "luminosity_functions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def setup_gpus(minimum_memory=2):\n",
    "    \"\"\"Sets up GPUs for training by finding available devices with sufficient memory.\"\"\"\n",
    "    gpu_list = GPUtil.getAvailable(\n",
    "        order=\"memory\",\n",
    "        limit=100,\n",
    "        maxLoad=0.5,\n",
    "        maxMemory=0.5\n",
    "    )\n",
    "    \n",
    "    available_gpus = []\n",
    "    for gpu in gpu_list:\n",
    "        gpu_props = torch.cuda.get_device_properties(gpu)\n",
    "        available_memory = gpu_props.total_memory / 1e9\n",
    "        \n",
    "        if available_memory >= minimum_memory:\n",
    "            available_gpus.append(gpu)\n",
    "            print(f\"GPU {gpu}: {available_memory:.2f} GB memory - Available for training\")\n",
    "    \n",
    "    if not available_gpus:\n",
    "        print(\"No suitable GPUs found - defaulting to CPU\")\n",
    "        return []\n",
    "    \n",
    "    # Set primary GPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(available_gpus[0])\n",
    "    return available_gpus\n",
    "\n",
    "def create_distributed_networks(hidden_features, num_transforms, num_nets, available_gpus, x_sample, theta_sample, primary_device):\n",
    "    \"\"\"Creates neural networks and ensures they're on the correct device.\"\"\"\n",
    "    print(f\"Creating neural network ensemble on device {primary_device}\")\n",
    "    \n",
    "    nets = []\n",
    "    # Create network builder\n",
    "    net_builder = ili.utils.load_nde_sbi(\n",
    "        engine=\"NPE\",\n",
    "        model=\"nsf\",\n",
    "        hidden_features=hidden_features,\n",
    "        num_transforms=num_transforms\n",
    "    )\n",
    "    \n",
    "    # Create all networks on primary device\n",
    "    for i in range(num_nets):\n",
    "        net = net_builder(batch_theta=theta_sample, batch_x=x_sample)\n",
    "        net = net.to(primary_device)\n",
    "        nets.append(net)\n",
    "        print(f\"Created network {i+1}/{num_nets} on {primary_device}\")\n",
    "    \n",
    "    return nets\n",
    "'''\n",
    "def initialize_training(hidden_features, num_transforms, train_args, prior, model_out_dir, name, x_all, theta, train_mask):\n",
    "    \"\"\"Initialize training with proper device handling.\n",
    "    \n",
    "    The prior is a MultipleIndependent distribution that needs special handling\n",
    "    for device placement. Instead of moving the prior, we'll create it on the\n",
    "    right device from the start.\n",
    "    \"\"\"\n",
    "    # Set up GPUs and select primary device\n",
    "    available_gpus = setup_gpus(minimum_memory=2)\n",
    "    primary_device = f'cuda:{available_gpus[0]}' if available_gpus else 'cpu'\n",
    "    print(f\"Using {primary_device} as primary device\")\n",
    "    \n",
    "    # Instead of moving the prior, prepare the data on the correct device\n",
    "    x_sample = x_all[train_mask][:32].to(primary_device)\n",
    "    theta_sample = theta[train_mask][:32].to(primary_device)\n",
    "    \n",
    "    # Create networks on the correct device\n",
    "    nets = create_distributed_networks(\n",
    "        hidden_features=hidden_features,\n",
    "        num_transforms=num_transforms,\n",
    "        num_nets=4,\n",
    "        available_gpus=available_gpus,\n",
    "        x_sample=x_sample,\n",
    "        theta_sample=theta_sample,\n",
    "        primary_device=primary_device\n",
    "    )\n",
    "    \n",
    "    # Initialize runner with the prior and device\n",
    "    runner = InferenceRunner.load(\n",
    "        backend=\"sbi\",\n",
    "        engine=\"NPE\",\n",
    "        prior=prior,  # Use prior as is, don't try to move it\n",
    "        nets=nets,\n",
    "        device=primary_device,\n",
    "        train_args=train_args,\n",
    "        proposal=None,\n",
    "        out_dir=model_out_dir,\n",
    "        name=name\n",
    "    )\n",
    "    \n",
    "    return runner, nets\n",
    "'''\n",
    "\n",
    "def initialize_training(hidden_features, num_transforms, train_args, prior, model_out_dir, name, x_all, theta, train_mask):\n",
    "    \"\"\"Initialize training with proper device handling.\"\"\"\n",
    "    # Set up GPUs and select primary device\n",
    "    available_gpus = setup_gpus(minimum_memory=2)\n",
    "    if available_gpus:\n",
    "        primary_device = f'cuda:{available_gpus[0]}'\n",
    "    else:\n",
    "        primary_device = 'cpu'\n",
    "    print(f\"Using {primary_device} as primary device\")\n",
    "\n",
    "    # Process data on the primary device\n",
    "    x_sample = x_all[train_mask][:32].to(primary_device)\n",
    "    theta_sample = theta[train_mask][:32].to(primary_device)\n",
    "\n",
    "    # Create networks on the correct device\n",
    "    nets = create_distributed_networks(\n",
    "        hidden_features=hidden_features,\n",
    "        num_transforms=num_transforms,\n",
    "        num_nets=4,\n",
    "        available_gpus=available_gpus,\n",
    "        x_sample=x_sample,\n",
    "        theta_sample=theta_sample,\n",
    "        primary_device=primary_device\n",
    "    )\n",
    "\n",
    "    # Initialize runner with the prior and device\n",
    "    runner = InferenceRunner.load(\n",
    "        backend=\"sbi\",\n",
    "        engine=\"NPE\",\n",
    "        prior=prior.to(primary_device),  # Move the prior to the primary device\n",
    "        nets=nets,\n",
    "        device=primary_device,\n",
    "        train_args=train_args,\n",
    "        proposal=None,\n",
    "        out_dir=model_out_dir,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "    return runner, nets\n",
    "\n",
    "def main():\n",
    "    # Basic configuration\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = \"IllustrisTNG\"\n",
    "    spec_type = \"attenuated\"\n",
    "    sps = \"BC03\"\n",
    "    snap = [\"044\"]\n",
    "    bands = \"all\"\n",
    "    \n",
    "    colours = False\n",
    "    luminosity_functions = True\n",
    "    \n",
    "    # Create unique name for this run\n",
    "    name = f\"{model}_{bands}_{sps}_{spec_type}_{n_bins_lf}_{n_bins_colour}\"\n",
    "    \n",
    "    # Initialize CAMELS simulation interface\n",
    "    cam = camels(model=model, sim_set='SB28')\n",
    "    \n",
    "    # Set output directories based on configuration\n",
    "    if colours and not luminosity_functions:\n",
    "        model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/colours_only/\"\n",
    "        plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/colours_only/\"\n",
    "    elif luminosity_functions and not colours:\n",
    "        model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/lf_only/\"\n",
    "        plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/lfs_only/\"\n",
    "    elif colours and luminosity_functions:\n",
    "        model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/colours_lfs/\"\n",
    "        plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/colours_lfs/\"\n",
    "    else:\n",
    "        raise ValueError(\"At least one of colours or luminosity_functions must be True\")\n",
    "    \n",
    "    print(f\"Saving model in {model_out_dir}\")\n",
    "    print(f\"Saving plots in {plots_out_dir}\")\n",
    "    \n",
    "    # Load and process data\n",
    "    df_info = pd.read_csv(\"/disk/xray15/aem2/data/28pams/Info_IllustrisTNG_L25n256_28params.txt\")\n",
    "    theta, x = get_theta_x_SB(luminosity_functions=luminosity_functions, colours=colours)\n",
    "    \n",
    "    # Set up device first\n",
    "    available_gpus = setup_gpus(minimum_memory=2)\n",
    "    primary_device = f'cuda:{available_gpus[0]}' if available_gpus else 'cpu'\n",
    "    print(f\"Using {primary_device} as primary device\")\n",
    "    \n",
    "    # Initialize prior on the primary device\n",
    "    prior = initialise_priors_SB28_splitGPU(\n",
    "        df=df_info, \n",
    "        device=primary_device,  # This will be used consistently\n",
    "        astro=True,\n",
    "        dust=False\n",
    "    )\n",
    "\n",
    "    # Process data\n",
    "    x_all = np.array([np.hstack(_x) for _x in x])\n",
    "    x_all = torch.tensor(x_all, dtype=torch.float32)\n",
    "    theta = torch.tensor(theta, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    # Ensure all data is on the same device\n",
    "    x_all = torch.tensor(np.array([np.hstack(_x) for _x in x]), \n",
    "                        dtype=torch.float32, \n",
    "                        device=primary_device)\n",
    "    theta = torch.tensor(theta, \n",
    "                        dtype=torch.float32, \n",
    "                        device=primary_device)\n",
    "\n",
    "\n",
    "    # Create train/test split\n",
    "    test_mask = create_test_mask()\n",
    "    train_mask = ~test_mask\n",
    "    \n",
    "    # Training parameters\n",
    "    train_args = {\n",
    "        \"training_batch_size\": 128,\n",
    "        \"learning_rate\": 5e-6,\n",
    "        \"stop_after_epochs\": 200,\n",
    "        \"validation_fraction\": 0.15\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Initialize training\n",
    "        runner, nets = initialize_training(\n",
    "            hidden_features=128,\n",
    "            num_transforms=4,\n",
    "            train_args=train_args,\n",
    "            prior=prior,\n",
    "            model_out_dir=model_out_dir,\n",
    "            name=name,\n",
    "            x_all=x_all,\n",
    "            theta=theta,\n",
    "            train_mask=train_mask\n",
    "        )\n",
    "        \n",
    "        # Create data loader\n",
    "        loader = NumpyLoader(\n",
    "            x=x_all[train_mask].clone().detach(),\n",
    "            theta=theta[train_mask].clone().detach()\n",
    "        )\n",
    "        \n",
    "        # Train the model and get results\n",
    "        posterior_ensemble, summaries = runner(loader=loader)\n",
    "        \n",
    "        print(\"Training completed successfully!\")\n",
    "        \n",
    "        # Explicitly return the results\n",
    "        return posterior_ensemble, summaries\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {str(e)}\")\n",
    "        raise  # Re-raise the exception after printing it\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()  # Store results in a single variable first\n",
    "    if results is not None:  # Check if we got valid results\n",
    "        posterior_ensemble, summaries = results  # Then unpack them\n",
    "        print(\"Successfully retrieved training results\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing samples from the entire test set to look at overall performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data\n",
    "x_test = x_all[test_mask]\n",
    "theta_test = theta[test_mask]\n",
    "\n",
    "# Number of samples to draw from posterior\n",
    "n_samples = 1000\n",
    "\n",
    "# Storage for predictions\n",
    "all_samples = []\n",
    "all_means = []\n",
    "all_stds = []\n",
    "\n",
    "# Generate posterior samples for each test point\n",
    "for i in range(len(x_test)):\n",
    "    # Get samples from the posterior\n",
    "    samples = posterior_ensemble.sample(\n",
    "        (n_samples,), \n",
    "        x=x_test[i].reshape(1, -1)\n",
    "    ).cpu().numpy()\n",
    "    \n",
    "    # Calculate mean and std of samples\n",
    "    mean = samples.mean(axis=0)\n",
    "    std = samples.std(axis=0)\n",
    "    \n",
    "    all_samples.append(samples)\n",
    "    all_means.append(mean)\n",
    "    all_stds.append(std)\n",
    "\n",
    "all_samples = np.array(all_samples)\n",
    "all_means = np.array(all_means)\n",
    "all_stds = np.array(all_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = df_pars.columns[1:29].tolist()  # Excluding 'name' column\n",
    "\n",
    "fig, axes = plt.subplots(7, 4, figsize=(16, 28)) \n",
    "axes = axes.flatten()\n",
    "\n",
    "fontsize = 10  \n",
    "\n",
    "plt.rcParams['figure.constrained_layout.use'] = True  \n",
    "\n",
    "# Plot each parameter\n",
    "for i in range(28):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # True vs predicted with error bars\n",
    "    ax.errorbar(\n",
    "        theta_test[:, i].cpu().numpy(),\n",
    "        all_means[:, i],\n",
    "        yerr=all_stds[:, i],\n",
    "        fmt='.',\n",
    "        color='k',\n",
    "        ecolor='blue',\n",
    "        capsize=0,\n",
    "        elinewidth=0.8,  \n",
    "        alpha=0.3,       \n",
    "        markersize=5    \n",
    "    )\n",
    "    \n",
    "    # Add true line\n",
    "    lims = [\n",
    "        min(ax.get_xlim()[0], ax.get_ylim()[0]),\n",
    "        max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "    ]\n",
    "    ax.plot(lims, lims, '--', color='black', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    # get metrics\n",
    "    rmse = np.sqrt(np.mean((theta_test[:, i].cpu().numpy() - all_means[:, i])**2))\n",
    "    r2 = np.corrcoef(theta_test[:, i].cpu().numpy(), all_means[:, i])[0, 1]**2\n",
    "    chi2 = np.mean(((theta_test[:, i].cpu().numpy() - all_means[:, i])**2) / (all_stds[:, i]**2))\n",
    "    \n",
    "    # add metrics box in top left corner\n",
    "    stats_text = f'RMSE = {rmse:.2f}\\n' + \\\n",
    "                 f'R² = {r2:.2f}\\n' + \\\n",
    "                 f'χ² = {chi2:.2f}'\n",
    "    ax.text(0.05, 0.95, stats_text,\n",
    "            transform=ax.transAxes,\n",
    "            bbox=dict(facecolor='white', alpha=0.8),\n",
    "            verticalalignment='top',\n",
    "            fontsize=fontsize-1)  # Slightly smaller font for stats\n",
    "    \n",
    "    # title: parameter name\n",
    "    ax.set_title(param_names[i], fontsize=fontsize, pad=5)  # Reduced padding\n",
    "    \n",
    "    # axis labels\n",
    "    ax.set_xlabel('True', fontsize=fontsize-1)\n",
    "    ax.set_ylabel('Inferred', fontsize=fontsize-1)\n",
    "    \n",
    "    # tick labels\n",
    "    ax.tick_params(axis='both', which='major', labelsize=fontsize-2)\n",
    "    \n",
    "    # internal padding\n",
    "    ax.margins(x=0.05, y=0.05)\n",
    "\n",
    "# subplot spacing\n",
    "plt.subplots_adjust(\n",
    "    left=0.01,    # Less space on left\n",
    "    right=0.7,   # Less space on right\n",
    "    bottom=0.05,  # Less space at bottom\n",
    "    top=0.7,     # Less space at top\n",
    "    wspace=0.2,   # Less space between plots horizontally\n",
    "    hspace=0.2    # Less space between plots vertically\n",
    ")\n",
    "\n",
    "\n",
    "# Save figure with detailed filename\n",
    "save_path = f'{plots_out_dir}/posterior_predictions_{model_params}.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "print(save_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_str = (f\"batch{train_args['training_batch_size']}_\"\n",
    "             f\"lr{train_args['learning_rate']}_\"\n",
    "             f\"epochs{train_args['stop_after_epochs']}_\"\n",
    "             f\"h{hidden_features}_t{num_transforms}\")\n",
    "\n",
    "# coverage plots\n",
    "metric = PosteriorCoverage(\n",
    "    num_samples=int(4e3),\n",
    "    sample_method='direct',\n",
    "    labels=cam.labels,\n",
    "    plot_list=[\"tarp\"], # \"coverage\", \"histogram\", \"predictions\", \n",
    "    out_dir=plots_out_dir,\n",
    ")\n",
    "\n",
    "# Generate plots\n",
    "figs = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x=x_all[test_mask].cpu(),\n",
    "    theta=theta[test_mask, :].cpu(),\n",
    "    signature=f\"coverage_{name}_{config_str}_\"  # Add config to filename\n",
    ")\n",
    "\n",
    "config_text = (\n",
    "    f\"Training Config:\\n\"\n",
    "    f\"Batch Size: {train_args['training_batch_size']}\\n\"\n",
    "    f\"Learning Rate: {train_args['learning_rate']}\\n\"\n",
    "    f\"Epochs: {train_args['stop_after_epochs']}\\n\"\n",
    "    f\"Hidden Features: {hidden_features}\\n\"\n",
    "    f\"Num Transforms: {num_transforms}\"\n",
    ")\n",
    "\n",
    "# Process each figure\n",
    "for i, fig in enumerate(figs):\n",
    "    plt.figure(fig.number)  # Activate the figure\n",
    "    plt.figtext(0.02, 0.98, config_text,\n",
    "                fontsize=8,\n",
    "                bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),\n",
    "                verticalalignment='top')\n",
    "    \n",
    "    # Save each figure with type indicator\n",
    "    plot_types = [\"tarp\"] #\"coverage\", \"histogram\", \"predictions\",\n",
    "    plt.savefig(os.path.join(plots_out_dir, \n",
    "                f'metric_{plot_types[i]}_{name}_{config_str}.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at a specific case (one observation randomly set with seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, SBIRunner returns a custom class instance to be able to pass signature strings\n",
    "# 1. prints our info on model configuration and architecture\n",
    "print(posterior_ensemble.signatures)\n",
    "\n",
    "\n",
    "# 2. choose a random input for training\n",
    "seed_in = 49\n",
    "np.random.seed(seed_in) # set seed for reproducability\n",
    "ind = np.random.randint(len(x_train[0])) # choose observation (random index from training data)\n",
    "\n",
    "# 3. generate posterior samples\n",
    "seed_samp = 32\n",
    "torch.manual_seed(seed_samp)# set seed for reproducability\n",
    "# then, for the chosen training sample (as chosen above in 2.)\n",
    "# generate 1000 samples from the posterior distribution using accept/reject sampling\n",
    "samples = posterior_ensemble.sample(\n",
    "    (1000,), \n",
    "    torch.Tensor(x_train[0][ind]).to(device))\n",
    "\n",
    "# 4. calculate the probability densities for each sample\n",
    "# i.e for each generated sample, calculate how likely it is using learned posterior distribution\n",
    "log_prob = posterior_ensemble.log_prob(\n",
    "    samples, # the generated samples from 3.\n",
    "    torch.Tensor(x_train[0][ind]).to(device) # the chosen observation from 2.\n",
    "    )\n",
    "\n",
    "# convert to numpy so can read easier.\n",
    "samples = samples.cpu().numpy()\n",
    "log_prob = log_prob.cpu().numpy()\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "def plot_posterior_samples_grid(samples, log_prob, param_names, df_info, model_name, train_args):\n",
    "   n_params = len(param_names)\n",
    "   n_cols = 4\n",
    "   n_rows = (n_params + n_cols - 1) // n_cols\n",
    "   \n",
    "   fig = plt.figure(figsize=(20, 5*n_rows))\n",
    "   \n",
    "   # Add main title\n",
    "   fig.suptitle('Posterior Probability Distributions', fontsize=16, y=0.98)\n",
    "   \n",
    "   gs = GridSpec(n_rows, n_cols, figure=fig)\n",
    "   \n",
    "   # Model info text\n",
    "   model_info = (\n",
    "       f\"Model Config:\\n\"\n",
    "       f\"Name: {model_name}\\n\"\n",
    "       f\"Hidden Features: {hidden_features}\\n\"\n",
    "       f\"Num Transforms: {num_transforms}\\n\"\n",
    "       f\"\\nTraining Args:\\n\"\n",
    "       f\"Batch Size: {train_args['training_batch_size']}\\n\"\n",
    "       f\"Learning Rate: {train_args['learning_rate']}\\n\"\n",
    "       f\"Stop After Epochs: {train_args['stop_after_epochs']}\"\n",
    "   )\n",
    "   \n",
    "   fig.text(0.02, 0.96, model_info, \n",
    "            fontsize=8,\n",
    "            bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),\n",
    "            verticalalignment='top')\n",
    "   \n",
    "   for i, name in enumerate(param_names):\n",
    "       row = i // n_cols\n",
    "       col = i % n_cols\n",
    "       \n",
    "       ax = fig.add_subplot(gs[row, col])\n",
    "       data = samples[:, i]\n",
    "       param_info = df_info[df_info['ParamName'] == name].iloc[0]\n",
    "       is_log = param_info['LogFlag'] == 1\n",
    "       \n",
    "       if is_log:\n",
    "           ax.hist(data, bins=50, density=True, alpha=0.6)\n",
    "           ax.set_xscale('log')\n",
    "           log_data = np.log10(data)\n",
    "           mean = np.mean(log_data)\n",
    "           std = np.std(log_data)\n",
    "           stats_text = f'Log10 Mean: {mean:.3f}\\nLog10 Std: {std:.3f}'\n",
    "           ax.set_xlabel('Parameter Value (log scale)', fontsize=8)\n",
    "       else:\n",
    "           ax.hist(data, bins=50, density=True, alpha=0.6)\n",
    "           mean = np.mean(data)\n",
    "           std = np.std(data)\n",
    "           stats_text = f'Mean: {mean:.3f}\\nStd: {std:.3f}'\n",
    "           ax.set_xlabel('Parameter Value', fontsize=8)\n",
    "       \n",
    "       ax.set_ylabel('Density', fontsize=8)\n",
    "       \n",
    "       ax.axvline(param_info['MinVal'], color='g', linestyle=':', alpha=0.5, label='Min')\n",
    "       ax.axvline(param_info['MaxVal'], color='g', linestyle=':', alpha=0.5, label='Max')\n",
    "       ax.axvline(param_info['FiducialVal'], color='r', linestyle='--', alpha=0.5, label='Fiducial')\n",
    "       \n",
    "       ax.text(0.02, 0.95, stats_text, transform=ax.transAxes, \n",
    "               verticalalignment='top', fontsize=8, \n",
    "               bbox=dict(facecolor='white', alpha=0.8))\n",
    "       \n",
    "       ax.set_title(f\"{name}\\n{param_info['Description']}\", fontsize=8, pad=5)\n",
    "       ax.tick_params(labelsize=8)\n",
    "       \n",
    "       if i == 0:\n",
    "           ax.legend(loc='upper right', fontsize=8)\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.subplots_adjust(top=0.93)  # Adjusted to make room for main title\n",
    "   return fig\n",
    "\n",
    "# Get all parameter names from df_info\n",
    "param_names = df_info['ParamName'].tolist()\n",
    "\n",
    "# Now try plotting again with the correct parameter names\n",
    "fig = plot_posterior_samples_grid(\n",
    "    samples, \n",
    "    log_prob, \n",
    "    param_names,  # Now contains all 28 parameter names correctly\n",
    "    df_info,\n",
    "    model_name=name,\n",
    "    train_args=train_args\n",
    ")\n",
    "\n",
    "# Save with model config in filename\n",
    "save_name = (f'parameter_posteriors_grid_{name}_'\n",
    "            f'h{hidden_features}_t{num_transforms}_'\n",
    "            f'b{train_args[\"training_batch_size\"]}_'\n",
    "            f'e{train_args[\"stop_after_epochs\"]}.png')\n",
    "\n",
    "os.makedirs(plots_out_dir, exist_ok=True)\n",
    "plt.savefig(os.path.join(plots_out_dir, save_name), \n",
    "            dpi=300, \n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
