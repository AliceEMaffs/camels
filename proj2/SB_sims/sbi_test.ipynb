{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Concepts\n",
    "'''\n",
    "1. Forward problem: θ → x (simulation)\n",
    "2. Inverse problem: x → θ (what we're solving)\n",
    "3. Posterior: P(θ|x) probability distribution over parameters\n",
    "4. Prior: Initial assumptions about parameter ranges\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# seaparate into train and test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "#from torch.distributions import Uniform, ExpTransform, TransformedDistribution #, AffineTransform\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import joblib\n",
    "import os \n",
    "import ili\n",
    "from ili.dataloaders import NumpyLoader\n",
    "from ili.inference import InferenceRunner\n",
    "from ili.validation.metrics import PosteriorCoverage#, PlotSinglePosterior\n",
    "\n",
    "from sbi.utils.user_input_checks import process_prior\n",
    "\n",
    "sys.path.append(\"/disk/xray15/aem2/camels/proj2\")\n",
    "from setup_params_1P import plot_uvlf, plot_colour\n",
    "from setup_params_SB import *\n",
    "from priors_SB import initialise_priors_SB28\n",
    "\n",
    "from variables_config_28 import uvlf_limits, n_bins_lf, colour_limits, n_bins_colour\n",
    "# parameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = \"IllustrisTNG\"\n",
    "spec_type = \"attenuated\"\n",
    "sps = \"BC03\"\n",
    "snap = [\"044\"]\n",
    "bands = \"all\" # or just GALEX?\n",
    "\n",
    "# lets try UVLF and colours this time.\n",
    "colours = False  \n",
    "luminosity_functions = True\n",
    "name = f\"{model}_{bands}_{sps}_{spec_type}_{n_bins_lf}_{n_bins_colour}\"\n",
    "\n",
    "# initialize CAMELS and load parameter info using camels.py\n",
    "cam = camels(model=model, sim_set='SB28')\n",
    "\n",
    "if colours and not luminosity_functions:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/colours_only/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/colours_only/\"\n",
    "    \n",
    "elif luminosity_functions and not colours:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/lf_only/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/lfs_only/\"\n",
    "\n",
    "elif colours and luminosity_functions:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/colours_lfs/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/colours_lfs/\"\n",
    "\n",
    "# You might want to add an else for safety:\n",
    "else:\n",
    "    raise ValueError(\"At least one of colours or luminosity_functions must be True\")\n",
    "\n",
    "print(\"Saving model in \", model_out_dir)\n",
    "print(\"Saving plots in \", plots_out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter info file (df_info) is used for defining priors\n",
    "# the actual parameter values come from the camels class which reads CosmoAstroSeed_IllustrisTNG_L25n256_SB28.txt\n",
    "\n",
    "#  parameters defined here: /disk/xray15/aem2/data/28pams/IllustrisTNG/SB/CosmoAstroSeed_IllustrisTNG_L25n256_SB28.txt which is used for theta\n",
    "df_pars = pd.read_csv('/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/CosmoAstroSeed_IllustrisTNG_L25n256_SB28.txt', delim_whitespace=True)\n",
    "print(df_pars)\n",
    "\n",
    "\n",
    "# prior values come from this:\n",
    "df_info = pd.read_csv(\"/disk/xray15/aem2/data/28pams/Info_IllustrisTNG_L25n256_28params.txt\")\n",
    "print(df_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = df_pars.iloc[:, 1:29].to_numpy()  # excluding 'name' column and 'seed' column\n",
    "\n",
    "print(theta)\n",
    "print(theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column names:\")\n",
    "print(df_pars.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first one (omega0) to see shape of prior:\n",
    "plt.hist(theta[:, 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    theta, x = get_theta_x_SB()\n",
    "    print(theta.shape, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colours:\n",
    "    fig = plot_colour(x)\n",
    "    plt.savefig('/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/colours_test/colour_check.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if luminosity_functions:\n",
    "    fig = plot_uvlf(x)\n",
    "    plt.savefig('/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/LFs_test/uvlf_check.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the priors and data\n",
    "prior = initialise_priors_SB28(\n",
    "    df=df_info, \n",
    "    device=device,\n",
    "    astro=True,\n",
    "    dust=False  # no dust for testing. set to False to only get the 28 model parameters.\n",
    "    # with dust = True, prior has 32 dimensions (28 parameters + 4 dust parameters) \n",
    ")\n",
    "\n",
    "# process the data\n",
    "x_all = np.array([np.hstack(_x) for _x in x])\n",
    "x_all = torch.tensor(x_all, dtype=torch.float32, device=device)\n",
    "\n",
    "print(\"Theta shape:\", theta.shape)\n",
    "print(\"X shape:\", x_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Move data to GPU as early as possible\n",
    "x_all = x_all.to(device)\n",
    "x_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.tensor(theta, dtype=torch.float32, device=device)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NaN values and normalize while on GPU\n",
    "x_all_cpu = x_all.cpu().numpy()  # Only move to CPU when necessary for sklearn\n",
    "x_all_cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data shape before processing:\", x_all_cpu.shape)\n",
    "print(\"Number of values:\",(x_all_cpu).sum())\n",
    "print(\"Number of NaN values:\", np.isnan(x_all_cpu).sum())\n",
    "print(\"Number of infinite values:\", np.isinf(x_all_cpu).sum())\n",
    "\n",
    "# how many nan values are there? if they are all nan something has gone horribly wrong.\n",
    "# this looks better - 18th Nov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get rid of NaN/inf values, replace with small random noise\n",
    "nan_mask = np.isnan(x_all_cpu) | np.isinf(x_all_cpu)\n",
    "nan_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if nan_mask.any():\n",
    "    x_all_cpu[nan_mask] = np.random.rand(np.sum(nan_mask)) * 1e-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data shape before processing:\", x_all_cpu.shape)\n",
    "print(\"Number of NaN values:\", np.isnan(x_all_cpu).sum())\n",
    "print(\"Number of infinite values:\", np.isinf(x_all_cpu).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize\n",
    "'''\n",
    "With normalization:\n",
    "- All values brought to similar scale\n",
    "- Neural network can learn more effectively\n",
    "- No single bin dominates the learning\n",
    "'''\n",
    "\n",
    "norm = Normalizer()\n",
    "\n",
    "# Option: Add small constant before normalizing\n",
    "epsilon = 1e-10  # Small constant\n",
    "x_all_shifted = x_all_cpu + epsilon\n",
    "x_all_normalized = norm.fit_transform(x_all_shifted)\n",
    "x_all = torch.tensor(x_all_normalized, dtype=torch.float32, device=device)\n",
    "\n",
    "'''\n",
    "x_all_normalized = norm.fit_transform(x_all_cpu)\n",
    "x_all = torch.tensor(x_all_normalized, dtype=torch.float32, device=device)\n",
    "'''\n",
    "x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some diagnostics for the normalized data\n",
    "def analyze_normalization(x_all):\n",
    "    \"\"\"Analyze the normalized data distribution\"\"\"\n",
    "    x_numpy = x_all.cpu().numpy()\n",
    "    \n",
    "    print(\"Normalization Statistics:\")\n",
    "    print(f\"Mean: {np.mean(x_numpy):.6f}\")\n",
    "    print(f\"Std: {np.std(x_numpy):.6f}\")\n",
    "    print(f\"Min: {np.min(x_numpy):.6f}\")\n",
    "    print(f\"Max: {np.max(x_numpy):.6f}\")\n",
    "    print(f\"Zero elements: {np.sum(x_numpy == 0)} out of {x_numpy.size}\")\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(x_numpy.flatten(), bins=50, density=True)\n",
    "    plt.title('Distribution of Normalized Values')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(plots_out_dir, 'normalization_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "analyze_normalization(x_all)\n",
    "\n",
    "# zero elements might refer to:\n",
    "# empty magnitude bins (no galaxies in that magnitude range)\n",
    "# below detection limit regions\n",
    "# is actually normal for UVLFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save normalizer\n",
    "joblib.dump(norm, f'/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/{name}_scaler.save')\n",
    "\n",
    "# Print final check\n",
    "print(\"Any NaN in normalized data:\", torch.isnan(x_all).any().item())\n",
    "print(\"Any inf in normalized data:\", torch.isinf(x_all).any().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make test mask\n",
    "test_mask = create_test_mask() # 10% testing\n",
    "test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = ~test_mask # 90% for training\n",
    "train_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NPE: train a neural network to learn the mapping between the observed data and the posterior distribution of the parameters\n",
    "# Use simulation-based approaches (like the CAMELS simulations you mentioned) to generate many realizations of the observed data and the corresponding model parameters.\n",
    "# Train a neural network to take the observed data as input and output the parameters of the posterior distribution (e.g. mean, variance) for those parameters.\n",
    "# Once the neural network is trained, you can apply it to the actual observed data to obtain estimates of the posterior distributions of the model parameters.\n",
    "\n",
    "# Training arguments\n",
    "train_args = {\n",
    "    \"training_batch_size\": 64, # changed from 4 to 10 as dealing with more sims, want it to be faster for initial testing.\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"stop_after_epochs\": 5, # loss function. waits to see if things improve.\n",
    "    \"validation_fraction\": 0.1,  # creates another split within the training data for validation\n",
    "}\n",
    "\n",
    "# Configure network \n",
    "hidden_features = 100\n",
    "num_transforms = 8\n",
    "\n",
    "net = ili.utils.load_nde_sbi(\n",
    "    engine=\"NPE\",                       # Neural Posterior Estimation\n",
    "    model=\"nsf\",                        # Neural Spline Flow\n",
    "    hidden_features=hidden_features,    # Network width\n",
    "    num_transforms=num_transforms,      # Network depth\n",
    "    # Remove device parameter as it's not allowed\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Data loader\n",
    "loader = NumpyLoader(\n",
    "\n",
    "    # x = x_all[train_mask]\n",
    "    # theta=theta[train_mask]\n",
    "    # clone - makes new memory allocation for this version of x/theta\n",
    "    # detach - doesnt affect computations on theta that were done previously (not to mess with test/train versions)\n",
    "    x=x_all[train_mask].clone().detach(),\n",
    "    theta=theta[train_mask].clone().detach()\n",
    ")\n",
    "\n",
    "\n",
    "# Runner setup with device specified here\n",
    "runner = InferenceRunner.load(\n",
    "    backend=\"sbi\",\n",
    "    engine=\"NPE\",\n",
    "    prior=prior,\n",
    "    nets=[net], # nets\n",
    "    device=device,  # Device specified in runner, not network\n",
    "    train_args=train_args,\n",
    "    proposal=None,\n",
    "    out_dir=model_out_dir,\n",
    "    name=name\n",
    ")\n",
    "\n",
    "# Run training - 'learn the likelihood'\n",
    "# this is training the neural network which will act like our likelihood!\n",
    "posterior_ensemble, summaries = runner(loader=loader)\n",
    "\n",
    "# process of training:\n",
    "'''\n",
    "- the neural network learns P(θ|x): probability of parameters given observations\n",
    "- uses training data to learn mapping from x → θ\n",
    "- then we validate on held-out portion of training data\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add training analysis\n",
    "def analyze_training_progress(summaries):\n",
    "    \"\"\"Analyze and print training progress statistics\"\"\"\n",
    "    train_losses = summaries[0]['training_log_probs']\n",
    "    val_losses = summaries[0]['validation_log_probs']\n",
    "    \n",
    "    print(\"\\nTraining Progress Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    for epoch in range(len(train_losses)):\n",
    "        train_loss = train_losses[epoch]\n",
    "        val_loss = val_losses[epoch]\n",
    "        gap = train_loss - val_loss\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, \"\n",
    "              f\"Val Loss = {val_loss:.4f}, Gap = {gap:.4f}\")\n",
    "\n",
    "# Analyze the training\n",
    "analyze_training_progress(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_diagnostics(summaries):\n",
    "    \"\"\"Plot training diagnostics without empty subplots\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))  # Changed to 1 row, 2 columns\n",
    "    \n",
    "    # Loss curves\n",
    "    train_losses = summaries[0]['training_log_probs']\n",
    "    val_losses = summaries[0]['validation_log_probs']\n",
    "    epochs = range(len(train_losses))\n",
    "    \n",
    "    ax1.plot(epochs, train_losses, '-', label='Training', color='blue')\n",
    "    ax1.plot(epochs, val_losses, '--', label='Validation', color='red')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Log probability')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overfitting gap\n",
    "    gap = np.array(train_losses) - np.array(val_losses)\n",
    "    ax2.plot(epochs, gap, '-', color='purple')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss difference')\n",
    "    ax2.set_title('Overfitting Gap')\n",
    "    ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Use the function\n",
    "fig = plot_training_diagnostics(summaries)\n",
    "plt.savefig(os.path.join(plots_out_dir, f'training_analysis_{name}.png'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train/validation loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,4))\n",
    "c = list(mcolors.TABLEAU_COLORS)\n",
    "for i, m in enumerate(summaries):\n",
    "    ax.plot(m['training_log_probs'], ls='-', label=f\"{i}_train\", c=c[i])\n",
    "    ax.plot(m['validation_log_probs'], ls='--', label=f\"{i}_val\", c=c[i])\n",
    "ax.set_xlim(0)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Log probability')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will this work or do we have to use it explicitly?\n",
    "x_train=x_all[train_mask].clone().detach(),\n",
    "theta_train=theta[train_mask].clone().detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, SBIRunner returns a custom class instance to be able to pass signature strings\n",
    "# 1. prints our info on model configuration and architecture\n",
    "print(posterior_ensemble.signatures)\n",
    "\n",
    "\n",
    "# 2. choose a random input for training\n",
    "seed_in = 49\n",
    "np.random.seed(seed_in) # set seed for reproducability\n",
    "ind = np.random.randint(len(x_train[0])) # choose observation (random index from training data)\n",
    "\n",
    "# 3. generate posterior samples\n",
    "seed_samp = 32\n",
    "torch.manual_seed(seed_samp)# set seed for reproducability\n",
    "# then, for the chosen training sample (as chosen above in 2.)\n",
    "# generate 1000 samples from the posterior distribution using accept/reject sampling\n",
    "samples = posterior_ensemble.sample(\n",
    "    (1000,), \n",
    "    torch.Tensor(x_train[0][ind]).to(device))\n",
    "\n",
    "# 4. calculate the probability densities for each sample\n",
    "# i.e for each generated sample, calculate how likely it is using learned posterior distribution\n",
    "log_prob = posterior_ensemble.log_prob(\n",
    "    samples, # the generated samples from 3.\n",
    "    torch.Tensor(x_train[0][ind]).to(device) # the chosen observation from 2.\n",
    "    )\n",
    "\n",
    "# convert to numpy so can read easier.\n",
    "samples = samples.cpu().numpy()\n",
    "log_prob = log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get parameter names from DataFrame columns, excluding 'name' and 'seed'\n",
    "param_names = df_pars.columns[1:5].tolist() # first 5 to test\n",
    "\n",
    "def plot_posterior_samples(samples, log_prob, param_names):\n",
    "    \"\"\"\n",
    "    Plot the posterior distributions for each parameter\n",
    "    \"\"\"\n",
    "    n_params = len(param_names)\n",
    "    fig, axes = plt.subplots(n_params, 1, figsize=(10, 4*n_params))\n",
    "    \n",
    "    for i, (ax, name) in enumerate(zip(axes, param_names)):\n",
    "        # Plot histogram of samples\n",
    "        ax.hist(samples[:, i], bins=50, density=True, alpha=0.6)\n",
    "        ax.set_xlabel(name)\n",
    "        ax.set_ylabel('Density')\n",
    "        \n",
    "        # Add mean and std\n",
    "        mean = samples[:, i].mean()\n",
    "        std = samples[:, i].std()\n",
    "        ax.axvline(mean, color='r', linestyle='--')\n",
    "        ax.text(0.02, 0.95, f'Mean: {mean:.3f}\\nStd: {std:.3f}', \n",
    "                transform=ax.transAxes, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Print parameter names to verify\n",
    "print(\"Parameter names:\", param_names)\n",
    "\n",
    "# Create the plot\n",
    "fig = plot_posterior_samples(samples, log_prob, param_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adding log flag to better interpret results:\n",
    "def plot_posterior_samples(samples, log_prob, param_names, df_info):\n",
    "    \"\"\"\n",
    "    Plot the posterior distributions accounting for LogFlag from df_info\n",
    "    \"\"\"\n",
    "    n_params = len(param_names)\n",
    "    fig, axes = plt.subplots(n_params, 1, figsize=(10, 4*n_params))\n",
    "    \n",
    "    for i, (ax, name) in enumerate(zip(axes, param_names)):\n",
    "        data = samples[:, i]\n",
    "        param_info = df_info[df_info['ParamName'] == name].iloc[0]\n",
    "        is_log = param_info['LogFlag'] == 1\n",
    "        \n",
    "        if is_log:\n",
    "            ax.hist(data, bins=50, density=True, alpha=0.6)\n",
    "            ax.set_xscale('log')\n",
    "            log_data = np.log10(data)\n",
    "            mean = np.mean(log_data)\n",
    "            std = np.std(log_data)\n",
    "            stats_text = f'Log10 Mean: {mean:.3f}\\nLog10 Std: {std:.3f}'\n",
    "            \n",
    "            ax.axvline(param_info['MinVal'], color='g', linestyle=':', alpha=0.5, label='Min')\n",
    "            ax.axvline(param_info['MaxVal'], color='g', linestyle=':', alpha=0.5, label='Max')\n",
    "            ax.axvline(param_info['FiducialVal'], color='r', linestyle='--', alpha=0.5, label='Fiducial')\n",
    "            \n",
    "        else:\n",
    "            ax.hist(data, bins=50, density=True, alpha=0.6)\n",
    "            mean = np.mean(data)\n",
    "            std = np.std(data)\n",
    "            stats_text = f'Mean: {mean:.3f}\\nStd: {std:.3f}'\n",
    "            \n",
    "            ax.axvline(param_info['MinVal'], color='g', linestyle=':', alpha=0.5, label='Min')\n",
    "            ax.axvline(param_info['MaxVal'], color='g', linestyle=':', alpha=0.5, label='Max')\n",
    "            ax.axvline(param_info['FiducialVal'], color='r', linestyle='--', alpha=0.5, label='Fiducial')\n",
    "        \n",
    "        # Add statistics text in top left\n",
    "        ax.text(0.02, 0.95, stats_text, transform=ax.transAxes, \n",
    "                verticalalignment='top', bbox=dict(facecolor='white', alpha=0.8))\n",
    "        \n",
    "        ax.set_xlabel(f\"{name}\\n{param_info['Description']}\")\n",
    "        ax.set_ylabel('Density')\n",
    "        \n",
    "        # Place legend in top right\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(0.98, 0.98))\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Create the plot using df_info\n",
    "fig = plot_posterior_samples(samples, log_prob, param_names, df_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior_samples_grid(samples, log_prob, param_names, df_info):\n",
    "    \"\"\"\n",
    "    Plot the posterior distributions in a grid layout\n",
    "    \"\"\"\n",
    "    n_params = len(param_names)\n",
    "    n_cols = 4  # 4 columns for 28 parameters\n",
    "    n_rows = (n_params + n_cols - 1) // n_cols  # Ceiling division\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (ax, name) in enumerate(zip(axes, param_names)):\n",
    "        data = samples[:, i]\n",
    "        param_info = df_info[df_info['ParamName'] == name].iloc[0]\n",
    "        is_log = param_info['LogFlag'] == 1\n",
    "        \n",
    "        if is_log:\n",
    "            ax.hist(data, bins=50, density=True, alpha=0.6)\n",
    "            ax.set_xscale('log')\n",
    "            log_data = np.log10(data)\n",
    "            mean = np.mean(log_data)\n",
    "            std = np.std(log_data)\n",
    "            stats_text = f'Log10 Mean: {mean:.3f}\\nLog10 Std: {std:.3f}'\n",
    "        else:\n",
    "            ax.hist(data, bins=50, density=True, alpha=0.6)\n",
    "            mean = np.mean(data)\n",
    "            std = np.std(data)\n",
    "            stats_text = f'Mean: {mean:.3f}\\nStd: {std:.3f}'\n",
    "        \n",
    "        # Add parameter limits\n",
    "        ax.axvline(param_info['MinVal'], color='g', linestyle=':', alpha=0.5, label='Min')\n",
    "        ax.axvline(param_info['MaxVal'], color='g', linestyle=':', alpha=0.5, label='Max')\n",
    "        ax.axvline(param_info['FiducialVal'], color='r', linestyle='--', alpha=0.5, label='Fiducial')\n",
    "        \n",
    "        # Add statistics text in top left\n",
    "        ax.text(0.02, 0.95, stats_text, transform=ax.transAxes, \n",
    "                verticalalignment='top', fontsize=8, bbox=dict(facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Make title from parameter name and description\n",
    "        ax.set_title(f\"{name}\\n{param_info['Description']}\", fontsize=8, pad=5)\n",
    "        ax.tick_params(labelsize=8)\n",
    "        \n",
    "        # Only show legend for first plot\n",
    "        if i == 0:\n",
    "            ax.legend(loc='upper right', bbox_to_anchor=(0.98, 0.98), fontsize=8)\n",
    "    \n",
    "    # Remove any empty subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Get all parameter names\n",
    "param_names = df_info['ParamName'].tolist()\n",
    "\n",
    "# Create the grid plot\n",
    "fig = plot_posterior_samples_grid(samples, log_prob, param_names, df_info)\n",
    "plt.savefig(os.path.join(plots_out_dir, f'parameter_posteriors_grid_{name}.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "def plot_posterior_samples_grid(samples, log_prob, param_names, df_info, model_name, train_args):\n",
    "    \"\"\"\n",
    "    Plot the posterior distributions in a grid layout with model info\n",
    "    \"\"\"\n",
    "    n_params = len(param_names)\n",
    "    n_cols = 4\n",
    "    n_rows = (n_params + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(20, 5*n_rows))\n",
    "    \n",
    "    # Create GridSpec with extra space at top for info\n",
    "    gs = GridSpec(n_rows, n_cols, figure=fig)\n",
    "    \n",
    "    # Add model info text to figure (not in grid)\n",
    "    model_info = (\n",
    "        f\"Model Config:\\n\"\n",
    "        f\"Name: {model_name}\\n\"\n",
    "        f\"Hidden Features: {hidden_features}\\n\"\n",
    "        f\"Num Transforms: {num_transforms}\\n\"\n",
    "        f\"\\nTraining Args:\\n\"\n",
    "        f\"Batch Size: {train_args['training_batch_size']}\\n\"\n",
    "        f\"Learning Rate: {train_args['learning_rate']}\\n\"\n",
    "        f\"Stop After Epochs: {train_args['stop_after_epochs']}\"\n",
    "    )\n",
    "    \n",
    "    fig.text(0.02, 0.98, model_info, \n",
    "             fontsize=8,\n",
    "             bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),\n",
    "             verticalalignment='top')\n",
    "    \n",
    "    # Plot parameters\n",
    "    for i, name in enumerate(param_names):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        \n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "        data = samples[:, i]\n",
    "        param_info = df_info[df_info['ParamName'] == name].iloc[0]\n",
    "        is_log = param_info['LogFlag'] == 1\n",
    "        \n",
    "        if is_log:\n",
    "            ax.hist(data, bins=50, density=True, alpha=0.6)\n",
    "            ax.set_xscale('log')\n",
    "            log_data = np.log10(data)\n",
    "            mean = np.mean(log_data)\n",
    "            std = np.std(log_data)\n",
    "            stats_text = f'Log10 Mean: {mean:.3f}\\nLog10 Std: {std:.3f}'\n",
    "        else:\n",
    "            ax.hist(data, bins=50, density=True, alpha=0.6)\n",
    "            mean = np.mean(data)\n",
    "            std = np.std(data)\n",
    "            stats_text = f'Mean: {mean:.3f}\\nStd: {std:.3f}'\n",
    "        \n",
    "        # Add parameter limits\n",
    "        ax.axvline(param_info['MinVal'], color='g', linestyle=':', alpha=0.5, label='Min')\n",
    "        ax.axvline(param_info['MaxVal'], color='g', linestyle=':', alpha=0.5, label='Max')\n",
    "        ax.axvline(param_info['FiducialVal'], color='r', linestyle='--', alpha=0.5, label='Fiducial')\n",
    "        \n",
    "        ax.text(0.02, 0.95, stats_text, transform=ax.transAxes, \n",
    "                verticalalignment='top', fontsize=8, \n",
    "                bbox=dict(facecolor='white', alpha=0.8))\n",
    "        \n",
    "        ax.set_title(f\"{name}\\n{param_info['Description']}\", fontsize=8, pad=5)\n",
    "        ax.tick_params(labelsize=8)\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.legend(loc='upper right', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Adjust layout to make room for info text\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    return fig\n",
    "\n",
    "# Get all parameter names from df_info\n",
    "param_names = df_info['ParamName'].tolist()\n",
    "\n",
    "# Now try plotting again with the correct parameter names\n",
    "fig = plot_posterior_samples_grid(\n",
    "    samples, \n",
    "    log_prob, \n",
    "    param_names,  # Now contains all 28 parameter names correctly\n",
    "    df_info,\n",
    "    model_name=name,\n",
    "    train_args=train_args\n",
    ")\n",
    "\n",
    "# Save with model config in filename\n",
    "save_name = (f'parameter_posteriors_grid_{name}_'\n",
    "            f'h{hidden_features}_t{num_transforms}_'\n",
    "            f'b{train_args[\"training_batch_size\"]}_'\n",
    "            f'e{train_args[\"stop_after_epochs\"]}.png')\n",
    "\n",
    "os.makedirs(plots_out_dir, exist_ok=True)\n",
    "plt.savefig(os.path.join(plots_out_dir, save_name), \n",
    "            dpi=300, \n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Coverage plots for each model\n",
    "\"\"\"\n",
    "metric = PosteriorCoverage(\n",
    "    num_samples=int(4e3),\n",
    "    sample_method='direct',\n",
    "    # sample_method=\"slice_np_vectorized\",\n",
    "    # sample_params={'num_chains': 1},\n",
    "    # sample_method=\"vi\",\n",
    "    # sample_params={\"dist\": \"maf\", \"n_particles\": 32, \"learning_rate\": 1e-2},\n",
    "    labels=cam.labels,\n",
    "    plot_list=[\"coverage\", \"histogram\", \"predictions\", \"tarp\"],\n",
    "    out_dir=plots_out_dir,\n",
    ")\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Evaluation Metrics\n",
    "'''\n",
    "- Coverage: How often true parameters fall within predicted ranges:\n",
    "-- perfect diagonal line means perfect coverage\n",
    "-- points above diaganol means over-confident predictions\n",
    "-- points below diagonal means under-confident\n",
    "-- large deviations from the diaganol means poorly calibrated model\n",
    "\n",
    "- Histogram:\n",
    "-- uniform (straight) distributions of p-values means well-calibrated model\n",
    "-- u-shaped dist: under-confident model\n",
    "-- bell shaped: over confident model\n",
    "\n",
    "- Predictions: Compare model predictions with observations\n",
    "-- True values from your test set (these are your \"observations\")\n",
    "-- Model's predicted distributions for each parameter\n",
    "-- Predicted distributions centered on true values\n",
    "-- Error bar estimates that accurately capture the true values\n",
    "-- Scatter along diagonal: Good predictions\n",
    "-- Systematic offset: Bias in predictions\n",
    "-- Wide spread: High uncertainty\n",
    "-- Clustering in certain regions: Model performs better for some parameter ranges\n",
    "# The predictions are comparing:\n",
    "# The true parameter values used in your simulations (like Omega0, sigma8, etc.) vs\n",
    "# What your neural posterior estimation (NPE) model predicts these values should be\n",
    "# based on the observables (your luminosity functions and/or colors)\n",
    "\n",
    "- TARP: Total Absolute Relative Probability\n",
    "-- lower values indicate better calibration\n",
    "-- comparing across parameters can help identify which are harder to predict\n",
    "-- high TARP values suggest need for model improvement for those parameters\n",
    "\n",
    "'''\n",
    "# use test data here.\n",
    "fig = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x=x_all[test_mask].cpu(),\n",
    "    # theta=theta[test_mask].cpu(),\n",
    "    theta=theta[test_mask, :].cpu(),\n",
    "    signature=f\"coverage_{name}_\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
