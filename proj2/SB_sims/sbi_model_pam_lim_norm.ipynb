{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import GPUtil\n",
    "import itertools\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import joblib\n",
    "import ili  # Import ili for the SBI functionality\n",
    "from ili.dataloaders import NumpyLoader\n",
    "from ili.inference import InferenceRunner\n",
    "from ili.validation.metrics import PosteriorCoverage, PlotSinglePosterior\n",
    "\n",
    "from sbi.utils.user_input_checks import process_prior\n",
    "\n",
    "sys.path.append(\"/disk/xray15/aem2/camels/proj2\")\n",
    "from setup_params_1P import plot_uvlf, plot_colour\n",
    "from setup_params_SB import *\n",
    "# from setup_params import *\n",
    "from priors_SB import initialise_priors_SB28\n",
    "from variables_config_28 import uvlf_limits, n_bins_lf, colour_limits, n_bins_colour\n",
    "# parameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = \"IllustrisTNG\"\n",
    "spec_type = \"attenuated\"\n",
    "sps = \"BC03\"\n",
    "snap = [\"044\"]\n",
    "bands = \"all\" # or just GALEX?\n",
    "\n",
    "colours = False  \n",
    "luminosity_functions = True\n",
    "name = f\"{model}_{bands}_{sps}_{spec_type}_{n_bins_lf}_{n_bins_colour}\"\n",
    "\n",
    "cam = camels(model=model, sim_set='SB28')\n",
    "\n",
    "if colours and not luminosity_functions:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/colours_only/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/colours_only/\"\n",
    "    \n",
    "elif luminosity_functions and not colours:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/lf_only/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/lfs_only/\"\n",
    "\n",
    "elif colours and luminosity_functions:\n",
    "    model_out_dir = \"/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/models/colours_lfs/\"\n",
    "    plots_out_dir = \"/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/sbi_plots/colours_lfs/\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"At least one of colours or luminosity_functions must be True\")\n",
    "\n",
    "print(\"Saving model in \", model_out_dir)\n",
    "print(\"Saving plots in \", plots_out_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check what type of data we have\n",
    "def convert_to_numpy(data):\n",
    "    \"\"\"\n",
    "    Safely converts data to numpy array regardless of whether it's a torch tensor or numpy array\n",
    "    \n",
    "    Parameters:\n",
    "        data: Input data (either torch.Tensor or numpy.ndarray)\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The data converted to numpy array\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(data):\n",
    "        # If it's a torch tensor, move to CPU and convert to numpy\n",
    "        return data.cpu().numpy()\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        # If it's already a numpy array, return as is\n",
    "        return data\n",
    "    else:\n",
    "        # If it's neither, try converting to numpy array\n",
    "        return np.array(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check what type of data we have\n",
    "def convert_to_numpy(data):\n",
    "    \"\"\"\n",
    "    Safely converts data to numpy array regardless of whether it's a torch tensor or numpy array\n",
    "    \n",
    "    Parameters:\n",
    "        data: Input data (either torch.Tensor or numpy.ndarray)\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The data converted to numpy array\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(data):\n",
    "        # If it's a torch tensor, move to CPU and convert to numpy\n",
    "        return data.cpu().numpy()\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        # If it's already a numpy array, return as is\n",
    "        return data\n",
    "    else:\n",
    "        # If it's neither, try converting to numpy array\n",
    "        return np.array(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uvlf_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parameter info file (df_info) is used for defining priors\n",
    "# the actual parameter values come from the camels class which reads CosmoAstroSeed_IllustrisTNG_L25n256_SB28.txt\n",
    "\n",
    "#  parameters defined here: /disk/xray15/aem2/data/28pams/IllustrisTNG/SB/CosmoAstroSeed_IllustrisTNG_L25n256_SB28.txt which is used for theta\n",
    "df_pars = pd.read_csv('/disk/xray15/aem2/data/28pams/IllustrisTNG/SB/CosmoAstroSeed_IllustrisTNG_L25n256_SB28.txt', delim_whitespace=True)\n",
    "print(df_pars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pams = 6\n",
    "end_of_pams = no_pams + 1 \n",
    "param_names = df_pars.columns[1:end_of_pams].tolist()  # Excluding 'name' column\n",
    "param_length = len(param_names)\n",
    "\n",
    "\n",
    "param_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_lst = df_pars.iloc[:, 1:end_of_pams].to_numpy()  # excluding 'name' column and 'seed' column\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, param_name in enumerate(param_names):\n",
    "    axes[i].hist(theta_lst[:, i], bins=50)\n",
    "    axes[i].set_title(param_name)\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# prior values come from this:\n",
    "df_info = pd.read_csv(\"/disk/xray15/aem2/data/28pams/Info_IllustrisTNG_L25n256_28params.txt\")\n",
    "print(df_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info6 = df_info[0:6]\n",
    "df_info6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite theta with the real theta\n",
    "if __name__ == \"__main__\":\n",
    "    theta, x = get_theta_x_SB(\n",
    "        luminosity_functions=luminosity_functions,\n",
    "        colours=colours,\n",
    "        uvlf_limits=uvlf_limits,\n",
    "        colour_limits=colour_limits,\n",
    "        n_bins_lf=n_bins_lf,\n",
    "        n_bins_colour=n_bins_colour\n",
    "    )\n",
    "    print(theta.shape, x.shape)\n",
    "\n",
    "if colours:\n",
    "    fig = plot_colour(x)\n",
    "    plt.savefig('/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/colours_test/colour_check.png')\n",
    "    plt.show()\n",
    "\n",
    "if luminosity_functions:\n",
    "    fig = plot_uvlf(x)\n",
    "    plt.savefig('/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/LFs_test/uvlf_check.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get the priors and data\n",
    "prior = initialise_priors_SB28(\n",
    "    df=df_info6, \n",
    "    device=device,\n",
    "    astro=True,\n",
    "    dust=False  # no dust for testing. set to False to only get the 28 model parameters.\n",
    "    # with dust = True, prior has 32 dimensions (28 parameters + 4 dust parameters) \n",
    ")\n",
    "\n",
    "# process the data\n",
    "x_all = np.array([np.hstack(_x) for _x in x])\n",
    "x_all = torch.tensor(x_all, dtype=torch.float32, device=device)\n",
    "\n",
    "print(\"Theta shape:\", theta.shape)\n",
    "print(\"X shape:\", x_all.shape)\n",
    "\n",
    "\n",
    "# Move data to GPU as early as possible\n",
    "x_all = x_all.to(device)\n",
    "print('x_all:', x_all)\n",
    "\n",
    "theta = torch.tensor(theta, dtype=torch.float32, device=device)\n",
    "print('theta:', theta)\n",
    "\n",
    "# Handle NaN values and normalize while on GPU\n",
    "x_all_cpu = x_all.cpu().numpy()  # Only move to CPU when necessary for sklearn\n",
    "print('x_all_cpu:', x_all_cpu)\n",
    "\n",
    "print(\"Data shape before processing:\", x_all_cpu.shape)\n",
    "print(\"Number of values:\",(x_all_cpu).sum())\n",
    "print(\"Number of NaN values:\", np.isnan(x_all_cpu).sum())\n",
    "print(\"Number of infinite values:\", np.isinf(x_all_cpu).sum())\n",
    "\n",
    "\n",
    "# get rid of NaN/inf values, replace with small random noise\n",
    "nan_mask = np.isnan(x_all_cpu) | np.isinf(x_all_cpu)\n",
    "print('nan_mask:', nan_mask)\n",
    "\n",
    "\n",
    "if nan_mask.any():\n",
    "    x_all_cpu[nan_mask] = np.random.rand(np.sum(nan_mask)) * 1e-10\n",
    "\n",
    "print(\"Data shape before processing:\", x_all_cpu.shape)\n",
    "print(\"Number of NaN values:\", np.isnan(x_all_cpu).sum())\n",
    "print(\"Number of infinite values:\", np.isinf(x_all_cpu).sum())\n",
    "\n",
    "print('x_all_cpu:', x_all_cpu)\n",
    "\n",
    "# make test mask\n",
    "test_mask = create_test_mask() # 10% testing\n",
    "train_mask = ~test_mask # 90% for training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # additional processing to solve cpu / cuda issues\n",
    "# # we want it to be numpy before we put into the model and then put it into the model\n",
    "\n",
    "# # # process the data\n",
    "# x_all = np.array([np.hstack(_x) for _x in x])\n",
    "# # x_all = torch.tensor(x_all, dtype=torch.float32, device=device)\n",
    "\n",
    "# print(\"Theta shape:\", theta.shape)\n",
    "# print(\"X shape:\", x_all.shape)\n",
    "\n",
    "\n",
    "# # # # Move data to GPU as early as possible\n",
    "# # x_all = x_all.to(device)\n",
    "# # # print('x_all:', x_all)\n",
    "\n",
    "# # theta = torch.tensor(theta, dtype=torch.float32, device=device)\n",
    "# # # print('theta:', theta)\n",
    "\n",
    "# # Handle NaN values and normalize while on GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If using only luminosity functions (LFs):\n",
    "\n",
    "2 GALEX filters (FUV, NUV)\n",
    "Each filter gets n_bins_lf (12) bins\n",
    "2 filters * 12 bins = 24 features\n",
    "\n",
    "If also using colors:\n",
    "\n",
    "1 color (FUV-NUV)\n",
    "Gets n_bins_colour (12) bins\n",
    "Total would be 24 + 12 = 36 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"Theta shape:\", theta.shape)\n",
    "# print(\"X shape:\", x_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class GlobalScaler:\n",
    "    def fit_transform(self, x):\n",
    "        self._mean_x = x.mean()\n",
    "        self._std_x = x.std()\n",
    "        return (x - self._mean_x) / self._std_x\n",
    "\n",
    "    def transform(self, x):\n",
    "        return (x - self._mean_x) / self._std_x\n",
    "\n",
    "    def inverse_transform(self, x_norm):\n",
    "        return (x_norm * self._std_x) + self._mean_x\n",
    "\n",
    "\n",
    "x_scaler = GlobalScaler()  # scales all features by same mean and std  over N samples\n",
    "theta_scaler = StandardScaler()  # scales each feature by its own mean and std over N samples\n",
    "\n",
    "# Convert theta tensors to numpy for scaling\n",
    "# convert to numpy\n",
    "x = convert_to_numpy(x)\n",
    "theta = convert_to_numpy(theta)\n",
    "\n",
    "print(\"Type of x:\", type(x))\n",
    "print(\"Type of theta:\", type(theta))\n",
    "\n",
    "# split into training and testing samples:\n",
    "\n",
    "# make test mask\n",
    "test_mask = create_test_mask() # 10% testing\n",
    "train_mask = ~test_mask # 90% for training\n",
    "\n",
    "\n",
    "# split data\n",
    "x_train=x_all_cpu[train_mask]\n",
    "theta_train=theta[train_mask]\n",
    "\n",
    "x_test = x_all_cpu[test_mask]\n",
    "theta_test = theta[test_mask]\n",
    "\n",
    "\n",
    "# apply scaling /normalisation\n",
    "x_train = x_scaler.fit_transform(x_train)\n",
    "x_test = x_scaler.transform(x_train)\n",
    "# theta_train_scaled = theta_scaler.fit_transform(theta_train_np)\n",
    "# theta_test_scaled = theta_scaler.transform(theta_test_np)\n",
    "theta_train = theta_scaler.fit_transform(theta_train)\n",
    "theta_test = theta_scaler.transform(theta_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colours:\n",
    "    fig = plot_colour(x_train)\n",
    "    plt.savefig('/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/colours_test/colour_check.png')\n",
    "    plt.show()\n",
    "\n",
    "if luminosity_functions:\n",
    "    fig = plot_uvlf(x_train)\n",
    "    plt.savefig('/disk/xray15/aem2/plots/28pams/IllustrisTNG/SB/test/LFs_test/uvlf_check.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = {\n",
    "    \"training_batch_size\": 16,     # Slightly reduced\n",
    "    \"learning_rate\": 1e-4,        # Original learning rate for fewer parameters\n",
    "    \"stop_after_epochs\": 20,      # More patience\n",
    "    \"clip_max_norm\": 2,        # Tight gradient control\n",
    "     \"validation_fraction\": 0.1,    # More validation data\n",
    "    \"max_num_epochs\": 200,       # Allow longer training\n",
    "    \"show_train_summary\": True\n",
    "}\n",
    "\n",
    "# Slightly increased capacity for better learning of core parameters\n",
    "hidden_features = 30\n",
    "num_transforms = 2\n",
    "num_nets = 1\n",
    "\n",
    "# refer back to training args in file name\n",
    "model_params = f\"batch{train_args['training_batch_size']}_\" \\\n",
    "               f\"lr{train_args['learning_rate']:.0e}_\" \\\n",
    "               f\"epochs{train_args['stop_after_epochs']}_\" \\\n",
    "               f\"val{train_args['validation_fraction']}_\" \\\n",
    "               f\"hidden{hidden_features}_\" \\\n",
    "               f\"transforms{num_transforms}\"\n",
    "\n",
    "# Create neural networks\n",
    "nets = [ili.utils.load_nde_sbi(\n",
    "    engine=\"NPE\",\n",
    "    model=\"nsf\",\n",
    "    hidden_features=hidden_features,\n",
    "    num_transforms=num_transforms,\n",
    ") for _ in range(num_nets)]\n",
    "\n",
    "# Modify loader to use core parameters\n",
    "loader = NumpyLoader(\n",
    "    x=x_train,#.clone().detach(),\n",
    "    theta=theta_train#.clone().detach()\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "# Update model output directory for phase 1\n",
    "# model_out_dir_phase1 = os.path.join(model_out_dir, \"phase1_core\")\n",
    "# os.makedirs(model_out_dir_phase1, exist_ok=True)\n",
    "\n",
    "# Create inference runner with core parameters\n",
    "runner = InferenceRunner.load(\n",
    "    backend=\"sbi\",\n",
    "    engine=\"NPE\",\n",
    "    prior=prior,\n",
    "    nets=nets,\n",
    "    device=device,\n",
    "    train_args=train_args,\n",
    "    proposal=None,\n",
    "    out_dir=model_out_dir,\n",
    "    # name=name + \"_phase1_core\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, update plotting to show only core parameters\n",
    "posterior_ensemble, summaries = runner(loader=loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_diagnostics(summaries):\n",
    "    \"\"\"Plot training diagnostics without empty subplots\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))  # Changed to 1 row, 2 columns\n",
    "    \n",
    "    # Loss curves\n",
    "    train_losses = summaries[0]['training_log_probs']\n",
    "    val_losses = summaries[0]['validation_log_probs']\n",
    "    epochs = range(len(train_losses))\n",
    "    \n",
    "    ax1.plot(epochs, train_losses, '-', label='Training', color='blue')\n",
    "    ax1.plot(epochs, val_losses, '--', label='Validation', color='red')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Log probability')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overfitting gap\n",
    "    gap = np.array(train_losses) - np.array(val_losses)\n",
    "    ax2.plot(epochs, gap, '-', color='purple')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss difference')\n",
    "    ax2.set_title('Overfitting Gap')\n",
    "    ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Use the function\n",
    "fig = plot_training_diagnostics(summaries)\n",
    "plt.savefig(os.path.join(plots_out_dir, f'loss_overfitting_{model_params}.png'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train/validation loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,4))\n",
    "c = list(mcolors.TABLEAU_COLORS)\n",
    "for i, m in enumerate(summaries):\n",
    "    ax.plot(m['training_log_probs'], ls='-', label=f\"{i}_train\", c=c[i])\n",
    "    ax.plot(m['validation_log_probs'], ls='--', label=f\"{i}_val\", c=c[i])\n",
    "ax.set_xlim(0)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Log probability')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing samples from the entire test set to look at overall performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(theta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After scaling, convert to torch tensors with correct dtype\n",
    "# x_train = torch.tensor(x_train, dtype=torch.float32, device=device) \n",
    "x_test = torch.tensor(x_test, dtype=torch.float32, device=device)\n",
    "# theta_train = torch.tensor(theta_train, dtype=torch.float32, device=device)\n",
    "theta_test = torch.tensor(theta_test, dtype=torch.float32, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(theta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get test data\n",
    "# x_test = x_all[test_mask]\n",
    "# theta_test = theta[test_mask]\n",
    "\n",
    "# we already have test data and it has been scaled already:\n",
    "# try this instead\n",
    "# x_test = convert_to_numpy(x_test).cpu()\n",
    "# theta_test = convert_to_numpy(theta_test).cpu()\n",
    "\n",
    "# Number of samples to draw from posterior\n",
    "n_samples = 1000\n",
    "\n",
    "# Storage for predictions\n",
    "all_samples = []\n",
    "all_means = []\n",
    "all_stds = []\n",
    "\n",
    "# Suppress the deprecation warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='nflows.transforms.lu')\n",
    "\n",
    "# Generate posterior samples for each test point\n",
    "for i in range(len(x_test)):\n",
    "    # Get samples from the posterior\n",
    "    with torch.no_grad():  # Add this for efficiency\n",
    "        samples = posterior_ensemble.sample(\n",
    "            (n_samples,), \n",
    "            x=x_test[i].reshape(1, -1)\n",
    "        ).cpu().numpy()\n",
    "    \n",
    "    # Calculate mean and std of samples\n",
    "    mean = samples.mean(axis=0)\n",
    "    std = samples.std(axis=0)\n",
    "    \n",
    "    all_samples.append(samples)\n",
    "    all_means.append(mean)\n",
    "    all_stds.append(std)\n",
    "\n",
    "all_samples = np.array(all_samples)\n",
    "all_means = np.array(all_means)\n",
    "all_stds = np.array(all_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Number of samples to draw from posterior\n",
    "# n_samples = 1000\n",
    "\n",
    "# # Storage for predictions\n",
    "# all_samples = []\n",
    "# all_means = []\n",
    "# all_stds = []\n",
    "\n",
    "# # Suppress the deprecation warning\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore', category=UserWarning, module='nflows.transforms.lu')\n",
    "\n",
    "# # Generate posterior samples for each test point\n",
    "# for i in range(len(x_test)):\n",
    "#     # Get samples from the posterior\n",
    "#     with torch.no_grad():  # Add this for efficiency\n",
    "#         samples = posterior_ensemble.sample(\n",
    "#             (n_samples,), \n",
    "#             x=x_test[i].reshape(1, -1)\n",
    "#         ).cpu().numpy()\n",
    "    \n",
    "#     # Calculate mean and std of samples\n",
    "#     mean = samples.mean(axis=0)\n",
    "#     std = samples.std(axis=0)\n",
    "    \n",
    "#     all_samples.append(samples)\n",
    "#     all_means.append(mean)\n",
    "#     all_stds.append(std)\n",
    "\n",
    "# all_samples = np.array(all_samples)\n",
    "# all_means = np.array(all_means)\n",
    "# all_stds = np.array(all_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = df_pars.columns[1:29].tolist()  # Excluding 'name' column\n",
    "\n",
    "fig, axes = plt.subplots(7, 4, figsize=(16, 28)) \n",
    "axes = axes.flatten()\n",
    "\n",
    "fontsize = 10  \n",
    "\n",
    "plt.rcParams['figure.constrained_layout.use'] = True  \n",
    "\n",
    "# Plot each parameter\n",
    "for i in range(28):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # True vs predicted with error bars\n",
    "    ax.errorbar(\n",
    "        theta_test[:, i].cpu().numpy(),\n",
    "        all_means[:, i],\n",
    "        yerr=all_stds[:, i],\n",
    "        fmt='.',\n",
    "        color='k',\n",
    "        ecolor='blue',\n",
    "        capsize=0,\n",
    "        elinewidth=0.8,  \n",
    "        alpha=0.3,       \n",
    "        markersize=5    \n",
    "    )\n",
    "    \n",
    "    # Add true line\n",
    "    lims = [\n",
    "        min(ax.get_xlim()[0], ax.get_ylim()[0]),\n",
    "        max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "    ]\n",
    "    ax.plot(lims, lims, '--', color='black', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    # get metrics\n",
    "    rmse = np.sqrt(np.mean((theta_test[:, i].cpu().numpy() - all_means[:, i])**2))\n",
    "    r2 = np.corrcoef(theta_test[:, i].cpu().numpy(), all_means[:, i])[0, 1]**2\n",
    "    chi2 = np.mean(((theta_test[:, i].cpu().numpy() - all_means[:, i])**2) / (all_stds[:, i]**2))\n",
    "    \n",
    "    # add metrics box in top left corner\n",
    "    stats_text = f'RMSE = {rmse:.2f}\\n' + \\\n",
    "                 f'R² = {r2:.2f}\\n' + \\\n",
    "                 f'χ² = {chi2:.2f}'\n",
    "    ax.text(0.05, 0.95, stats_text,\n",
    "            transform=ax.transAxes,\n",
    "            bbox=dict(facecolor='white', alpha=0.8),\n",
    "            verticalalignment='top',\n",
    "            fontsize=fontsize-1)  # Slightly smaller font for stats\n",
    "    \n",
    "    # title: parameter name\n",
    "    ax.set_title(param_names[i], fontsize=fontsize, pad=5)  # Reduced padding\n",
    "    \n",
    "    # axis labels\n",
    "    ax.set_xlabel('True', fontsize=fontsize-1)\n",
    "    ax.set_ylabel('Inferred', fontsize=fontsize-1)\n",
    "    \n",
    "    # tick labels\n",
    "    ax.tick_params(axis='both', which='major', labelsize=fontsize-2)\n",
    "    \n",
    "    # internal padding\n",
    "    ax.margins(x=0.05, y=0.05)\n",
    "\n",
    "# subplot spacing\n",
    "plt.subplots_adjust(\n",
    "    left=0.01,    # Less space on left\n",
    "    right=0.7,   # Less space on right\n",
    "    bottom=0.05,  # Less space at bottom\n",
    "    top=0.7,     # Less space at top\n",
    "    wspace=0.2,   # Less space between plots horizontally\n",
    "    hspace=0.2    # Less space between plots vertically\n",
    ")\n",
    "\n",
    "\n",
    "# Save figure with detailed filename\n",
    "save_path = f'{plots_out_dir}/posterior_predictions_{model_params}.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "print(save_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_str = (f\"batch{train_args['training_batch_size']}_\"\n",
    "             f\"lr{train_args['learning_rate']}_\"\n",
    "             f\"epochs{train_args['stop_after_epochs']}_\"\n",
    "             f\"h{hidden_features}_t{num_transforms}\")\n",
    "\n",
    "# coverage plots\n",
    "metric = PosteriorCoverage(\n",
    "    num_samples=int(4e3),\n",
    "    sample_method='direct',\n",
    "    labels=cam.labels,\n",
    "    plot_list=[\"tarp\"], # \"coverage\", \"histogram\", \"predictions\", \n",
    "    out_dir=plots_out_dir,\n",
    ")\n",
    "\n",
    "# Generate plots\n",
    "figs = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x=x_all[test_mask].cpu(),\n",
    "    theta=theta[test_mask, :].cpu(),\n",
    "    signature=f\"coverage_{name}_{config_str}_\"  # Add config to filename\n",
    ")\n",
    "\n",
    "config_text = (\n",
    "    f\"Training Config:\\n\"\n",
    "    f\"Batch Size: {train_args['training_batch_size']}\\n\"\n",
    "    f\"Learning Rate: {train_args['learning_rate']}\\n\"\n",
    "    f\"Epochs: {train_args['stop_after_epochs']}\\n\"\n",
    "    f\"Hidden Features: {hidden_features}\\n\"\n",
    "    f\"Num Transforms: {num_transforms}\"\n",
    ")\n",
    "\n",
    "# Process each figure\n",
    "for i, fig in enumerate(figs):\n",
    "    plt.figure(fig.number)  # Activate the figure\n",
    "    plt.figtext(0.02, 0.98, config_text,\n",
    "                fontsize=8,\n",
    "                bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),\n",
    "                verticalalignment='top')\n",
    "    \n",
    "    # Save each figure with type indicator\n",
    "    plot_types = [\"tarp\"] #\"coverage\", \"histogram\", \"predictions\",\n",
    "    plt.savefig(os.path.join(plots_out_dir, \n",
    "                f'metric_{plot_types[i]}_{name}_{config_str}.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metric(summaries, posterior):\n",
    "    # Get validation loss at convergence\n",
    "    final_val_loss = summaries[0]['validation_log_probs'][-1]\n",
    "    \n",
    "    # Get average RMSE across parameters\n",
    "    rmse_vals = [\n",
    "        np.sqrt(np.mean((theta_test[:, i].cpu().numpy() - all_means[:, i])**2))\n",
    "        for i in range(len(param_names))\n",
    "    ]\n",
    "    avg_rmse = np.mean(rmse_vals)\n",
    "    \n",
    "    # Get average R² across parameters  \n",
    "    r2_vals = [\n",
    "        np.corrcoef(theta_test[:, i].cpu().numpy(), all_means[:, i])[0, 1]**2 \n",
    "        for i in range(len(param_names))\n",
    "    ]\n",
    "    avg_r2 = np.mean(r2_vals)\n",
    "\n",
    "    # Composite score (lower is better)\n",
    "    # Weighting validation loss more heavily since it's key for convergence\n",
    "    composite_score = -final_val_loss + avg_rmse - avg_r2\n",
    "\n",
    "    return {\n",
    "        'composite_score': composite_score,\n",
    "        'val_loss': final_val_loss,\n",
    "        'avg_rmse': avg_rmse,\n",
    "        'avg_r2': avg_r2\n",
    "    }\n",
    "\n",
    "results = {\n",
    "    'architecture': {\n",
    "        'hidden_features': hidden_features,\n",
    "        'num_transforms': num_transforms,\n",
    "        'num_nets': num_nets\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': train_args['training_batch_size'],\n",
    "        'learning_rate': train_args['learning_rate'],\n",
    "        'epochs': train_args['stop_after_epochs']\n",
    "    },\n",
    "    'performance': get_performance_metric(summaries, posterior_ensemble)\n",
    "}\n",
    "\n",
    "# Flatten the nested dictionary into a single level\n",
    "flat_results = {\n",
    "   'hidden_features': results['architecture']['hidden_features'],\n",
    "   'num_transforms': results['architecture']['num_transforms'], \n",
    "   'num_nets': results['architecture']['num_nets'],\n",
    "   'batch_size': results['training']['batch_size'],\n",
    "   'learning_rate': results['training']['learning_rate'],\n",
    "   'epochs': results['training']['epochs'],\n",
    "   'composite_score': results['performance']['composite_score'],\n",
    "   'val_loss': results['performance']['val_loss'],\n",
    "   'avg_rmse': results['performance']['avg_rmse'], \n",
    "   'avg_r2': results['performance']['avg_r2']\n",
    "}\n",
    "\n",
    "# Create DataFrame with single row\n",
    "df = pd.DataFrame([flat_results])\n",
    "\n",
    "# Improve formatting\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_results(results):\n",
    "   flat_results = {\n",
    "       'hidden_features': results['architecture']['hidden_features'],\n",
    "       'num_transforms': results['architecture']['num_transforms'], \n",
    "       'num_nets': results['architecture']['num_nets'],\n",
    "       'batch_size': results['training']['batch_size'],\n",
    "       'learning_rate': results['training']['learning_rate'],\n",
    "       'epochs': results['training']['epochs'],\n",
    "       'composite_score': results['performance']['composite_score'],\n",
    "       'val_loss': results['performance']['val_loss'],\n",
    "       'avg_rmse': results['performance']['avg_rmse'],\n",
    "       'avg_r2': results['performance']['avg_r2']\n",
    "   }\n",
    "   \n",
    "   df_row = pd.DataFrame([flat_results])\n",
    "   \n",
    "   if os.path.exists('sbi_results_norm.csv'):\n",
    "       df_row.to_csv('sbi_results_norm.csv', mode='a', header=False, index=False)\n",
    "   else:\n",
    "       df_row.to_csv('sbi_results_norm.csv', index=False)\n",
    "       \n",
    "save_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The composite score weights validation loss more heavily since it's crucial for the model's predictive performance. Lower composite scores indicate better overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (camels)",
   "language": "python",
   "name": "camels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
