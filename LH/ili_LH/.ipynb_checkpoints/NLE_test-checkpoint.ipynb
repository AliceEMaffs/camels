{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ltu-ili jupyter interface\n",
    "This is a tutorial for using the ltu-ili inference framework in a jupyter notebook. \n",
    "\n",
    "This notebook assumes you have installed the ltu-ili package from the installation instructions in [INSTALL.md](../INSTALL.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:04:52.441763Z",
     "start_time": "2023-11-03T19:04:52.395335Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ignore warnings for readability\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "from torch.distributions import Uniform, ExpTransform, TransformedDistribution #, AffineTransform\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import ili\n",
    "from ili.dataloaders import NumpyLoader\n",
    "from ili.inference import InferenceRunner\n",
    "from ili.validation.metrics import PosteriorCoverage, PlotSinglePosterior\n",
    "\n",
    "from sbi.utils.user_input_checks import process_prior\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pars = pd.read_csv('/home/jovyan/camels/LH/CosmoAstroSeed_IllustrisTNG_L25n256_LH.txt', delim_whitespace=True)\n",
    "df_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = df_pars[['Omega_m', 'sigma_8', 'A_SN1', 'A_AGN1', 'A_SN2', 'A_AGN2']].to_numpy()\n",
    "print(theta)\n",
    "print(theta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the LH_X files\n",
    "directory = \"/home/jovyan/camels/LH/get_LF/output/\"\n",
    "\n",
    "# Get all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Filter out files that start with \"LH_\" and end with \".txt\"\n",
    "LH_X_files = [file for file in files if file.startswith(\"LH_\") and file.endswith(\".txt\")]\n",
    "\n",
    "# Initialize lists to store data\n",
    "phia = []\n",
    "phi_sigmaa = []\n",
    "binsa = []\n",
    "LH_X_values = []\n",
    "\n",
    "# Iterate over LH_X files\n",
    "for LH_X_file in LH_X_files:\n",
    "    # Define the file path\n",
    "    file_path = os.path.join(directory, LH_X_file)\n",
    "    \n",
    "    # Extract LH_X value from the file name (remove the \".txt\" extension)\n",
    "    LH_X = LH_X_file[:-4]\n",
    "    \n",
    "    # Initialize an empty dictionary to store variable names and their values\n",
    "    variable_data = {}\n",
    "\n",
    "    # Open the text file for reading\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Initialize variables to store the current variable name and its values\n",
    "        current_variable_name = None\n",
    "        current_variable_values = []\n",
    "\n",
    "        # Iterate over each line in the file\n",
    "        for line in file:\n",
    "            # Remove leading and trailing whitespace from the line\n",
    "            line = line.strip()\n",
    "\n",
    "            # Check if the line is empty\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Check if the line is a variable name\n",
    "            if line in ['phi', 'phi_sigma', 'hist', 'massBinLimits']:\n",
    "                # If it's a new variable name, update the current variable name and reset the values list\n",
    "                if current_variable_name is not None:\n",
    "                    variable_data[current_variable_name] = current_variable_values\n",
    "                    current_variable_values = []\n",
    "\n",
    "                current_variable_name = line\n",
    "            else:\n",
    "                # If it's not a variable name, convert the value to float and append it to the values list\n",
    "                current_variable_values.append(float(line))\n",
    "\n",
    "        # Add the last variable data to the dictionary\n",
    "        if current_variable_name is not None:\n",
    "            variable_data[current_variable_name] = current_variable_values\n",
    "        \n",
    "        # Extract specific variables\n",
    "        phi = variable_data.get('phi')\n",
    "        phi_sigma = variable_data.get('phi_sigma')\n",
    "        bins = variable_data.get('massBinLimits')\n",
    "\n",
    "        phia.append(phi)\n",
    "        phi_sigmaa.append(phi_sigma)\n",
    "        binsa.append(bins)\n",
    "        LH_X_values.append(LH_X)\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df_x = pd.DataFrame({'LH_X': LH_X_values, 'phi': phia, 'phi_sigma': phi_sigmaa, 'bins': binsa})\n",
    "\n",
    "# Display the DataFrame\n",
    "df_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x['phi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zero or negative values with 1\n",
    "df_x['phi'] = np.where(df_x['phi'] <= 0, 1, df_x['phi'])\n",
    "\n",
    "# Convert the 'phi' column to a numpy array and apply np.log10\n",
    "log_phi_values = np.log10(df_x['phi'].to_numpy())\n",
    "\n",
    "# Add the log-transformed values back to the DataFrame in a new column\n",
    "df_x['log_phi'] = log_phi_values\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pandas series to np.array\n",
    "x = np.array(df_x['phi'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test:\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "x_train, x_test, theta_train, theta_test = train_test_split(x, theta, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure the shapes match\n",
    "print(f'x_train shape: {x_train.shape}, theta_train shape: {theta_train.shape}')\n",
    "print(f'x_test shape: {x_test.shape}, theta_test shape: {theta_test.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy NPE\n",
    "This example attempts to infer 3 unknown parameters from a 20-dimensional 1D data vector using amortized posterior inference. We train the models from a simple synthetic catalog. This tutorial mirrors the same configuration as in [examples/toy_sbi.py](../examples/toy_sbi.py), but demonstrates how one would interact with the inference pipeline in a jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataloader\n",
    "loader = NumpyLoader(x=x, theta=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for omega m, between 0.1 and 0.5\n",
    "plt.hist(theta[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for sigma_8, between 0.6 and 1\n",
    "plt.hist(theta[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for A_SN1, between 0.25 and 4\n",
    "plt.hist(theta[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for A_AGN1, between 0.25 and 4\n",
    "plt.hist(theta[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for A_SN2, between 0.5 and 2\n",
    "plt.hist(theta[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for A_AGN2, between 0.5 and 2\n",
    "plt.hist(theta[:, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is your posterior - predicted value of the luminosity function...?\n",
    "# maybe log it\n",
    "plt.hist(x[:, 0])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:04:56.177736Z",
     "start_time": "2023-11-03T19:04:55.959360Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot some examples of the data\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for i in range(6):\n",
    "    ind = np.random.randint(len(theta))\n",
    "    ax.plot(x[ind], alpha=0.5, label=f'({theta[ind, 0]:.2f}, {theta[ind, 1]:.2f}, {theta[ind, 2]:.2f})')\n",
    "ax.set_yscale('log')\n",
    "ax.legend(title='theta')\n",
    "ax.set_title('Data vectors x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SBIRunner object will handle all of the data normalization and model training for us. We just need to provide it with:\n",
    "- our parameter prior\n",
    "- our inference type (SNPE/SNLE/SNRE)\n",
    "- our desired neural network architecture\n",
    "- our training hyperparameters\n",
    "\n",
    "On the backend, it does a validation split among the provided training data, trains the neural networks with an Adam optimizer, and enforces an early stopping criterion to prevent overfitting. All the parameters of these processes can be independently configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_priors(device=\"cpu\", astro=True, dust=True):\n",
    "\n",
    "    combined_priors = []\n",
    "\n",
    "    if astro:\n",
    "        base_dist1 = Uniform(\n",
    "            torch.log(torch.tensor([0.25], device=device)),\n",
    "            torch.log(torch.tensor([4], device=device)),\n",
    "        )\n",
    "        base_dist2 = Uniform(\n",
    "            torch.log(torch.tensor([0.5], device=device)),\n",
    "            torch.log(torch.tensor([2], device=device)),\n",
    "        )\n",
    "        astro_prior1 = TransformedDistribution(base_dist1, ExpTransform())\n",
    "        astro_prior2 = TransformedDistribution(base_dist2, ExpTransform())\n",
    "        omega_prior = Uniform(\n",
    "            torch.tensor([0.1], device=device),\n",
    "            torch.tensor([0.5], device=device),\n",
    "        )\n",
    "        sigma8_prior = Uniform(\n",
    "            torch.tensor([0.6], device=device),\n",
    "            torch.tensor([1.0], device=device),\n",
    "        )\n",
    "        combined_priors += [\n",
    "            omega_prior,# prior for omega m, between 0.1 and 0.5: uniform\n",
    "            sigma8_prior,# prior for sigma_8, between 0.6 and 1: uniform\n",
    "            astro_prior1,# prior for A_SN1, between 0.25 and 4: exponential\n",
    "            astro_prior1,# prior for A_AGN1, between 0.25 and 4: exponential\n",
    "            astro_prior2,# prior for A_SN2, between 0.5 and 2: exponential\n",
    "            astro_prior2,# prior for A_AGN2, between 0.5 and 2: exponential\n",
    "        ]\n",
    "\n",
    "    prior = process_prior(combined_priors)\n",
    "\n",
    "    return prior[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = initialise_priors()\n",
    "print(prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Toy NLE\n",
    "This example uses the same dataset as the previous SNPE example, but uses a likelihood estimation model instead of an amortized posterior estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader and priors already defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:58:07.435823Z",
     "start_time": "2023-11-03T18:58:07.396124Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# instantiate your neural networks to be used as an ensemble\n",
    "nets = [\n",
    "    ili.utils.load_nde_sbi(engine='NLE', model='maf')#, hidden_features=50, num_transforms=5),\n",
    "]\n",
    "\n",
    "# define training arguments\n",
    "train_args = {\n",
    "    'training_batch_size': 10,\n",
    "    'learning_rate': 1e-4\n",
    "}\n",
    "\n",
    "''' \n",
    "# conditions otherwise set as:\n",
    "\n",
    "final_round: bool = False,\n",
    "training_batch_size: int = 50,\n",
    "learning_rate: float = 5e-4,\n",
    "validation_fraction: float = 0.1,\n",
    "stop_after_epochs: int = 20,\n",
    "max_num_epochs: int = 2**31 - 1,\n",
    "clip_max_norm: Optional[float] = 5.0,\n",
    "calibration_kernel: Optional[Callable] = None,\n",
    "resume_training: bool = False,\n",
    "retrain_from_scratch: bool = False,\n",
    "show_train_summary: bool = False,\n",
    "dataloader_kwargs: Optional[Dict] = None,\n",
    "component_perturbation: float = 5e-3,\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# initialize the trainer\n",
    "runner = InferenceRunner.load(\n",
    "    backend='sbi',\n",
    "    engine='NLE',\n",
    "    prior=prior,\n",
    "    nets=nets,\n",
    "    device=device,\n",
    "    embedding_net=None,\n",
    "    train_args=train_args,\n",
    "    proposal=None,\n",
    "    out_dir=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:59:18.553308Z",
     "start_time": "2023-11-03T18:58:10.815567Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train the model. this outputs a posterior model and training logs\n",
    "posterior_ensemble, summaries = runner(loader=loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train/validation loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,4))\n",
    "c = list(mcolors.TABLEAU_COLORS)\n",
    "for i, m in enumerate(summaries):\n",
    "    ax.plot(m['training_log_probs'], ls='-', label=f\"{i}_train\", c=c[i])\n",
    "    ax.plot(m['validation_log_probs'], ls='--', label=f\"{i}_val\", c=c[i])\n",
    "ax.set_xlim(0)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Log probability')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a random input\n",
    "seed_in = 49\n",
    "np.random.seed(seed_in)\n",
    "ind = np.random.randint(len(theta))\n",
    "\n",
    "# generate samples from the posterior using accept/reject sampling\n",
    "seed_samp = 32\n",
    "torch.manual_seed(seed_samp)\n",
    "samples = posterior_ensemble.sample((1000,), torch.Tensor(x[ind]).to(device))\n",
    "\n",
    "# calculate the log_prob for each sample\n",
    "log_prob = posterior_ensemble.log_prob(samples, torch.Tensor(x[ind]).to(device))\n",
    "\n",
    "samples = samples.cpu().numpy()\n",
    "log_prob = log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the posterior samples and the true value\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10,4), gridspec_kw={'width_ratios': [1,1,0.05]})\n",
    "for i in range(2):\n",
    "    axs[i].plot(theta[ind,i], theta[ind,i+1], 'r+', markersize=10, label='true')\n",
    "    im = axs[i].scatter(samples[:,i], samples[:,i+1], c=log_prob, s=4, label='samples')\n",
    "    axs[i].set_aspect('equal')\n",
    "    axs[i].set_xlim(0,1)\n",
    "    axs[i].set_ylim(0,1)\n",
    "    axs[i].set_xlabel(f'$\\\\theta_{i}$')\n",
    "    axs[i].set_ylabel(f'$\\\\theta_{i+1}$')\n",
    "    axs[i].legend()\n",
    "plt.colorbar(im, label='log probability', use_gridspec=True, cax=axs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing samples from the ensemble posterior\n",
    "\n",
    "metric = PosteriorCoverage(\n",
    "    num_samples=1000, sample_method='direct', \n",
    "    labels=[f'$\\\\theta_{i}$' for i in range(3)],\n",
    "    plot_list = [\"coverage\", \"histogram\", \"predictions\", \"tarp\", \"logprob\"],\n",
    "    out_dir=None\n",
    ")\n",
    "\n",
    "fig = metric(\n",
    "    posterior=posterior_ensemble, # NeuralPosteriorEnsemble instance from sbi package\n",
    "    x=x_test, theta=theta_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ltu-ili's built-in validation metrics to plot the posterior for this point\n",
    "metric = PlotSinglePosterior(\n",
    "    num_samples=1000, sample_method='direct', \n",
    "    labels=[f'$\\\\theta_{i}$' for i in range(3)]\n",
    ")\n",
    "fig = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x_obs = x[ind], theta_fid=theta[ind]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ltu-ili's built-in validation metrics to plot the posterior for this point\n",
    "metric = PlotSinglePosterior(\n",
    "    num_samples=1000, sample_method='direct', \n",
    "    labels=[f'$\\\\theta_{i}$' for i in range(3)]\n",
    ")\n",
    "fig = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x_obs = x_test[ind], theta_fid=theta_test[ind]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels",
   "language": "python",
   "name": "camels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
