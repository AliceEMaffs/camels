{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ltu-ili jupyter interface\n",
    "This is a tutorial for using the ltu-ili inference framework in a jupyter notebook. \n",
    "\n",
    "This notebook assumes you have installed the ltu-ili package from the installation instructions in [INSTALL.md](../INSTALL.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:04:52.441763Z",
     "start_time": "2023-11-03T19:04:52.395335Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ignore warnings for readability\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# seaparate into train and test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "from torch.distributions import Uniform, ExpTransform, TransformedDistribution #, AffineTransform\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import ili\n",
    "from ili.dataloaders import NumpyLoader\n",
    "from ili.inference import InferenceRunner\n",
    "from ili.validation.metrics import PosteriorCoverage, PlotSinglePosterior\n",
    "\n",
    "from sbi.utils.user_input_checks import process_prior\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pars = pd.read_csv('/home/jovyan/camels/LH/CosmoAstroSeed_IllustrisTNG_L25n256_LH.txt', delim_whitespace=True)\n",
    "df_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = df_pars[['Omega_m', 'sigma_8', 'A_SN1', 'A_AGN1', 'A_SN2', 'A_AGN2']].to_numpy()\n",
    "print(theta)\n",
    "print(theta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the LH_X files\n",
    "directory = \"/home/jovyan/camels/LH/get_LF/output/\"\n",
    "\n",
    "# Get all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Filter out files that start with \"LH_\" and end with \".txt\"\n",
    "LH_X_files = [file for file in files if file.startswith(\"LH_\") and file.endswith(\".txt\")]\n",
    "\n",
    "# Initialize lists to store data\n",
    "phia = []\n",
    "phi_sigmaa = []\n",
    "binsa = []\n",
    "LH_X_values = []\n",
    "\n",
    "# Iterate over LH_X files\n",
    "for LH_X_file in LH_X_files:\n",
    "    # Define the file path\n",
    "    file_path = os.path.join(directory, LH_X_file)\n",
    "    \n",
    "    # Extract LH_X value from the file name (remove the \".txt\" extension)\n",
    "    LH_X = LH_X_file[:-4]\n",
    "    \n",
    "    # Initialize an empty dictionary to store variable names and their values\n",
    "    variable_data = {}\n",
    "\n",
    "    # Open the text file for reading\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Initialize variables to store the current variable name and its values\n",
    "        current_variable_name = None\n",
    "        current_variable_values = []\n",
    "\n",
    "        # Iterate over each line in the file\n",
    "        for line in file:\n",
    "            # Remove leading and trailing whitespace from the line\n",
    "            line = line.strip()\n",
    "\n",
    "            # Check if the line is empty\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Check if the line is a variable name\n",
    "            if line in ['phi', 'phi_sigma', 'hist', 'massBinLimits']:\n",
    "                # If it's a new variable name, update the current variable name and reset the values list\n",
    "                if current_variable_name is not None:\n",
    "                    variable_data[current_variable_name] = current_variable_values\n",
    "                    current_variable_values = []\n",
    "\n",
    "                current_variable_name = line\n",
    "            else:\n",
    "                # If it's not a variable name, convert the value to float and append it to the values list\n",
    "                current_variable_values.append(float(line))\n",
    "\n",
    "        # Add the last variable data to the dictionary\n",
    "        if current_variable_name is not None:\n",
    "            variable_data[current_variable_name] = current_variable_values\n",
    "        \n",
    "        # Extract specific variables\n",
    "        phi = variable_data.get('phi')\n",
    "        phi_sigma = variable_data.get('phi_sigma')\n",
    "        bins = variable_data.get('massBinLimits')\n",
    "\n",
    "        phia.append(phi)\n",
    "        phi_sigmaa.append(phi_sigma)\n",
    "        binsa.append(bins)\n",
    "        LH_X_values.append(LH_X)\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df_x = pd.DataFrame({'LH_X': LH_X_values, 'phi': phia, 'phi_sigma': phi_sigmaa, 'bins': binsa})\n",
    "\n",
    "# Display the DataFrame\n",
    "df_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform x data into log space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x['phi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_x['phi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â clean up 0 values to avoid nan/-inf values when logging\n",
    "\n",
    "# Function to replace 0 or negative values with 1\n",
    "def replace_zeros(phi_list):\n",
    "    # set as 1e-5 for now:\n",
    "    return [1e-5 if x == 0 else x for x in phi_list]\n",
    "\n",
    "# Apply the function to each entry in the 'phi' column\n",
    "df_x['phi_0s'] = df_x['phi'].apply(replace_zeros)\n",
    "df_x['phi_0s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pandas series to np.array\n",
    "x = np.log10((np.array(df_x['phi_0s'].tolist())))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy NPE\n",
    "This example attempts to infer 3 unknown parameters from a 20-dimensional 1D data vector using amortized posterior inference. We train the models from a simple synthetic catalog. This tutorial mirrors the same configuration as in [examples/toy_sbi.py](../examples/toy_sbi.py), but demonstrates how one would interact with the inference pipeline in a jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for omega m, between 0.1 and 0.5\n",
    "plt.hist(theta[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for sigma_8, between 0.6 and 1\n",
    "plt.hist(theta[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for A_SN1, between 0.25 and 4\n",
    "plt.hist(theta[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for A_AGN1, between 0.25 and 4\n",
    "plt.hist(theta[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for A_SN2, between 0.5 and 2\n",
    "plt.hist(theta[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for A_AGN2, between 0.5 and 2\n",
    "plt.hist(theta[:, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming x and theta are your data arrays\n",
    "# First split: into train+validation and test sets\n",
    "# can stratify the sets you pick to train but in this case we are OK for now to make random selection (fixed random selection)\n",
    "x_temp, x_test, theta_temp, theta_test = train_test_split(x, theta, test_size=0.2, random_state=0)\n",
    "\n",
    "# Second split: into train and validation sets\n",
    "x_train, x_val, theta_train, theta_val = train_test_split(x_temp, theta_temp, test_size=0.25, random_state=0)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "print('x: full data:', x.shape)\n",
    "print('x: training set:', x_train.shape)\n",
    "print('x: validation set:', x_val.shape)\n",
    "print('x: testing set:', x_test.shape)\n",
    "\n",
    "print('theta: full data:', theta.shape)\n",
    "print('theta: training set:', theta_train.shape)\n",
    "print('theta: validation set:', theta_val.shape)\n",
    "print('theta: testing set:', theta_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is your posterior - predicted value of the luminosity function...?\n",
    "# maybe log it\n",
    "plt.hist(x_train[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:04:56.177736Z",
     "start_time": "2023-11-03T19:04:55.959360Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot some examples of the data\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for i in range(6):\n",
    "    ind = np.random.randint(len(theta_train))\n",
    "    ax.plot(x_train[ind], alpha=0.5, label=f'({theta_train[ind, 0]:.2f}, {theta_train[ind, 1]:.2f}, {theta_train[ind, 2]:.2f},{theta_train[ind, 3]:.2f},{theta_train[ind, 4]:.2f},{theta_train[ind, 5]:.2f})')\n",
    "#ax.set_yscale('log')\n",
    "ax.legend(title='theta')\n",
    "ax.set_title('Data vectors x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SBIRunner object will handle all of the data normalization and model training for us. We just need to provide it with:\n",
    "- our parameter prior\n",
    "- our inference type (SNPE/SNLE/SNRE)\n",
    "- our desired neural network architecture\n",
    "- our training hyperparameters\n",
    "\n",
    "On the backend, it does a validation split among the provided training data, trains the neural networks with an Adam optimizer, and enforces an early stopping criterion to prevent overfitting. All the parameters of these processes can be independently configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_priors(device=\"cpu\", astro=True, dust=True):\n",
    "\n",
    "    combined_priors = []\n",
    "\n",
    "    if astro:\n",
    "        base_dist1 = Uniform(\n",
    "            torch.log(torch.tensor([0.25], device=device)),\n",
    "            torch.log(torch.tensor([4], device=device)),\n",
    "        )\n",
    "        base_dist2 = Uniform(\n",
    "            torch.log(torch.tensor([0.5], device=device)),\n",
    "            torch.log(torch.tensor([2], device=device)),\n",
    "        )\n",
    "        astro_prior1 = TransformedDistribution(base_dist1, ExpTransform())\n",
    "        astro_prior2 = TransformedDistribution(base_dist2, ExpTransform())\n",
    "        omega_prior = Uniform(\n",
    "            torch.tensor([0.1], device=device),\n",
    "            torch.tensor([0.5], device=device),\n",
    "        )\n",
    "        sigma8_prior = Uniform(\n",
    "            torch.tensor([0.6], device=device),\n",
    "            torch.tensor([1.0], device=device),\n",
    "        )\n",
    "        combined_priors += [\n",
    "            omega_prior,# prior for omega m, between 0.1 and 0.5: uniform\n",
    "            sigma8_prior,# prior for sigma_8, between 0.6 and 1: uniform\n",
    "            astro_prior1,# prior for A_SN1, between 0.25 and 4: exponential\n",
    "            astro_prior1,# prior for A_AGN1, between 0.25 and 4: exponential\n",
    "            astro_prior2,# prior for A_SN2, between 0.5 and 2: exponential\n",
    "            astro_prior2,# prior for A_AGN2, between 0.5 and 2: exponential\n",
    "        ]\n",
    "\n",
    "    prior = process_prior(combined_priors)\n",
    "\n",
    "    return prior[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = initialise_priors()\n",
    "print(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataloader\n",
    "loader = NumpyLoader(x=x_train, theta=theta_train)\n",
    "\n",
    "# instantiate your neural networks to be used as an ensemble\n",
    "# are the NN here setting our likelihoods?\n",
    "nets = [\n",
    "    ili.utils.load_nde_sbi(engine='NPE', model='maf', hidden_features=50, num_transforms=5),\n",
    "    ili.utils.load_nde_sbi(engine='NPE', model='mdn', hidden_features=50, num_components=6)\n",
    "]\n",
    "\n",
    "# hyperparameter search\n",
    "# for batch_size in [4, 8, 16, 32]:\n",
    "# for learning_rate in [1e-2, 1e-3, 1e-4]:\n",
    "# for hidden_features in [50, 100, 200]\n",
    "# for num_components in [6, 12, 18]\n",
    "\n",
    "# define training arguments\n",
    "train_args = {\n",
    "    'training_batch_size': 10, # batch_size\n",
    "    'learning_rate': 1e-4 # learning_rate\n",
    "}\n",
    "\n",
    "#             training_batch_size=50,\n",
    "#             learning_rate=5e-4,\n",
    "#             validation_fraction=0.1,\n",
    "#             stop_after_epochs=20,\n",
    "#             clip_max_norm=5,\n",
    "\n",
    "# initialize the trainer\n",
    "runner = InferenceRunner.load(\n",
    "    backend='sbi',\n",
    "    engine='NPE',\n",
    "    prior=prior,\n",
    "    nets=nets,\n",
    "    device=device,\n",
    "    embedding_net=None,\n",
    "    train_args=train_args,\n",
    "    proposal=None,\n",
    "    out_dir=None\n",
    ")\n",
    "\n",
    "# need to play with training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:06:50.836068Z",
     "start_time": "2023-11-03T19:06:14.142618Z"
    }
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "posterior_ensemble, summaries = runner(loader=loader, seed=1)\n",
    "# seed fixes the validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the output of the runner is a posterior model and a log of training statistics. The posterior model is a [NeuralPosteriorEnsemble](https://github.com/mackelab/sbi/blob/6c4fa7a6fd254d48d0c18640c832f2d80ab2257a/sbi/utils/posterior_ensemble.py#L19) model and automatically combines samples and probability densities from its component networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:06:52.252093Z",
     "start_time": "2023-11-03T19:06:52.042239Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot train/validation loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,4))\n",
    "c = list(mcolors.TABLEAU_COLORS)\n",
    "for i, m in enumerate(summaries):\n",
    "    ax.plot(m['training_log_probs'], ls='-', label=f\"{i}_train\", c=c[i])\n",
    "    ax.plot(m['validation_log_probs'], ls='--', label=f\"{i}_val\", c=c[i])\n",
    "ax.set_xlim(0)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Log probability')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of theta: {theta_train.shape}\")\n",
    "print(f\"Shape of x: {x_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:06:54.953647Z",
     "start_time": "2023-11-03T19:06:54.836424Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now, SBIRunner returns a custom class instance to be able to pass signature strings\n",
    "# This class has simply for attributes a NeuralPosteriorEstimate and a string list \n",
    "print(posterior_ensemble.signatures)\n",
    "\n",
    "# choose a random input\n",
    "seed_in = 49\n",
    "np.random.seed(seed_in)\n",
    "ind = np.random.randint(len(theta_train))\n",
    "\n",
    "# generate samples from the posterior using accept/reject sampling\n",
    "seed_samp = 32\n",
    "torch.manual_seed(seed_samp)\n",
    "samples = posterior_ensemble.sample((1000,), torch.Tensor(x_train[ind]).to(device))\n",
    "\n",
    "# calculate the log_prob for each sample\n",
    "log_prob = posterior_ensemble.log_prob(samples, torch.Tensor(x_train[ind]).to(device))\n",
    "\n",
    "samples = samples.cpu().numpy()\n",
    "log_prob = log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# samples is the posterior, P(theta_hat | y_i), conditioned on data y_i, generated from theta_i (prior)\n",
    "plt.hist(samples[:, 2])\n",
    "print('Prior:',theta[ind, 2])\n",
    "print('Histogram is Posterior:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posterior samples and the true value for all pairs of theta values\n",
    "fig, axs = plt.subplots(5, 5, figsize=(15, 15))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if i != j:\n",
    "            axs[i, j].plot(theta_train[ind, i], theta_train[ind, j], 'r+', markersize=10, label='true')\n",
    "            im = axs[i, j].scatter(samples[:, i], samples[:, j], c=log_prob, s=4, label='samples', cmap='viridis')\n",
    "            axs[i, j].set_xlabel(f'$\\\\theta_{i}$')\n",
    "            axs[i, j].set_ylabel(f'$\\\\theta_{j}$')\n",
    "            axs[i, j].legend()\n",
    "        else:\n",
    "            axs[i, j].axis('off')\n",
    "\n",
    "# Add a color bar for log probability\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax, label='log probability')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('True values are marked with red pluses, and the pairs of theta values are plotted against each other.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['$\\\\Omega_m$', '$\\\\sigma_8$', 'A_SN1', 'A_AGN1', 'A_SN2', 'A_AGN2']\n",
    "\n",
    "# use ltu-ili's built-in validation metrics to plot the posterior for this point\n",
    "metric = PlotSinglePosterior(\n",
    "    num_samples=1000, sample_method='direct', \n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "seed_in = 49\n",
    "np.random.seed(seed_in)\n",
    "ind_2 = np.random.randint(len(theta_test))\n",
    "\n",
    "\n",
    "fig = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x_obs = x_test[ind_2], theta_fid=theta_test[ind_2]\n",
    ")\n",
    "# this is using [ind] from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ind_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the ensemble of trained posteriors models\n",
    "By default, running a SampleBasedMEtric with posterior from above will compute the metrics using the ensemble model. That is to say the ensemble is considered one model, with the weights of each posterior in the ensemble being the val_log_prob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:08:03.949691Z",
     "start_time": "2023-11-03T19:07:59.103834Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drawing samples from the ensemble posterior\n",
    "labels = ['$\\\\Omega_m$', '$\\\\sigma_8$', 'A_SN1', 'A_AGN1', 'A_SN2', 'A_AGN2']\n",
    "\n",
    "metric = PosteriorCoverage(\n",
    "    num_samples=1000, sample_method='direct', \n",
    "    labels=labels,\n",
    "    plot_list = [\"coverage\", \"histogram\", \"predictions\", \"tarp\"],\n",
    "    out_dir=None\n",
    "    \n",
    ")\n",
    "\n",
    "fig = metric(\n",
    "    posterior=posterior_ensemble, # NeuralPosteriorEnsemble instance from sbi package\n",
    "    x=x_test, theta=theta_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ensemble model, it looks like our posteriors are well-calibrated when evaluated on marginal distributions, but slightly negatively biased in the multivariate TARP coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating each trained posterior in the ensemble\n",
    "Below, we compute separately each SampleBasedMetric for every posterior in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:08:19.933411Z",
     "start_time": "2023-11-03T19:08:17.751688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Then for the MDN\n",
    "fig = metric(\n",
    "    posterior=posterior_ensemble.posteriors[1],\n",
    "    x=x_test, theta=theta_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we see that we are largely consistent and calibrated in the univariate coverage, with some slight negative bias shown in the multivariate coverage. It looks like the MAF model has slightly better constraints than the MDN model, while retaining the same calibration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels",
   "language": "python",
   "name": "camels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
