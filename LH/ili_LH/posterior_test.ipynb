{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ltu-ili jupyter interface\n",
    "This is a tutorial for using the ltu-ili inference framework in a jupyter notebook. \n",
    "\n",
    "This notebook assumes you have installed the ltu-ili package from the installation instructions in [INSTALL.md](../INSTALL.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:04:52.441763Z",
     "start_time": "2023-11-03T19:04:52.395335Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ignore warnings for readability\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "from torch.distributions import Uniform, ExpTransform, TransformedDistribution #, AffineTransform\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import ili\n",
    "from ili.dataloaders import NumpyLoader\n",
    "from ili.inference import InferenceRunner\n",
    "from ili.validation.metrics import PosteriorCoverage, PlotSinglePosterior\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pars = pd.read_csv('/home/jovyan/camels/LH/CosmoAstroSeed_IllustrisTNG_L25n256_LH.txt', delim_whitespace=True)\n",
    "df_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = df_pars[['Omega_m', 'sigma_8', 'A_SN1', 'A_AGN1', 'A_SN2', 'A_AGN2', 'seed']].to_numpy()\n",
    "print(theta)\n",
    "print(theta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the LH_X files\n",
    "directory = \"/home/jovyan/camels/LH/get_LF/output/\"\n",
    "\n",
    "# Get all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Filter out files that start with \"LH_\" and end with \".txt\"\n",
    "LH_X_files = [file for file in files if file.startswith(\"LH_\") and file.endswith(\".txt\")]\n",
    "\n",
    "# Initialize lists to store data\n",
    "phia = []\n",
    "phi_sigmaa = []\n",
    "binsa = []\n",
    "LH_X_values = []\n",
    "\n",
    "# Iterate over LH_X files\n",
    "for LH_X_file in LH_X_files:\n",
    "    # Define the file path\n",
    "    file_path = os.path.join(directory, LH_X_file)\n",
    "    \n",
    "    # Extract LH_X value from the file name (remove the \".txt\" extension)\n",
    "    LH_X = LH_X_file[:-4]\n",
    "    \n",
    "    # Initialize an empty dictionary to store variable names and their values\n",
    "    variable_data = {}\n",
    "\n",
    "    # Open the text file for reading\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Initialize variables to store the current variable name and its values\n",
    "        current_variable_name = None\n",
    "        current_variable_values = []\n",
    "\n",
    "        # Iterate over each line in the file\n",
    "        for line in file:\n",
    "            # Remove leading and trailing whitespace from the line\n",
    "            line = line.strip()\n",
    "\n",
    "            # Check if the line is empty\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Check if the line is a variable name\n",
    "            if line in ['phi', 'phi_sigma', 'hist', 'massBinLimits']:\n",
    "                # If it's a new variable name, update the current variable name and reset the values list\n",
    "                if current_variable_name is not None:\n",
    "                    variable_data[current_variable_name] = current_variable_values\n",
    "                    current_variable_values = []\n",
    "\n",
    "                current_variable_name = line\n",
    "            else:\n",
    "                # If it's not a variable name, convert the value to float and append it to the values list\n",
    "                current_variable_values.append(float(line))\n",
    "\n",
    "        # Add the last variable data to the dictionary\n",
    "        if current_variable_name is not None:\n",
    "            variable_data[current_variable_name] = current_variable_values\n",
    "        \n",
    "        # Extract specific variables\n",
    "        phi = variable_data.get('phi')\n",
    "        phi_sigma = variable_data.get('phi_sigma')\n",
    "        bins = variable_data.get('massBinLimits')\n",
    "\n",
    "        phia.append(phi)\n",
    "        phi_sigmaa.append(phi_sigma)\n",
    "        binsa.append(bins)\n",
    "        LH_X_values.append(LH_X)\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df_x = pd.DataFrame({'LH_X': LH_X_values, 'phi': phia, 'phi_sigma': phi_sigmaa, 'bins': binsa})\n",
    "\n",
    "# Display the DataFrame\n",
    "df_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x['phi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_x['phi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pandas series to np.array\n",
    "x = np.array(df_x['phi'].tolist())\n",
    "x.shape # shape 11 because they are in 11 bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy NPE\n",
    "This example attempts to infer 3 unknown parameters from a 20-dimensional 1D data vector using amortized posterior inference. We train the models from a simple synthetic catalog. This tutorial mirrors the same configuration as in [examples/toy_sbi.py](../examples/toy_sbi.py), but demonstrates how one would interact with the inference pipeline in a jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:04:55.105160Z",
     "start_time": "2023-11-03T19:04:55.058299Z"
    }
   },
   "outputs": [],
   "source": [
    "''' in tutorial:\n",
    "# create synthetic catalog\n",
    "def simulator(params): #Likelihood = P(y|given parameters)\n",
    "    # create toy simulations\n",
    "    x = np.arange(10)\n",
    "    y = 3 * params[0] * np.sin(x) + params[1] * x ** 2 - 2 * params[2] * x\n",
    "    y += np.random.randn(len(x))# add random noise\n",
    "    return y\n",
    "\n",
    "seed_sim = 12345\n",
    "np.random.seed(seed_sim)\n",
    "theta = np.random.rand(200, 3)  # 200 simulations, 3 parameters\n",
    "x = np.array([simulator(t) for t in theta])\n",
    "\n",
    "# make a dataloader\n",
    "loader = NumpyLoader(x=x, theta=theta)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataloader\n",
    "loader = NumpyLoader(x=x, theta=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info on prior:\n",
    "# prior for omega m, between 0.1 and 0.5\n",
    "# prior for sigma_8, between 0.6 and 1\n",
    "# prior for A_SN1, between 0.25 and 4\n",
    "# prior for A_AGN1, between 0.25 and 4\n",
    "# prior for A_SN2, between 0.5 and 2\n",
    "# prior for A_AGN2, between 0.5 and 2\n",
    "# prior for random seed, between 0 and 1000 (as there are 1000 sims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for omega m, between 0.1 and 0.5\n",
    "plt.hist(theta[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for sigma_8, between 0.6 and 1\n",
    "plt.hist(theta[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for A_SN1, between 0.25 and 4\n",
    "plt.hist(theta[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for A_AGN1, between 0.25 and 4\n",
    "plt.hist(theta[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for A_SN2, between 0.5 and 2\n",
    "plt.hist(theta[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for A_AGN2, between 0.5 and 2\n",
    "plt.hist(theta[:, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior for random seed, between 0 and 1000 (as there are 1000 sims)\n",
    "plt.hist(theta[:, 6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is your posterior - predicted value of the luminosity function...?\n",
    "# maybe log it\n",
    "plt.hist(x[:, 0])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:04:56.177736Z",
     "start_time": "2023-11-03T19:04:55.959360Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot some examples of the data\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for i in range(5):\n",
    "    ind = np.random.randint(len(theta))\n",
    "    ax.plot(x[ind], alpha=0.5, label=f'({theta[ind, 0]:.2f}, {theta[ind, 1]:.2f}, {theta[ind, 2]:.2f})')\n",
    "ax.set_yscale('log')\n",
    "ax.legend(title='theta')\n",
    "ax.set_title('Data vectors x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SBIRunner object will handle all of the data normalization and model training for us. We just need to provide it with:\n",
    "- our parameter prior\n",
    "- our inference type (SNPE/SNLE/SNRE)\n",
    "- our desired neural network architecture\n",
    "- our training hyperparameters\n",
    "\n",
    "On the backend, it does a validation split among the provided training data, trains the neural networks with an Adam optimizer, and enforces an early stopping criterion to prevent overfitting. All the parameters of these processes can be independently configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise Priors.\n",
    "# info on prior:\n",
    "# prior for omega m, between 0.1 and 0.5: uniform\n",
    "# prior for sigma_8, between 0.6 and 1: uniform\n",
    "# prior for A_SN1, between 0.25 and 4: exponential\n",
    "# prior for A_AGN1, between 0.25 and 4: exponential\n",
    "# prior for A_SN2, between 0.5 and 2: exponential\n",
    "# prior for A_AGN2, between 0.5 and 2: exponential\n",
    "# prior for random seed, between 0 and 1000 (as there are 1000 sims): uniform\n",
    "\n",
    "# This needs to be changed to match the shape of the prior for each of the parameters:\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Uniform, TransformedDistribution, ExpTransform\n",
    "\n",
    "class JointPrior:\n",
    "    def __init__(self, priors):\n",
    "        self.priors = priors\n",
    "    \n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        samples = [prior.sample(sample_shape) for prior in self.priors]\n",
    "        return torch.cat(samples, dim=-1)\n",
    "    \n",
    "    def log_prob(self, value):\n",
    "        log_probs = [prior.log_prob(value[..., i:i+1]) for i, prior in enumerate(self.priors)]\n",
    "        return torch.sum(torch.cat(log_probs, dim=-1), dim=-1)\n",
    "    \n",
    "    def get_priors(self):\n",
    "        return self.priors\n",
    "    \n",
    "def initialise_priors(device=\"cpu\", astro=True, dust=True): # cuda not available\n",
    "    combined_priors = []\n",
    "\n",
    "    if astro:\n",
    "        base_dist1 = Uniform(\n",
    "            torch.log(torch.tensor([0.25], device=device)),\n",
    "            torch.log(torch.tensor([4], device=device)),\n",
    "        )\n",
    "        base_dist2 = Uniform(\n",
    "            torch.log(torch.tensor([0.5], device=device)),\n",
    "            torch.log(torch.tensor([2], device=device)),\n",
    "        )\n",
    "        astro_prior1 = TransformedDistribution(base_dist1, ExpTransform())\n",
    "        astro_prior2 = TransformedDistribution(base_dist2, ExpTransform())\n",
    "        omega_prior = Uniform(\n",
    "            torch.tensor([0.1], device=device),\n",
    "            torch.tensor([0.5], device=device),\n",
    "        )\n",
    "        sigma8_prior = Uniform(\n",
    "            torch.tensor([0.6], device=device),\n",
    "            torch.tensor([1.0], device=device),\n",
    "        )\n",
    "        combined_priors += [\n",
    "            omega_prior,\n",
    "            sigma8_prior,\n",
    "            astro_prior1,\n",
    "            astro_prior2,\n",
    "            astro_prior1,\n",
    "            astro_prior2,\n",
    "        ]\n",
    "\n",
    "    joint_prior = JointPrior(combined_priors)\n",
    "\n",
    "    return joint_prior\n",
    "\n",
    "# Collect all priors in a list\n",
    "priors = initialise_priors()\n",
    "print(priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_priors = priors.get_priors()\n",
    "for i, prior in enumerate(individual_priors):\n",
    "    print(f\"Prior {i}: {prior}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# instantiate your neural networks to be used as an ensemble\n",
    "nets = [\n",
    "    ili.utils.load_nde_sbi(engine='NPE', model='maf', hidden_features=50, num_transforms=5),\n",
    "    ili.utils.load_nde_sbi(engine='NPE', model='mdn', hidden_features=50, num_components=6)\n",
    "]\n",
    "\n",
    "# define training arguments\n",
    "train_args = {\n",
    "    'training_batch_size': 32,\n",
    "    'learning_rate': 1e-4\n",
    "}\n",
    "\n",
    "# initialize the trainer\n",
    "runner = InferenceRunner.load(\n",
    "    backend='sbi',\n",
    "    engine='NPE',\n",
    "    prior=priors,\n",
    "    nets=nets,\n",
    "    device=device,\n",
    "    embedding_net=None,\n",
    "    train_args=train_args,\n",
    "    proposal=None,\n",
    "    out_dir=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:06:50.836068Z",
     "start_time": "2023-11-03T19:06:14.142618Z"
    }
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "posterior_ensemble, summaries = runner(loader=loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the output of the runner is a posterior model and a log of training statistics. The posterior model is a [NeuralPosteriorEnsemble](https://github.com/mackelab/sbi/blob/6c4fa7a6fd254d48d0c18640c832f2d80ab2257a/sbi/utils/posterior_ensemble.py#L19) model and automatically combines samples and probability densities from its component networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:06:52.252093Z",
     "start_time": "2023-11-03T19:06:52.042239Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot train/validation loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,4))\n",
    "c = list(mcolors.TABLEAU_COLORS)\n",
    "for i, m in enumerate(summaries):\n",
    "    ax.plot(m['training_log_probs'], ls='-', label=f\"{i}_train\", c=c[i])\n",
    "    ax.plot(m['validation_log_probs'], ls='--', label=f\"{i}_val\", c=c[i])\n",
    "ax.set_xlim(0)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Log probability')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of theta: {theta.shape}\")\n",
    "print(f\"Shape of x: {x.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:06:54.953647Z",
     "start_time": "2023-11-03T19:06:54.836424Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now, SBIRunner returns a custom class instance to be able to pass signature strings\n",
    "# This class has simply for attributes a NeuralPosteriorEstimate and a string list \n",
    "print(posterior_ensemble.signatures)\n",
    "\n",
    "# choose a random input\n",
    "seed_in = 49\n",
    "np.random.seed(seed_in)\n",
    "ind = np.random.randint(len(theta))\n",
    "\n",
    "# generate samples from the posterior using accept/reject sampling\n",
    "seed_samp = 32\n",
    "torch.manual_seed(seed_samp)\n",
    "samples = posterior_ensemble.sample((1000,), torch.Tensor(x[ind]).to(device))\n",
    "\n",
    "# calculate the log_prob for each sample\n",
    "log_prob = posterior_ensemble.log_prob(samples, torch.Tensor(x[ind]).to(device))\n",
    "\n",
    "samples = samples.cpu().numpy()\n",
    "log_prob = log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# samples is the posterior, P(theta_hat | y_i), conditioned on data y_i, generated from theta_i (prior)\n",
    "plt.hist(samples[:, 1])\n",
    "print('Prior:',theta[ind, 1])\n",
    "print('Histogram is Posterior:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:06:56.498564Z",
     "start_time": "2023-11-03T19:06:56.086003Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the posterior samples and the true value\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10,4), gridspec_kw={'width_ratios': [1,1,0.05]})\n",
    "for i in range(2):\n",
    "    axs[i].plot(theta[ind,i], theta[ind,i+1], 'r+', markersize=10, label='true')\n",
    "    im = axs[i].scatter(samples[:,i], samples[:,i+1], c=log_prob, s=4, label='samples')\n",
    "    axs[i].set_aspect('equal')\n",
    "    axs[i].set_xlim(0,1)\n",
    "    axs[i].set_ylim(0,1)\n",
    "    axs[i].set_xlabel(f'$\\\\theta_{i}$')\n",
    "    axs[i].set_ylabel(f'$\\\\theta_{i+1}$')\n",
    "    axs[i].legend()\n",
    "plt.colorbar(im, label='log probability', use_gridspec=True, cax=axs[2])\n",
    "print('true is from our prior, and theta0,1,2 are our possible outputs for the observed data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posterior samples and the true value for all pairs of theta values\n",
    "fig, axs = plt.subplots(7, 7, figsize=(15, 15))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for i in range(7):\n",
    "    for j in range(7):\n",
    "        if i != j:\n",
    "            axs[i, j].plot(theta[ind, i], theta[ind, j], 'r+', markersize=10, label='true')\n",
    "            im = axs[i, j].scatter(samples[:, i], samples[:, j], c=log_prob, s=4, label='samples', cmap='viridis')\n",
    "            axs[i, j].set_xlabel(f'$\\\\theta_{i}$')\n",
    "            axs[i, j].set_ylabel(f'$\\\\theta_{j}$')\n",
    "            axs[i, j].legend()\n",
    "        else:\n",
    "            axs[i, j].axis('off')\n",
    "\n",
    "# Add a color bar for log probability\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax, label='log probability')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('True values are marked with red pluses, and the pairs of theta values are plotted against each other.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:07:08.281307Z",
     "start_time": "2023-11-03T19:07:05.155804Z"
    }
   },
   "outputs": [],
   "source": [
    "# use ltu-ili's built-in validation metrics to plot the posterior for this point\n",
    "metric = PlotSinglePosterior(\n",
    "    num_samples=1000, sample_method='direct', \n",
    "    labels=[f'$\\\\theta_{i}$' for i in range(7)]\n",
    ")\n",
    "fig = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x_obs = x[ind], theta_fid=theta[ind]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the ensemble of trained posteriors models\n",
    "By default, running a SampleBasedMEtric with posterior from above will compute the metrics using the ensemble model. That is to say the ensemble is considered one model, with the weights of each posterior in the ensemble being the val_log_prob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:08:03.949691Z",
     "start_time": "2023-11-03T19:07:59.103834Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drawing samples from the ensemble posterior\n",
    "\n",
    "metric = PosteriorCoverage(\n",
    "    num_samples=1000, sample_method='direct', \n",
    "    labels=[f'$\\\\theta_{i}$' for i in range(7)],\n",
    "    plot_list = [\"coverage\", \"histogram\", \"predictions\", \"tarp\"],\n",
    "    out_dir=None\n",
    ")\n",
    "\n",
    "fig = metric(\n",
    "    posterior=posterior_ensemble, # NeuralPosteriorEnsemble instance from sbi package\n",
    "    x=x, theta=theta\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ensemble model, it looks like our posteriors are well-calibrated when evaluated on marginal distributions, but slightly negatively biased in the multivariate TARP coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating each trained posterior in the ensemble\n",
    "Below, we compute separately each SampleBasedMetric for every posterior in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:08:14.333383Z",
     "start_time": "2023-11-03T19:08:10.249645Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drawing samples for each posterior in the ensemble\n",
    "# First the MAF posterior\n",
    "metric = PosteriorCoverage(num_samples=1000, \n",
    "    sample_method='direct', labels=[f'$\\\\theta_{i}$' for i in range(3)],\n",
    "    plot_list = [\"coverage\", \"histogram\", \"predictions\", \"tarp\"])\n",
    "\n",
    "fig = metric(\n",
    "    posterior=posterior_ensemble.posteriors[0],\n",
    "    x=x, theta=theta\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:08:19.933411Z",
     "start_time": "2023-11-03T19:08:17.751688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Then for the MDN\n",
    "fig = metric(\n",
    "    posterior=posterior_ensemble.posteriors[1],\n",
    "    x=x, theta=theta\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we see that we are largely consistent and calibrated in the univariate coverage, with some slight negative bias shown in the multivariate coverage. It looks like the MAF model has slightly better constraints than the MDN model, while retaining the same calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Toy NLE\n",
    "This example uses the same dataset as the previous SNPE example, but uses a likelihood estimation model instead of an amortized posterior estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader and priors already defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:58:07.435823Z",
     "start_time": "2023-11-03T18:58:07.396124Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# instantiate your neural networks to be used as an ensemble\n",
    "nets = [\n",
    "    ili.utils.load_nde_sbi(engine='NLE', model='maf', hidden_features=50, num_transforms=5),\n",
    "    ili.utils.load_nde_sbi(engine='NLE', model='made', hidden_features=50, num_transforms=5),\n",
    "]\n",
    "\n",
    "# define training arguments\n",
    "train_args = {\n",
    "    'training_batch_size': 32,\n",
    "    'learning_rate': 5e-5\n",
    "}\n",
    "\n",
    "# initialize the trainer\n",
    "runner = InferenceRunner.load(\n",
    "    backend='sbi',\n",
    "    engine='NLE',\n",
    "    prior=priors,\n",
    "    nets=nets,\n",
    "    device=device,\n",
    "    embedding_net=None,\n",
    "    train_args=train_args,\n",
    "    proposal=None,\n",
    "    out_dir=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:59:18.553308Z",
     "start_time": "2023-11-03T18:58:10.815567Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train the model. this outputs a posterior model and training logs\n",
    "posterior_ensemble, summaries = runner(loader=loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:59:19.290472Z",
     "start_time": "2023-11-03T18:59:18.556504Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot train/validation loss\n",
    "f, ax = plt.subplots(1, 1, figsize=(6,4))\n",
    "c = list(mcolors.TABLEAU_COLORS)\n",
    "for i, m in enumerate(summaries):\n",
    "    ax.plot(m['training_log_probs'], ls='-', label=f\"{i}_train\", c=c[i])\n",
    "    ax.plot(m['validation_log_probs'], ls='--', label=f\"{i}_val\", c=c[i])\n",
    "ax.set_xlim(0)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Log probability')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:00:49.010526Z",
     "start_time": "2023-11-03T18:59:19.291746Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# choose a random input\n",
    "seed_ind = 24\n",
    "np.random.seed(seed_ind)\n",
    "ind = np.random.randint(len(theta))\n",
    "\n",
    "# generate samples from the posterior using MCMC\n",
    "samples = posterior_ensemble.sample(\n",
    "    (1000,), x[ind], \n",
    "    method='slice_np_vectorized', num_chains=6\n",
    ").detach().cpu().numpy()\n",
    "\n",
    "# calculate the potential (prop. to log_prob) for each sample\n",
    "log_prob = posterior_ensemble.log_prob(\n",
    "    samples, \n",
    "    x[ind]\n",
    ").detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note: Amortized liklihood estimators do not directly estimate the posterior, but instead build a model for the relative likelihood. This can be combined with a prior to estimate the potential function, which is proportional to the log_probability up to a normalizing constant. Hence, we use MCMC sampling for SNLE models to probe the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:00:58.224129Z",
     "start_time": "2023-11-03T19:00:57.821996Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot the posterior samples and the true value\n",
    "f, axs = plt.subplots(1, 3, figsize=(10,4), gridspec_kw={'width_ratios': [1,1,0.05]})\n",
    "for i in range(2):\n",
    "    axs[i].plot(theta[ind,i], theta[ind,i+1], 'r+', markersize=10, label='true')\n",
    "    im = axs[i].scatter(samples[:,i], samples[:,i+1], c=log_prob, s=4, label='samples')\n",
    "    axs[i].set_aspect('equal')\n",
    "    axs[i].set_xlim(0,1)\n",
    "    axs[i].set_ylim(0,1)\n",
    "    axs[i].set_xlabel(f'$\\\\theta_{i}$')\n",
    "    axs[i].set_ylabel(f'$\\\\theta_{i+1}$')\n",
    "    axs[i].legend()\n",
    "plt.colorbar(im, label='potential', use_gridspec=True, cax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### (We will sample only from the ensemble posterior in this section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:04:23.382763Z",
     "start_time": "2023-11-03T19:02:28.439726Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# use ltu-ili's built-in validation metrics to plot the posterior for this point\n",
    "metric = PosteriorCoverage(\n",
    "    num_samples=1000, sample_method='vi',\n",
    "    sample_params={'dist': 'maf', 'n_particles': 32, 'learning_rate': 1e-2},\n",
    "    labels=[f'$\\\\theta_{i}$' for i in range(7)],\n",
    "    plot_list = [\"coverage\", \"histogram\", \"predictions\", \"tarp\"]\n",
    ")\n",
    "\n",
    "# Drawing samples from the ensemble posterior\n",
    "fig = metric(\n",
    "    posterior=posterior_ensemble, # NeuralPosteriorEnsemble instance from sbi package\n",
    "    x=x, theta=theta\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Looks like we're well-calibrated in both univariate and multivariate tests, with much tighter constraints on $\\theta_1$ than the SNPE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAMELS CMD 2D NPE\n",
    "The real utility of ltu-ili is in its flexibility for applicaiton to a wide variety of real datasets. Through the use of customizable embedding networks, this framework can be extended to handle e.g. image, point cloud, or time-series inputs.\n",
    "\n",
    "In this example, our inputs are 2D slices of the gas temperature of the simulations in the [CAMELS Multifield Dataset](https://camels-multifield-dataset.readthedocs.io/en/latest/) and our target parameters are 6 cosmological and astrophysical parameters (Omega_m, sigma_8, A_SN1, A_AGN1, A_SN2, A_AGN2) that were used to run the simulation. This inference pipeline follows the procedure of the [CAMELS 2D Inference colab example](https://colab.research.google.com/drive/1-BmkA8JSc36O8g9pj7FenD1YSLKqjQR3?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "# Download CAMELS 2D maps and latin hypercube parameters (~3.2 GB)\n",
    "import os\n",
    "os.makedirs('./toy', exist_ok=True)\n",
    "!curl https://users.flatironinstitute.org/~fvillaescusa/priv/DEPnzxoWlaTQ6CjrXqsm0vYi8L7Jy/CMD/2D_maps/data/Maps_T_IllustrisTNG_LH_z=0.00.npy -o ./toy/Maps_T_IllustrisTNG_LH_z=0.00.npy\n",
    "!curl https://users.flatironinstitute.org/~fvillaescusa/priv/DEPnzxoWlaTQ6CjrXqsm0vYi8L7Jy/CMD/2D_maps/data/params_LH_IllustrisTNG.txt -o ./toy/params_LH_IllustrisTNG.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:08:54.331712Z",
     "start_time": "2023-11-03T19:08:42.947883Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "x = np.log10(np.load('./toy/Maps_T_IllustrisTNG_LH_z=0.00.npy'))\n",
    "theta = np.loadtxt('./toy/params_LH_IllustrisTNG.txt')\n",
    "theta = np.repeat(theta, 15, axis=0)\n",
    "\n",
    "# subsample (for speed)\n",
    "x = x[::2]\n",
    "theta = theta[::2]\n",
    "\n",
    "# conform images to pytorch expected shape\n",
    "x = x[:,None,...]\n",
    "\n",
    "# make a dataloader\n",
    "loader = NumpyLoader(x=x, theta=theta)\n",
    "\n",
    "# determine prior bounds\n",
    "prior_min = theta.min(axis=0)\n",
    "prior_max = theta.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:09:17.198719Z",
     "start_time": "2023-11-03T19:09:16.185035Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = [r'$\\Omega_m$', r'$\\sigma_8$', r'$A_{SN1}$',\n",
    "          r'$A_{AGN1}$', r'$A_{SN2}$', r'$A_{AGN2}$']\n",
    "\n",
    "# plot a few examples of the data\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i in range(4):\n",
    "    ind = np.random.randint(len(x))\n",
    "    im = axs[i].imshow(x[ind,0], cmap='RdBu_r')\n",
    "\n",
    "    title = [f'{labels[j]}={theta[ind,j]:.2f}' for j in range(6)]\n",
    "    title.insert(3, '\\n')\n",
    "    axs[i].set_title(' '.join(title))\n",
    "    axs[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:09:21.119576Z",
     "start_time": "2023-11-03T19:09:21.081910Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's build a convolutional embedding network for processing images\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=8, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layers(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc_layers(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:09:22.946536Z",
     "start_time": "2023-11-03T19:09:22.905935Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a prior\n",
    "prior = ili.utils.Uniform(low=prior_min, high=prior_max, device=device)\n",
    "\n",
    "# instantiate a CNN embedding network\n",
    "embedding_net = ConvNet()\n",
    "\n",
    "# instantiate your neural networks to be used as an ensemble\n",
    "nets = [\n",
    "    ili.utils.load_nde_sbi(engine='NPE', model='maf', hidden_features=50, num_transforms=5,\n",
    "                           embedding_net=embedding_net),\n",
    "    ili.utils.load_nde_sbi(engine='NPE', model='mdn', hidden_features=50, num_components=6,\n",
    "                           embedding_net=embedding_net)\n",
    "]\n",
    "\n",
    "# define training arguments\n",
    "train_args = {\n",
    "    'training_batch_size': 32,\n",
    "    'learning_rate': 5e-5,\n",
    "    'stop_after_epochs': 50\n",
    "}\n",
    "\n",
    "# initialize the trainer\n",
    "runner = InferenceRunner.load(\n",
    "    backend='sbi',\n",
    "    engine='NPE',\n",
    "    prior=prior,\n",
    "    nets=nets,\n",
    "    device=device,\n",
    "    embedding_net=embedding_net,\n",
    "    train_args=train_args,\n",
    "    proposal=None,\n",
    "    out_dir=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:30:05.429934Z",
     "start_time": "2023-11-03T19:09:23.917845Z"
    }
   },
   "outputs": [],
   "source": [
    "# train the model. this outputs a posterior model and training logs\n",
    "posterior_ensemble, summaries = runner(loader=loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:35:59.524281Z",
     "start_time": "2023-11-03T19:35:58.772214Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot train/validation loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,4))\n",
    "c = list(mcolors.TABLEAU_COLORS)\n",
    "for i, m in enumerate(summaries):\n",
    "    ax.plot(m['training_log_probs'], ls='-', label=f\"{i}_train\", c=c[i])\n",
    "    ax.plot(m['validation_log_probs'], ls='--', label=f\"{i}_val\", c=c[i])\n",
    "ax.set_xlim(0)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Log probability')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:37:30.567488Z",
     "start_time": "2023-11-03T19:37:30.371169Z"
    }
   },
   "outputs": [],
   "source": [
    "# choose a random input\n",
    "ind = np.random.randint(len(theta))\n",
    "x_test = torch.Tensor(x[ind]).to(device)\n",
    "\n",
    "# generate samples from the posterior using accept/reject sampling\n",
    "samples = posterior_ensemble.sample((1000,), x_test)\n",
    "# calculate the log_prob for each sample\n",
    "log_prob = posterior_ensemble.log_prob(samples, x_test)\n",
    "\n",
    "samples = samples.cpu().numpy()\n",
    "log_prob = log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:37:31.692426Z",
     "start_time": "2023-11-03T19:37:31.233527Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the posterior samples and the true value\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10,4), gridspec_kw={'width_ratios': [1,1,0.05]})\n",
    "for i in range(2):\n",
    "    axs[i].plot(theta[ind,i], theta[ind,i+1], 'r+', markersize=10, label='true')\n",
    "    im = axs[i].scatter(samples[:,i], samples[:,i+1], c=log_prob, s=4, label='samples')\n",
    "    axs[i].set_xlim(prior_min[i], prior_max[i])\n",
    "    axs[i].set_ylim(prior_min[i+1], prior_max[i+1])\n",
    "    axs[i].set_xlabel(labels[i])\n",
    "    axs[i].set_ylabel(labels[i+1])\n",
    "    axs[i].legend()\n",
    "plt.colorbar(im, label='log probability', use_gridspec=True, cax=axs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:37:47.913525Z",
     "start_time": "2023-11-03T19:37:35.055855Z"
    }
   },
   "outputs": [],
   "source": [
    "# use ltu-ili's built-in validation metrics to plot the posterior for this point\n",
    "metric = PlotSinglePosterior(\n",
    "    num_samples=1000, sample_method='direct', \n",
    "    labels=labels, out_dir=None\n",
    ")\n",
    "fig = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x_obs = x[ind], theta_fid=theta[ind],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:39:48.152159Z",
     "start_time": "2023-11-03T19:39:19.823250Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate and plot the rank statistics to describe univariate posterior coverage\n",
    "metric = PosteriorCoverage(\n",
    "    num_samples=1000, sample_method='direct',\n",
    "    labels=labels, plot_list = [\"coverage\", \"histogram\", \"predictions\", \"tarp\"]\n",
    ")\n",
    "fig = metric(\n",
    "    posterior=posterior_ensemble,\n",
    "    x=x[::8], theta=theta[::8]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we are picking up on clear information on $\\Omega_m$ and some on $\\sigma_8$ and $A_\\mathrm{SN1}$, but not much on the other parameters. This mimics the finding of the CAMELS CMD example, albeit with worse constraints (likely since we have not optimized our CNN architecture). We can see that in the parameters which we can constrain well, we are fairly well-calibrated, but on the whole our posteriors are slightly negatively biased. This is likely the result of the limited prior range, which is cutting off the tails of the posterior for the unconstrained variables.\n",
    "\n",
    "Nonetheless, this demonstrates how the ltu-ili framework can be used to perform inference on real datasets using customizable embedding networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ili-torch",
   "language": "python",
   "name": "ili-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
