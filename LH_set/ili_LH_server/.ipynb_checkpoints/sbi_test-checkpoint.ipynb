{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e456c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbi example taken from ltu-ili examples - understanding ili \n",
    "# simpler example to play with parameters/training input/arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4af7a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Uniform, ExpTransform, TransformedDistribution #, AffineTransform\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import ili\n",
    "from ili.dataloaders import StaticNumpyLoader\n",
    "from ili.inference import InferenceRunner\n",
    "from ili.validation import ValidationRunner\n",
    "from ili.dataloaders import NumpyLoader\n",
    "from ili.inference import InferenceRunner\n",
    "from ili.validation.metrics import PosteriorCoverage, PlotSinglePosterior\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4e18c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.309   0.979   3.11234 1.12194 0.6685  0.53182]\n",
      " [0.3026  0.9394  3.42001 3.96137 1.03311 1.1607 ]\n",
      " [0.4282  0.753   0.70613 0.37423 1.96292 0.6272 ]\n",
      " ...\n",
      " [0.1582  0.7854  0.93952 2.23148 1.85446 1.32961]\n",
      " [0.3854  0.9778  0.93692 0.42869 1.48761 1.32042]\n",
      " [0.4322  0.6142  1.28521 0.35799 1.1591  0.86754]]\n",
      "(1000, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1882/4189281711.py:2: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df_pars = pd.read_csv('/home/jovyan/camels/LH/CosmoAstroSeed_IllustrisTNG_L25n256_LH.txt', delim_whitespace=True)\n"
     ]
    }
   ],
   "source": [
    "# Get theta\n",
    "df_pars = pd.read_csv('/home/jovyan/camels/LH/CosmoAstroSeed_IllustrisTNG_L25n256_LH.txt', delim_whitespace=True)\n",
    "theta = df_pars[['Omega_m', 'sigma_8', 'A_SN1', 'A_AGN1', 'A_SN2', 'A_AGN2']].to_numpy()\n",
    "print(theta)\n",
    "print(theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "924e3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_priors(device=\"cpu\", astro=True, dust=True):\n",
    "\n",
    "    combined_priors = []\n",
    "\n",
    "    if astro:\n",
    "        base_dist1 = Uniform(\n",
    "            torch.log(torch.tensor([0.25], device=device)),\n",
    "            torch.log(torch.tensor([4], device=device)),\n",
    "        )\n",
    "        base_dist2 = Uniform(\n",
    "            torch.log(torch.tensor([0.5], device=device)),\n",
    "            torch.log(torch.tensor([2], device=device)),\n",
    "        )\n",
    "        astro_prior1 = TransformedDistribution(base_dist1, ExpTransform())\n",
    "        astro_prior2 = TransformedDistribution(base_dist2, ExpTransform())\n",
    "        omega_prior = Uniform(\n",
    "            torch.tensor([0.1], device=device),\n",
    "            torch.tensor([0.5], device=device),\n",
    "        )\n",
    "        sigma8_prior = Uniform(\n",
    "            torch.tensor([0.6], device=device),\n",
    "            torch.tensor([1.0], device=device),\n",
    "        )\n",
    "        combined_priors += [\n",
    "            omega_prior,# prior for omega m, between 0.1 and 0.5: uniform\n",
    "            sigma8_prior,# prior for sigma_8, between 0.6 and 1: uniform\n",
    "            astro_prior1,# prior for A_SN1, between 0.25 and 4: exponential\n",
    "            astro_prior1,# prior for A_AGN1, between 0.25 and 4: exponential\n",
    "            astro_prior2,# prior for A_SN2, between 0.5 and 2: exponential\n",
    "            astro_prior2,# prior for A_AGN2, between 0.5 and 2: exponential\n",
    "        ]\n",
    "\n",
    "    prior = process_prior(combined_priors)\n",
    "\n",
    "    return prior[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c0a732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultipleIndependent()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/sbi/utils/user_input_checks.py:76: UserWarning: Prior was provided as a sequence of 6 priors. They will be\n",
      "            interpreted as independent of each other and matched in order to the\n",
      "            components of the parameter.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/sbi/utils/user_input_checks.py:209: UserWarning: Casting 1D Uniform prior to BoxUniform to match sbi batch requirements.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Uniform, ExpTransform, TransformedDistribution #, AffineTransform\n",
    "import torch\n",
    "from sbi.utils.user_input_checks import process_prior\n",
    "\n",
    "prior = initialise_priors()\n",
    "print(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0374e5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get x\n",
    "\n",
    "# Define the directory containing the LH_X files\n",
    "directory = \"/home/jovyan/camels/LH/get_LF/output/\"\n",
    "\n",
    "# Get all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Filter out files that start with \"LH_\" and end with \".txt\"\n",
    "LH_X_files = [file for file in files if file.startswith(\"LH_\") and file.endswith(\".txt\")]\n",
    "\n",
    "# Initialize lists to store data\n",
    "phia = []\n",
    "phi_sigmaa = []\n",
    "binsa = []\n",
    "LH_X_values = []\n",
    "\n",
    "# Iterate over LH_X files\n",
    "for LH_X_file in LH_X_files:\n",
    "    # Define the file path\n",
    "    file_path = os.path.join(directory, LH_X_file)\n",
    "    \n",
    "    # Extract LH_X value from the file name (remove the \".txt\" extension)\n",
    "    LH_X = LH_X_file[:-4]\n",
    "    \n",
    "    # Initialize an empty dictionary to store variable names and their values\n",
    "    variable_data = {}\n",
    "\n",
    "    # Open the text file for reading\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Initialize variables to store the current variable name and its values\n",
    "        current_variable_name = None\n",
    "        current_variable_values = []\n",
    "\n",
    "        # Iterate over each line in the file\n",
    "        for line in file:\n",
    "            # Remove leading and trailing whitespace from the line\n",
    "            line = line.strip()\n",
    "\n",
    "            # Check if the line is empty\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Check if the line is a variable name\n",
    "            if line in ['phi', 'phi_sigma', 'hist', 'massBinLimits']:\n",
    "                # If it's a new variable name, update the current variable name and reset the values list\n",
    "                if current_variable_name is not None:\n",
    "                    variable_data[current_variable_name] = current_variable_values\n",
    "                    current_variable_values = []\n",
    "\n",
    "                current_variable_name = line\n",
    "            else:\n",
    "                # If it's not a variable name, convert the value to float and append it to the values list\n",
    "                current_variable_values.append(float(line))\n",
    "\n",
    "        # Add the last variable data to the dictionary\n",
    "        if current_variable_name is not None:\n",
    "            variable_data[current_variable_name] = current_variable_values\n",
    "        \n",
    "        # Extract specific variables\n",
    "        phi = variable_data.get('phi')\n",
    "        phi_sigma = variable_data.get('phi_sigma')\n",
    "        bins = variable_data.get('massBinLimits')\n",
    "\n",
    "        phia.append(phi)\n",
    "        phi_sigmaa.append(phi_sigma)\n",
    "        binsa.append(bins)\n",
    "        LH_X_values.append(LH_X)\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df_x = pd.DataFrame({'LH_X': LH_X_values, 'phi': phia, 'phi_sigma': phi_sigmaa, 'bins': binsa})\n",
    "\n",
    "# convert pandas series to np.array to get x\n",
    "x = np.array(df_x['phi'].tolist())\n",
    "x.shape # shape 11 because they are in 11 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5057afca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPE\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Run SBI inference for toy data.\")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--model\", type=str,\n",
    "    default=\"NPE\",\n",
    "    help=\"Configuration file to use for model training.\")\n",
    "\n",
    "# Filter out unrecognized arguments to avoid errors in Jupyter\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "print(args.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22d9ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a working directory\n",
    "if not os.path.isdir(\"toy\"):\n",
    "    os.mkdir(\"toy\")\n",
    "\n",
    "# push the as numpy files\n",
    "np.save(\"toy/theta.npy\", theta)\n",
    "np.save(\"toy/x.npy\", x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c1c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataloader\n",
    "from ili.dataloaders import NumpyLoader\n",
    "\n",
    "loader = NumpyLoader(x=x, theta=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c68989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:MODEL INFERENCE CLASS: NPE\n",
      "INFO:root:Training model 1 / 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 88 epochs."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training model 2 / 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training neural network. Epochs trained: 5"
     ]
    }
   ],
   "source": [
    "# instantiate your neural networks to be used as an ensemble\n",
    "nets = [\n",
    "    ili.utils.load_nde_sbi(engine='NPE', model='maf', hidden_features=50, num_transforms=5),\n",
    "    ili.utils.load_nde_sbi(engine='NPE', model='mdn', hidden_features=50, num_components=6)\n",
    "]\n",
    "\n",
    "# define training arguments\n",
    "train_args = {\n",
    "    'training_batch_size': 32,\n",
    "    'learning_rate': 1e-4\n",
    "}\n",
    "\n",
    "# initialize the trainer\n",
    "runner = InferenceRunner.load(\n",
    "    backend='sbi',\n",
    "    engine='NPE',\n",
    "    prior=prior,\n",
    "    nets=nets,\n",
    "    device=device,\n",
    "    embedding_net=None,\n",
    "    train_args=train_args,\n",
    "    proposal=None,\n",
    "    out_dir=None\n",
    ")\n",
    "\n",
    "# run the trainer\n",
    "runner(loader=loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27625301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the trained posterior model to predict on a single example from\n",
    "# the test set\n",
    "val_runner = ValidationRunner.from_config(\n",
    "    f\"configs/val/toy_sbi_{args.model}.yaml\")\n",
    "\n",
    "val_runner(loader=all_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361ffd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254493b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
