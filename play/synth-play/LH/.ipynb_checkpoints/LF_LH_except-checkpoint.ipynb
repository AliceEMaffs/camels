{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute magnitude is defined to be the apparent magnitude an object would have if it were located at a distance of 10 parsecs.\n",
    "In astronomy, absolute magnitude (M) is a measure of the luminosity of a celestial object on an inverse logarithmic astronomical magnitude scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8154/2918952320.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/jovyan/camels/play/synth-play/LH\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from unyt import yr, Myr, kpc, arcsec, nJy, Mpc, Msun, erg, s, Hz\n",
    "from astropy.cosmology import Planck18 as cosmo\n",
    "from scipy import signal\n",
    "import os \n",
    "import csv\n",
    "import resource\n",
    "import pickle\n",
    "import shutil\n",
    "import json\n",
    "from synthesizer.grid import Grid\n",
    "from synthesizer.parametric import SFH, ZDist\n",
    "from synthesizer.particle.stars import sample_sfhz\n",
    "from synthesizer.parametric import Stars as ParametricStars\n",
    "from synthesizer.particle.particles import CoordinateGenerator\n",
    "from synthesizer.filters import Filter, FilterCollection\n",
    "from synthesizer.sed import combine_list_of_seds\n",
    "\n",
    "from synthesizer.load_data.load_camels import load_CAMELS_IllustrisTNG\n",
    "from synthesizer.kernel_functions import Kernel\n",
    "\n",
    "from synthesizer.conversions import lnu_to_absolute_mag\n",
    "\n",
    "# to clear output\n",
    "from IPython.display import clear_output\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current directory:\", current_directory)\n",
    "# Start the timer\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Alternative method for LF:\n",
    "# try this method again, but using AB mag instead of mass, and suply your own bins (up to -17, say)\n",
    "def calc_lf(ab_mag, volume, massBinLimits):\n",
    "# OG:        hist, dummy = np.histogram(np.log10(mstar), bins = massBinLimits)\n",
    "        hist, dummy = np.histogram(ab_mag, bins = massBinLimits)\n",
    "        hist = np.float64(hist)\n",
    "        phi = (hist / volume) / (massBinLimits[1] - massBinLimits[0])\n",
    "        phi_sigma = (np.sqrt(hist) / volume) /\\\n",
    "                    (massBinLimits[1] - massBinLimits[0]) # Poisson errors\n",
    "        return phi, phi_sigma, hist\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "phi_arr =[] #phi\n",
    "phi_sigma_arr =[] # phi_sigma\n",
    "hist_arr = [] # hist\n",
    "z_arr = [] #redshift_074, \n",
    "abs_mag_arr = [] #absolute mag (th filter)\n",
    "Vcom_arr = [] # comoving vol\n",
    "massBinLimits_arr = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(phi_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LH_560',\n",
       " 'LH_561',\n",
       " 'LH_562',\n",
       " 'LH_563',\n",
       " 'LH_564',\n",
       " 'LH_565',\n",
       " 'LH_566',\n",
       " 'LH_567',\n",
       " 'LH_568',\n",
       " 'LH_569',\n",
       " 'LH_570',\n",
       " 'LH_571',\n",
       " 'LH_572',\n",
       " 'LH_573',\n",
       " 'LH_574',\n",
       " 'LH_575',\n",
       " 'LH_576',\n",
       " 'LH_577',\n",
       " 'LH_578',\n",
       " 'LH_579',\n",
       " 'LH_580',\n",
       " 'LH_581',\n",
       " 'LH_582',\n",
       " 'LH_583',\n",
       " 'LH_584',\n",
       " 'LH_585',\n",
       " 'LH_586',\n",
       " 'LH_587',\n",
       " 'LH_588',\n",
       " 'LH_589',\n",
       " 'LH_590',\n",
       " 'LH_591',\n",
       " 'LH_592',\n",
       " 'LH_593',\n",
       " 'LH_594',\n",
       " 'LH_595',\n",
       " 'LH_596',\n",
       " 'LH_597',\n",
       " 'LH_598',\n",
       " 'LH_599',\n",
       " 'LH_600',\n",
       " 'LH_601',\n",
       " 'LH_602',\n",
       " 'LH_603',\n",
       " 'LH_604',\n",
       " 'LH_605',\n",
       " 'LH_606',\n",
       " 'LH_607',\n",
       " 'LH_608',\n",
       " 'LH_609',\n",
       " 'LH_610',\n",
       " 'LH_611',\n",
       " 'LH_612',\n",
       " 'LH_613',\n",
       " 'LH_614',\n",
       " 'LH_615',\n",
       " 'LH_616',\n",
       " 'LH_617',\n",
       " 'LH_618',\n",
       " 'LH_619',\n",
       " 'LH_620',\n",
       " 'LH_621',\n",
       " 'LH_622',\n",
       " 'LH_623',\n",
       " 'LH_624',\n",
       " 'LH_625',\n",
       " 'LH_626',\n",
       " 'LH_627',\n",
       " 'LH_628',\n",
       " 'LH_629',\n",
       " 'LH_630',\n",
       " 'LH_631',\n",
       " 'LH_632',\n",
       " 'LH_633',\n",
       " 'LH_634',\n",
       " 'LH_635',\n",
       " 'LH_636',\n",
       " 'LH_637',\n",
       " 'LH_638',\n",
       " 'LH_639',\n",
       " 'LH_640',\n",
       " 'LH_641',\n",
       " 'LH_642',\n",
       " 'LH_643',\n",
       " 'LH_644',\n",
       " 'LH_645',\n",
       " 'LH_646',\n",
       " 'LH_647',\n",
       " 'LH_648',\n",
       " 'LH_649',\n",
       " 'LH_650',\n",
       " 'LH_651',\n",
       " 'LH_652',\n",
       " 'LH_653',\n",
       " 'LH_654',\n",
       " 'LH_655',\n",
       " 'LH_656',\n",
       " 'LH_657',\n",
       " 'LH_658',\n",
       " 'LH_659',\n",
       " 'LH_660',\n",
       " 'LH_661',\n",
       " 'LH_662',\n",
       " 'LH_663',\n",
       " 'LH_664',\n",
       " 'LH_665',\n",
       " 'LH_666',\n",
       " 'LH_667',\n",
       " 'LH_668',\n",
       " 'LH_669',\n",
       " 'LH_670',\n",
       " 'LH_671',\n",
       " 'LH_672',\n",
       " 'LH_673',\n",
       " 'LH_674',\n",
       " 'LH_675',\n",
       " 'LH_676',\n",
       " 'LH_677',\n",
       " 'LH_678',\n",
       " 'LH_679',\n",
       " 'LH_680',\n",
       " 'LH_681',\n",
       " 'LH_682',\n",
       " 'LH_683',\n",
       " 'LH_684',\n",
       " 'LH_685',\n",
       " 'LH_686',\n",
       " 'LH_687',\n",
       " 'LH_688',\n",
       " 'LH_689',\n",
       " 'LH_690',\n",
       " 'LH_691',\n",
       " 'LH_692',\n",
       " 'LH_693',\n",
       " 'LH_694',\n",
       " 'LH_695',\n",
       " 'LH_696',\n",
       " 'LH_697',\n",
       " 'LH_698',\n",
       " 'LH_699',\n",
       " 'LH_700',\n",
       " 'LH_701',\n",
       " 'LH_702',\n",
       " 'LH_703',\n",
       " 'LH_704',\n",
       " 'LH_705',\n",
       " 'LH_706',\n",
       " 'LH_707',\n",
       " 'LH_708',\n",
       " 'LH_709',\n",
       " 'LH_710',\n",
       " 'LH_711',\n",
       " 'LH_712',\n",
       " 'LH_713',\n",
       " 'LH_714',\n",
       " 'LH_715',\n",
       " 'LH_716',\n",
       " 'LH_717',\n",
       " 'LH_718',\n",
       " 'LH_719',\n",
       " 'LH_720',\n",
       " 'LH_721',\n",
       " 'LH_722',\n",
       " 'LH_723',\n",
       " 'LH_724',\n",
       " 'LH_725',\n",
       " 'LH_726',\n",
       " 'LH_727',\n",
       " 'LH_728',\n",
       " 'LH_729',\n",
       " 'LH_730',\n",
       " 'LH_731',\n",
       " 'LH_732',\n",
       " 'LH_733',\n",
       " 'LH_734',\n",
       " 'LH_735',\n",
       " 'LH_736',\n",
       " 'LH_737',\n",
       " 'LH_738',\n",
       " 'LH_739',\n",
       " 'LH_740',\n",
       " 'LH_741',\n",
       " 'LH_742',\n",
       " 'LH_743',\n",
       " 'LH_744',\n",
       " 'LH_745',\n",
       " 'LH_746',\n",
       " 'LH_747',\n",
       " 'LH_748',\n",
       " 'LH_749',\n",
       " 'LH_750',\n",
       " 'LH_751',\n",
       " 'LH_752',\n",
       " 'LH_753',\n",
       " 'LH_754',\n",
       " 'LH_755',\n",
       " 'LH_756',\n",
       " 'LH_757',\n",
       " 'LH_758',\n",
       " 'LH_759',\n",
       " 'LH_760',\n",
       " 'LH_761',\n",
       " 'LH_762',\n",
       " 'LH_763',\n",
       " 'LH_764',\n",
       " 'LH_765',\n",
       " 'LH_766',\n",
       " 'LH_767',\n",
       " 'LH_768',\n",
       " 'LH_769',\n",
       " 'LH_770',\n",
       " 'LH_771',\n",
       " 'LH_772',\n",
       " 'LH_773',\n",
       " 'LH_774',\n",
       " 'LH_775',\n",
       " 'LH_776',\n",
       " 'LH_777',\n",
       " 'LH_778',\n",
       " 'LH_779',\n",
       " 'LH_780',\n",
       " 'LH_781',\n",
       " 'LH_782',\n",
       " 'LH_783',\n",
       " 'LH_784',\n",
       " 'LH_785',\n",
       " 'LH_786',\n",
       " 'LH_787',\n",
       " 'LH_788',\n",
       " 'LH_789',\n",
       " 'LH_790',\n",
       " 'LH_791',\n",
       " 'LH_792',\n",
       " 'LH_793',\n",
       " 'LH_794',\n",
       " 'LH_795',\n",
       " 'LH_796',\n",
       " 'LH_797',\n",
       " 'LH_798',\n",
       " 'LH_799',\n",
       " 'LH_800',\n",
       " 'LH_801',\n",
       " 'LH_802',\n",
       " 'LH_803',\n",
       " 'LH_804',\n",
       " 'LH_805',\n",
       " 'LH_806',\n",
       " 'LH_807',\n",
       " 'LH_808',\n",
       " 'LH_809',\n",
       " 'LH_810',\n",
       " 'LH_811',\n",
       " 'LH_812',\n",
       " 'LH_813',\n",
       " 'LH_814',\n",
       " 'LH_815',\n",
       " 'LH_816',\n",
       " 'LH_817',\n",
       " 'LH_818',\n",
       " 'LH_819',\n",
       " 'LH_820',\n",
       " 'LH_821',\n",
       " 'LH_822',\n",
       " 'LH_823',\n",
       " 'LH_824',\n",
       " 'LH_825',\n",
       " 'LH_826',\n",
       " 'LH_827',\n",
       " 'LH_828',\n",
       " 'LH_829',\n",
       " 'LH_830',\n",
       " 'LH_831',\n",
       " 'LH_832',\n",
       " 'LH_833',\n",
       " 'LH_834',\n",
       " 'LH_835',\n",
       " 'LH_836',\n",
       " 'LH_837',\n",
       " 'LH_838',\n",
       " 'LH_839',\n",
       " 'LH_840',\n",
       " 'LH_841',\n",
       " 'LH_842',\n",
       " 'LH_843',\n",
       " 'LH_844',\n",
       " 'LH_845',\n",
       " 'LH_846',\n",
       " 'LH_847',\n",
       " 'LH_848',\n",
       " 'LH_849',\n",
       " 'LH_850',\n",
       " 'LH_851',\n",
       " 'LH_852',\n",
       " 'LH_853',\n",
       " 'LH_854',\n",
       " 'LH_855',\n",
       " 'LH_856',\n",
       " 'LH_857',\n",
       " 'LH_858',\n",
       " 'LH_859',\n",
       " 'LH_860',\n",
       " 'LH_861',\n",
       " 'LH_862',\n",
       " 'LH_863',\n",
       " 'LH_864',\n",
       " 'LH_865',\n",
       " 'LH_866',\n",
       " 'LH_867',\n",
       " 'LH_868',\n",
       " 'LH_869',\n",
       " 'LH_870',\n",
       " 'LH_871',\n",
       " 'LH_872',\n",
       " 'LH_873',\n",
       " 'LH_874',\n",
       " 'LH_875',\n",
       " 'LH_876',\n",
       " 'LH_877',\n",
       " 'LH_878',\n",
       " 'LH_879',\n",
       " 'LH_880',\n",
       " 'LH_881',\n",
       " 'LH_882',\n",
       " 'LH_883',\n",
       " 'LH_884',\n",
       " 'LH_885',\n",
       " 'LH_886',\n",
       " 'LH_887',\n",
       " 'LH_888',\n",
       " 'LH_889',\n",
       " 'LH_890',\n",
       " 'LH_891',\n",
       " 'LH_892',\n",
       " 'LH_893',\n",
       " 'LH_894',\n",
       " 'LH_895',\n",
       " 'LH_896',\n",
       " 'LH_897',\n",
       " 'LH_898',\n",
       " 'LH_899',\n",
       " 'LH_900',\n",
       " 'LH_901',\n",
       " 'LH_902',\n",
       " 'LH_903',\n",
       " 'LH_904',\n",
       " 'LH_905',\n",
       " 'LH_906',\n",
       " 'LH_907',\n",
       " 'LH_908',\n",
       " 'LH_909',\n",
       " 'LH_910',\n",
       " 'LH_911',\n",
       " 'LH_912',\n",
       " 'LH_913',\n",
       " 'LH_914',\n",
       " 'LH_915',\n",
       " 'LH_916',\n",
       " 'LH_917',\n",
       " 'LH_918',\n",
       " 'LH_919',\n",
       " 'LH_920',\n",
       " 'LH_921',\n",
       " 'LH_922',\n",
       " 'LH_923',\n",
       " 'LH_924',\n",
       " 'LH_925',\n",
       " 'LH_926',\n",
       " 'LH_927',\n",
       " 'LH_928',\n",
       " 'LH_929',\n",
       " 'LH_930',\n",
       " 'LH_931',\n",
       " 'LH_932',\n",
       " 'LH_933',\n",
       " 'LH_934',\n",
       " 'LH_935',\n",
       " 'LH_936',\n",
       " 'LH_937',\n",
       " 'LH_938',\n",
       " 'LH_939',\n",
       " 'LH_940',\n",
       " 'LH_941',\n",
       " 'LH_942',\n",
       " 'LH_943',\n",
       " 'LH_944',\n",
       " 'LH_945',\n",
       " 'LH_946',\n",
       " 'LH_947',\n",
       " 'LH_948',\n",
       " 'LH_949',\n",
       " 'LH_950',\n",
       " 'LH_951',\n",
       " 'LH_952',\n",
       " 'LH_953',\n",
       " 'LH_954',\n",
       " 'LH_955',\n",
       " 'LH_956',\n",
       " 'LH_957',\n",
       " 'LH_958',\n",
       " 'LH_959',\n",
       " 'LH_960',\n",
       " 'LH_961',\n",
       " 'LH_962',\n",
       " 'LH_963',\n",
       " 'LH_964',\n",
       " 'LH_965',\n",
       " 'LH_966',\n",
       " 'LH_967',\n",
       " 'LH_968',\n",
       " 'LH_969',\n",
       " 'LH_970',\n",
       " 'LH_971',\n",
       " 'LH_972',\n",
       " 'LH_973',\n",
       " 'LH_974',\n",
       " 'LH_975',\n",
       " 'LH_976',\n",
       " 'LH_977',\n",
       " 'LH_978',\n",
       " 'LH_979',\n",
       " 'LH_980',\n",
       " 'LH_981',\n",
       " 'LH_982',\n",
       " 'LH_983',\n",
       " 'LH_984',\n",
       " 'LH_985',\n",
       " 'LH_986',\n",
       " 'LH_987',\n",
       " 'LH_988',\n",
       " 'LH_989',\n",
       " 'LH_990',\n",
       " 'LH_991',\n",
       " 'LH_992',\n",
       " 'LH_993',\n",
       " 'LH_994',\n",
       " 'LH_995',\n",
       " 'LH_996',\n",
       " 'LH_997',\n",
       " 'LH_998',\n",
       " 'LH_999']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#directories = [f'LH_{i}' for i in range(1000)]\n",
    "# TEST:\n",
    "# LH_50 ,LH_104 skipped. LH_426,LH_466, LH_513, LH_559, 630 bad data, run again later.\n",
    "directories = [f'LH_{i}' for i in range(560, 1000)]\n",
    "directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the log file\n",
    "log_file_path = \"/home/jovyan/camels/play/synth-play/LH/log.txt\"\n",
    "\n",
    "# Initialize the list of skipped directories\n",
    "skipped_directories = []\n",
    "\n",
    "# Function to log messages\n",
    "def log_message(message):\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(message + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221, 7, 401)\n"
     ]
    }
   ],
   "source": [
    "grid_name = \"bc03-2016-Miles_chabrier-0.1,100.hdf5\"\n",
    "grid_dir = \"/home/jovyan/\"\n",
    "\n",
    "# Create a new grid\n",
    "#grid = Grid(grid_name, grid_dir=grid_dir, read_lines=False)\n",
    "\n",
    "# instead of using a filter, which requires us to load in large SEDs first, pass the grid wavelength\n",
    "#filt1 = Filter(\"top_hat/filter.1\", lam_min=1400, lam_max=1600, new_lam=grid.lam)\n",
    "#filt_lst = [filt1]\n",
    "\n",
    "# Define a new set of wavelengths\n",
    "lims_lams=(1400, 1600)\n",
    "\n",
    "grid = Grid(grid_name, grid_dir=grid_dir, lam_lims=lims_lams, read_lines=False)\n",
    "print(grid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the directory where the text files will be saved\n",
    "output_dir = \"/home/jovyan/camels/play/synth-play/LH/output/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "# Loop over multiple LH directories\n",
    "for LH_X in directories:\n",
    "    try:\n",
    "        log_message(f\"Processing directory: {LH_X}\")\n",
    "\n",
    "        # Get gals\n",
    "        dir_ = '/home/jovyan/Data/Sims/IllustrisTNG/LH/' + LH_X\n",
    "\n",
    "        try:\n",
    "            gals_074 = load_CAMELS_IllustrisTNG(\n",
    "                dir_,\n",
    "                snap_name='snapshot_074.hdf5', \n",
    "                fof_name='groups_074.hdf5',\n",
    "                verbose=False  # Suppress output\n",
    "            )\n",
    "        except Exception as e:\n",
    "            log_message(f\"Error loading CAMELS data for {LH_X}: {e}\")\n",
    "            skipped_directories.append(LH_X)\n",
    "            continue\n",
    "\n",
    "        # Clear the output after each iteration\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        try:\n",
    "            cat_074 = dir_+'/groups_074.hdf5'\n",
    "            # Open file\n",
    "            f_h5py = h5py.File(cat_074, 'r')\n",
    "\n",
    "            # Read different attributes of the header\n",
    "            boxSize_074 = f_h5py['Header'].attrs[u'BoxSize']/1e3 # Mpc/h\n",
    "            redshift_074 = f_h5py['Header'].attrs[u'Redshift']\n",
    "        except Exception as e:\n",
    "            log_message(f\"Error reading HDF5 file for {LH_X}: {e}\")\n",
    "            skipped_directories.append(LH_X)\n",
    "            continue\n",
    "\n",
    "        # Filter galaxies to only include those with 100 or more star particles\n",
    "        try:\n",
    "            gals_074 = [gal for gal in gals_074 if len(gal.stars.ages) >= 100]\n",
    "            spec_list = []\n",
    "            # Lets work with z=0 so gals_025\n",
    "            for i in gals_074:\n",
    "                gal = i\n",
    "                # Get spectra incident An Sed object containing the stellar spectra\n",
    "                spec = gal.stars.get_spectra_incident(grid)\n",
    "                spec.get_fnu0()\n",
    "                spec_list.append(spec)\n",
    "\n",
    "            # Combine\n",
    "            seds = combine_list_of_seds(spec_list)\n",
    "            filt1 = Filter(\"top_hat/filter.1\", lam_min=1400, lam_max=1600, new_lam=grid.lam)\n",
    "            filt_lst = [filt1]\n",
    "            seds.lnu # Rest frame lumd\n",
    "            seds.get_photo_luminosities(filt_lst)\n",
    "            seds.photo_luminosities.photo_luminosities\n",
    "            abs_mag = lnu_to_absolute_mag(seds.photo_luminosities.photo_luminosities)\n",
    "            abs_mag_th = abs_mag[0]\n",
    "            # Next steps, get luminosity function for these magnitudes\n",
    "            # Co-moving volume: BoxSize_025 and redshift:\n",
    "            little_h =  0.6711\n",
    "            Vphys = (boxSize_074/little_h )**3\n",
    "            Vcom = Vphys * ((1+redshift_074)**3)\n",
    "\n",
    "            massBinLimits = np.arange(-22, -16, 0.5)\n",
    "            phi, phi_sigma, hist = calc_lf(abs_mag_th, Vcom, massBinLimits)\n",
    "            # NOTE: 074 is the same redshift as CV_0/025\n",
    "            massBinLimits = massBinLimits[:-1]\n",
    "        except Exception as e:\n",
    "            log_message(f\"Error processing galaxy data for {LH_X}: {e}\")\n",
    "            skipped_directories.append(LH_X)\n",
    "            continue\n",
    "\n",
    "        # Define output file path\n",
    "        output_file = f\"{output_dir}{LH_X}.txt\"\n",
    "\n",
    "        try:\n",
    "            # Write the data to the text file line by line\n",
    "            with open(output_file, 'w') as txtfile:\n",
    "                # Write phi values\n",
    "                txtfile.write(\"phi\\n\")\n",
    "                for value in phi:\n",
    "                    txtfile.write(f\"{value}\\n\")\n",
    "\n",
    "                # Write phi_sigma values\n",
    "                txtfile.write(\"phi_sigma\\n\")\n",
    "                for value in phi_sigma:\n",
    "                    txtfile.write(f\"{value}\\n\")\n",
    "\n",
    "                # Write hist values\n",
    "                txtfile.write(\"hist\\n\")\n",
    "                for value in hist:\n",
    "                    txtfile.write(f\"{value}\\n\")\n",
    "\n",
    "                # Write massBinLimits values\n",
    "                txtfile.write(\"massBinLimits\\n\")\n",
    "                for value in massBinLimits:\n",
    "                    txtfile.write(f\"{value}\\n\")\n",
    "            \n",
    "            log_message(f\"Written out: {output_file}\")\n",
    "        except Exception as e:\n",
    "            log_message(f\"Error writing output file for {LH_X}: {e}\")\n",
    "            skipped_directories.append(LH_X)\n",
    "            continue\n",
    "\n",
    "        # Clear the terminal output after each iteration\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')       \n",
    "\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error processing {LH_X}: {e}\")\n",
    "        skipped_directories.append(LH_X)\n",
    "\n",
    "# Write the skipped directories to the log file at the end\n",
    "if skipped_directories:\n",
    "    log_message(\"Skipped directories:\")\n",
    "    for dir in skipped_directories:\n",
    "        log_message(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try data frame instead\n",
    "grid_name = \"bc03-2016-Miles_chabrier-0.1,100.hdf5\"\n",
    "grid_dir = \"/home/jovyan/\"\n",
    "grid = Grid(grid_name, grid_dir=grid_dir, read_lines=False)\n",
    "\n",
    "# Define the directory where the text files will be saved\n",
    "output_dir = \"/home/jovyan/camels/play/synth-play/LH/output/\"\n",
    "\n",
    "for LH_X in directories:\n",
    "    try:\n",
    "        # get grid for all galaxies\n",
    "        # gives grid of metalicity and age which is used to map on our camels galaxies\n",
    "\n",
    "        # get gals\n",
    "        dir_ = '/home/jovyan/Data/Sims/IllustrisTNG/LH/' + LH_X\n",
    "        gals_074 = load_CAMELS_IllustrisTNG(\n",
    "            dir_,\n",
    "            snap_name='snapshot_074.hdf5', \n",
    "            fof_name='groups_074.hdf5',\n",
    "            verbose=False  # Suppress output\n",
    "        )\n",
    "        # Clear the output after each iteration\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        cat_074 = dir_+'/groups_074.hdf5'\n",
    "        # open file\n",
    "        f_h5py = h5py.File(cat_074, 'r')\n",
    "\n",
    "        # read different attributes of the header\n",
    "        boxSize_074 = f_h5py['Header'].attrs[u'BoxSize']/1e3 #Mpc/h\n",
    "        redshift_074 = f_h5py['Header'].attrs[u'Redshift']\n",
    "\n",
    "        # Filter galaxies to only include those with 100 or more star particles\n",
    "        gals_074 = [gal for gal in gals_074 if len(gal.stars.ages) >= 100]\n",
    "        spec_list = []\n",
    "        # Lets work with z=0 so gals_025\n",
    "        for i in gals_074:\n",
    "            gal = i\n",
    "            # get_spectra_incident An Sed object containing the stellar spectra\n",
    "            spec = gal.stars.get_spectra_incident(grid)\n",
    "            spec.get_fnu0()\n",
    "            spec_list.append(spec)\n",
    "\n",
    "        # combine\n",
    "        seds = combine_list_of_seds(spec_list)\n",
    "        filt1 = Filter(\"top_hat/filter.1\", lam_min=1400, lam_max=1600, new_lam=grid.lam)\n",
    "        filt_lst = [filt1]\n",
    "        seds.lnu # rest frame lumd\n",
    "        seds.get_photo_luminosities(filt_lst)\n",
    "        seds.photo_luminosities.photo_luminosities\n",
    "        abs_mag = lnu_to_absolute_mag(seds.photo_luminosities.photo_luminosities)\n",
    "        abs_mag_th = abs_mag[0]\n",
    "        # next steps, get luminosity function for these magnitudes\n",
    "        # co-moving volume: BoxSize_025 and redshift:\n",
    "        little_h =  0.6711\n",
    "        Vphys = (boxSize_074/little_h )**3\n",
    "        Vcom = Vphys * ((1+redshift_074)**3)\n",
    "\n",
    "        massBinLimits = np.arange(-22, -16, 0.5)\n",
    "        phi, phi_sigma, hist = calc_lf(abs_mag_th, Vcom, massBinLimits)\n",
    "        # NOTE: 074 is the same redshift as CV_0/025\n",
    "        massBinLimits = massBinLimits[:-1]\n",
    "\n",
    "        # Define output file path\n",
    "        output_file = f\"{output_dir}{LH_X}.txt\"\n",
    "\n",
    "        # Write the data to the text file line by line\n",
    "        with open(output_file, 'w') as txtfile:\n",
    "            # Write phi values\n",
    "            txtfile.write(\"phi\\n\")\n",
    "            for value in phi:\n",
    "                txtfile.write(f\"{value}\\n\")\n",
    "\n",
    "            # Write phi_sigma values\n",
    "            txtfile.write(\"phi_sigma\\n\")\n",
    "            for value in phi_sigma:\n",
    "                txtfile.write(f\"{value}\\n\")\n",
    "\n",
    "            # Write hist values\n",
    "            txtfile.write(\"hist\\n\")\n",
    "            for value in hist:\n",
    "                txtfile.write(f\"{value}\\n\")\n",
    "\n",
    "            # Write massBinLimits values\n",
    "            txtfile.write(\"massBinLimits\\n\")\n",
    "            for value in massBinLimits:\n",
    "                txtfile.write(f\"{value}\\n\")\n",
    "\n",
    "        print('Written out: /home/jovyan/camels/play/synth-play/LH/output/',LH_X)\n",
    "\n",
    "        # Clear the terminal output after each iteration\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')       \n",
    "        \n",
    "    except Exception as e:\n",
    "        error_message = f\"Error processing {LH_X}: {e}\"\n",
    "        print(error_message)\n",
    "        skipped_directories.append(LH_X)\n",
    "\n",
    "        # Append the error message to the log file\n",
    "        with open(log_file_path, 'a') as log_file:\n",
    "            log_file.write(error_message + '\\n')\n",
    "\n",
    "# Write the skipped directories to the log file at the end\n",
    "if skipped_directories:\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(\"Skipped directories:\\n\")\n",
    "        for dir in skipped_directories:\n",
    "            log_file.write(f\"{dir}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get memory usage in bytes\n",
    "memory_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "print(\"Memory usage end:\", memory_usage/1000000, \"GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# will do latest LH_ set in loop as a test\n",
    "label_z = 'z = 0.46'\n",
    "\n",
    "\n",
    "# Plot the luminosity function\n",
    "plt.errorbar(massBinLimits, phi, yerr=phi_sigma, fmt='o', color='blue',label=label_z)\n",
    "plt.xlabel('Absolute Magnitude (AB)')\n",
    "plt.ylabel('Number Density (Mpc^-3 mag^-1)')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.title('Luminosity Function XMM-OM filter')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
